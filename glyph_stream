#!/usr/bin/env python3
import argparse, collections, io, json, math, os, platform, random, re, shlex
import shutil, socket, subprocess, sys, tempfile, time, traceback, unicodedata, uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Set, Tuple, Union

# Conditional imports with elegant fallbacks
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    
try:
    from PIL import Image, ImageDraw, ImageFont, ImageOps
    HAS_PILLOW = True
except ImportError:
    HAS_PILLOW = False

try:
    import cv2
    HAS_CV2 = True
except ImportError:
    HAS_CV2 = False

try:
    import pyfiglet
    HAS_FIGLET = True
except ImportError:
    HAS_FIGLET = False

try:
    import yt_dlp
    HAS_YT_DLP = True
except ImportError:
    HAS_YT_DLP = False

try:
    import colorama
    colorama.init()
    HAS_COLORAMA = True
except ImportError:
    HAS_COLORAMA = False

try:
    import rich
    from rich.console import Console, Group
    from rich.panel import Panel
    from rich.table import Table
    from rich.text import Text
    from rich.align import Align
    from rich.prompt import Prompt, Confirm
    HAS_RICH = True
    CONSOLE = Console(highlight=True)
except ImportError:
    HAS_RICH = False
    CONSOLE = None

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

try:
    import pyvirtualdisplay
    HAS_VIRTUAL_DISPLAY = True
except ImportError:
    HAS_VIRTUAL_DISPLAY = False

# Unified module availability tracking
MODULES = {
    "numpy": np if HAS_NUMPY else None,
    "pillow": Image if HAS_PILLOW else None,
    "cv2": cv2 if HAS_CV2 else None,
    "pyfiglet": pyfiglet if HAS_FIGLET else None,
    "yt_dlp": yt_dlp if HAS_YT_DLP else None,
    "colorama": colorama if HAS_COLORAMA else None,
    "rich": rich if HAS_RICH else None,
    "psutil": psutil if HAS_PSUTIL else None,
    "pyvirtualdisplay": pyvirtualdisplay if HAS_VIRTUAL_DISPLAY else None
}

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸŒŒ Hyperdimensional Environment Awareness System            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Terminal and environment detection with elegant fallbacks
TERMINAL_WIDTH = shutil.get_terminal_size().columns if hasattr(shutil, 'get_terminal_size') else 80
TERMINAL_HEIGHT = shutil.get_terminal_size().lines if hasattr(shutil, 'get_terminal_size') else 24
INTERACTIVE_TERMINAL = sys.stdout.isatty()

# Enhanced detection for Unicode and color support
SUPPORTS_UNICODE = (sys.stdout.encoding and 'utf' in sys.stdout.encoding.lower() and 
                   not any(k in os.environ for k in ('NO_UNICODE', 'ASCII_ONLY')))

SUPPORTS_COLOR = (HAS_COLORAMA or 
                 sys.platform != 'win32' or 
                 any(k in os.environ for k in ('ANSICON', 'WT_SESSION')) or
                 os.environ.get('TERM_PROGRAM') in ('vscode', 'iTerm.app') or
                 os.environ.get('COLORTERM') in ('truecolor', '24bit') or
                 os.environ.get('TERM', '').endswith(('color', '256color')) or 
                 os.environ.get('FORCE_COLOR'))

# Author and project metadata with dimensional capabilities matrix
AUTHOR_INFO = {
    "name": "Lloyd Handyside",
    "email": "ace1928@gmail.com",
    "org": "Neuroforge",
    "org_email": "lloyd.handyside@neuroforge.io",
    "contributors": ["Eidos <syntheticeidos@gmail.com>", "Prismatic Architect <prism@neuroforge.io>"],
    "version": "1.0.1",
    "updated": "2025-03-16",
    "codename": "Prismatic Cipher",
    "release_stage": "stable",
    "license": "MIT",
    "repository": "github.com/Ace1928/glyph_forge",
    "documentation": "https://neuroforge.io/docs/glyph_stream",
    "support": "https://neuroforge.io/support",
    "keywords": [
        "unicode-art", "terminal-graphics", "dimensional-rendering", 
        "reality-transmutation", "prismatic-encoding", "visual-transcendence"
    ],
    "capabilities": {
        "image_processing": HAS_PILLOW,
        "video_processing": HAS_CV2,
        "streaming": HAS_YT_DLP,
        "enhanced_display": HAS_RICH,
        "text_art": HAS_FIGLET,
        "virtual_display": HAS_VIRTUAL_DISPLAY,
        "system_monitoring": HAS_PSUTIL
    },
    "preferences": {
        "banner_style": "cosmic",
        "color_scheme": "prismatic",
        "edge_detection": "adaptive",
        "default_scale": 2,
        "show_banner_on_import": True
    }
}

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸŒ  Unified Dimensional Banner System                        â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class BannerEngine:
    """ğŸ­ Dimensional banner system with adaptive rendering capabilities."""
    _instance = None
    _cache = {}
    
    @classmethod
    def get_instance(cls) -> 'BannerEngine':
        """ğŸ”„ Access singleton with lazy initialization."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def __init__(self):
        """Initialize banner engine with environment awareness."""
        self.has_figlet = HAS_FIGLET
        self.supports_unicode = SUPPORTS_UNICODE
        self.supports_color = SUPPORTS_COLOR
        self.terminal_width = TERMINAL_WIDTH
    
    def display(self, style: str = "auto", color: bool = True, width: Optional[int] = None) -> None:
        """ğŸ¨ Display dimensional banner with optimal rendering."""
        # Auto-select style based on terminal width
        if style == "auto":
            style = "cosmic" if self.terminal_width >= 80 else "compact" if self.terminal_width >= 40 else "minimal"
        
        banner = self.generate(style, color, width)
        print(banner) if not HAS_RICH else CONSOLE.print(banner)
    
    def generate(self, style: str = "full", color: bool = True, width: Optional[int] = None) -> str:
        """ğŸŒˆ Generate dimensional banner with specified parameters."""
        width = width or self.terminal_width
        use_color = color and self.supports_color
        use_unicode = self.supports_unicode and style != "ascii"
        
        # Smart style adaptation
        if width < 40: style = "minimal"
        elif width < 80 and style == "full": style = "compact"
        if not use_unicode: style = "ascii"
        
        # Efficient caching with template generation
        cache_key = f"{style}:{width}:{use_color}:{use_unicode}"
        if cache_key not in self._cache:
            if len(self._cache) > 10: self._cache.clear()
            self._cache[cache_key] = self._generate_template(style, width, use_unicode)
        
        # Populate template with dynamic content
        symbols = self._get_symbol_set(use_unicode)
        metadata = {**AUTHOR_INFO, **symbols}
        
        # Generate title art with figlet if available
        metadata["title_art"] = (pyfiglet.figlet_format("GLYPH STREAM", font="standard") 
                               if self.has_figlet else "GLYPH STREAM")
            
        banner = self._cache[cache_key].format(**metadata)
        return self._apply_colors(banner) if use_color and not HAS_RICH else banner
    
    def _get_symbol_set(self, unicode_support: bool) -> Dict[str, str]:
        """ğŸ”£ Get appropriate symbol set based on Unicode support."""
        return {
            "stars": "âœ§âœ¦âœ«" if unicode_support else "*-*",
            "dimension": "âŸ" if unicode_support else "+",
            "cosmic": "âš" if unicode_support else "#",
            "bullet": "â€¢" if unicode_support else "*",
            "top_left": "â•”" if unicode_support else "+",
            "top_right": "â•—" if unicode_support else "+",
            "bottom_left": "â•š" if unicode_support else "+",
            "bottom_right": "â•" if unicode_support else "+",
            "horizontal": "â•" if unicode_support else "-",
            "vertical": "â•‘" if unicode_support else "|",
            "star_elem": "âœ§" if unicode_support else "*",
            "star2": "âœ¦" if unicode_support else "*",
            "star3": "âœ«" if unicode_support else "*",
            "diamond": "â—ˆ" if unicode_support else "<>"
        }
    
    def _generate_template(self, style: str, width: int, unicode: bool) -> str:
        """ğŸ“ Generate appropriate banner template based on style."""
        templates = {
            "minimal": (
                "{top_left}" + "{horizontal}" * (width-2) + "{top_right}\n"
                "{vertical} GlyphStream v{version} {vertical}\n"
                "{bottom_left}" + "{horizontal}" * (width-2) + "{bottom_right}"
            ),
            "compact": (
                "{top_left}" + "{horizontal}" * (width-2) + "{top_right}\n"
                "{vertical} {stars} GLYPH STREAM v{version} - {codename} {stars} {vertical}\n"
                "{vertical} Dimensional Unicode Transmutation Engine {vertical}\n"
                "{bottom_left}" + "{horizontal}" * (width-2) + "{bottom_right}"
            ),
            "ascii": (
                "+" + "-" * (width-2) + "+\n"
                "| GLYPH STREAM v{version} |\n"
                "| Dimensional Unicode Engine |\n"
                "+" + "-" * (width-2) + "+"
            ),
            "cosmic": (
                "{top_left}" + "{horizontal}" * (width-2) + "{top_right}\n"
                "{vertical} {title_art} {vertical}\n"
                "{vertical}" + "â•" * (width-4) + "{vertical}\n"
                "{vertical}  {star_elem}{star2}{cosmic} PRISMATIC CIPHER v{version} {cosmic}{star2}{star_elem}  {vertical}\n"
                "{vertical}  {dimension} Transform reality through visual transcendence {dimension}  {vertical}\n"
                "{bottom_left}" + "{horizontal}" * (width-2) + "{bottom_right}\n"
                "{cosmic}{star_elem} ADAPTIVE UNIVERSALITY MATRIX INITIALIZED {star_elem}{cosmic}"
            ),
            "full": (
                "{top_left}" + "{horizontal}" * (width-2) + "{top_right}\n"
                "{vertical} {title_art} {vertical}\n"
                "{vertical}" + "â•" * (width-4) + "{vertical}\n"
                "{vertical} {dimension} Prismatic Cipher Edition v{version} {dimension} {vertical}\n"
                "{vertical}  {star_elem} DIMENSIONAL GLYPH TRANSMUTATION ENGINE {star_elem}  {vertical}\n"
                "{vertical}" + "â•" * (width-4) + "{vertical}\n"
                "{vertical}  {bullet} Maintainer: {name} ã€Œ{email}ã€ {vertical}\n"
                "{vertical}  {dimension} Organization: {org} ã€Œ{org_email}ã€ {vertical}\n"
                "{bottom_left}" + "{horizontal}" * (width-2) + "{bottom_right}"
            )
        }
        return templates.get(style, templates["full"])
    
    def _apply_colors(self, banner: str) -> str:
        """ğŸ¨ Apply ANSI colors to banner with style-specific enhancements."""
        color_map = {
            "GLYPH STREAM": "\033[96m",  # BRIGHT_CYAN
            "DIMENSIONAL": "\033[95m",   # BRIGHT_MAGENTA
            "PRISMATIC CIPHER": "\033[95m",  # BRIGHT_MAGENTA
            "Transform": "\033[32m",     # GREEN
            "Maintainer:": "\033[33m",   # YELLOW
            "Organization:": "\033[33m", # YELLOW
            "ADAPTIVE UNIVERSALITY MATRIX": "\033[95m"  # BRIGHT_MAGENTA
        }
        
        # Colorize special symbols
        symbol_colors = {
            s: "\033[35m" for s in ["âœ§", "âŸ", "âœ¦", "âš", "*-*"]  # MAGENTA
        }
        
        # Colorize border elements
        border_color = "\033[34m"  # BLUE
        border_elements = ["â•", "â•”", "â•š", "â•‘", "+", "-", "|"]
        
        RESET = "\033[0m"
        result = []
        
        for line in banner.split('\n'):
            # Check for specific phrases
            colored = False
            for phrase, color in color_map.items():
                if phrase in line:
                    result.append(f"{color}{line}{RESET}")
                    colored = True
                    break
                    
            if not colored:
                # Check for border elements
                if any(elem in line for elem in border_elements):
                    result.append(f"{border_color}{line}{RESET}")
                    continue
                
                # Check for special symbols
                for sym, color in symbol_colors.items():
                    if sym in line:
                        result.append(f"{color}{line}{RESET}")
                        colored = True
                        break
                
                # Default case
                if not colored:
                    result.append(line)
                
        return "\n".join(result)

# Initialize banner singleton
BANNER_ENGINE = BannerEngine.get_instance()

def display_banner(style: str = "auto", color: bool = True, width: Optional[int] = None) -> None:
    """ğŸ­ Display dimensional banner with environment adaptation."""
    BANNER_ENGINE.display(style, color, width)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸ› ï¸ System Intelligence & Environment Analysis               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SystemContext:
    """ğŸ§  Self-aware execution environment with adaptive capabilities."""
    
    def __init__(self) -> None:
        """Initialize system context with unified analysis."""
        # Core system attributes with elegant fallbacks
        self.attributes = {
            # System identification
            "platform": platform.system(),
            "architecture": platform.machine(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "hostname": platform.node(),
            
            # Terminal capabilities
            "terminal_width": TERMINAL_WIDTH,
            "terminal_height": TERMINAL_HEIGHT,
            "is_interactive": INTERACTIVE_TERMINAL,
            "supports_unicode": SUPPORTS_UNICODE,
            "supports_ansi_color": SUPPORTS_COLOR,
            
            # Memory and CPU resources
            "memory_total": getattr(psutil.virtual_memory(), "total", 0) if HAS_PSUTIL else 0,
            "memory_available": getattr(psutil.virtual_memory(), "available", 0) if HAS_PSUTIL else 0,
            "cpu_count": psutil.cpu_count(logical=True) if HAS_PSUTIL else os.cpu_count() or 2,
            "cpu_physical": psutil.cpu_count(logical=False) if HAS_PSUTIL else os.cpu_count() or 1,
            
            # Network status and hardware acceleration
            "network_connected": self._quick_network_check(),
            "numpy_optimized": self._check_numpy_optimization(),
            "has_cuda": False,  # Default, would be set if detected
            "has_metal": platform.system() == "Darwin"  # Simple macOS check
        }
        
        # Derive capabilities from attributes
        tier = self._get_performance_tier()
        self.capabilities = {
            "can_display_unicode": self.attributes["supports_unicode"],
            "can_display_color": self.attributes["supports_ansi_color"],
            "can_display_animations": self.attributes["is_interactive"] and self.attributes["supports_ansi_color"],
            "has_high_performance": tier >= 2,
            "has_network_access": self.attributes["network_connected"]
        }
        
        # Identify constraints based on system analysis
        self.constraints = {
            "limited_width": self.attributes["terminal_width"] < 60,
            "limited_height": self.attributes["terminal_height"] < 20,
            "max_art_width": self.attributes["terminal_width"] - (2 if self.attributes["terminal_width"] < 60 else 4),
            "max_art_height": self.attributes["terminal_height"] - (4 if self.attributes["terminal_height"] < 20 else 6),
            "performance_tier": tier,
            "max_scale_factor": 1 if tier == 0 else 2 if tier == 1 else 4,
            "default_fps": 5 if tier == 0 else 10 if tier == 1 else 15
        }

    def _quick_network_check(self) -> bool:
        """ğŸŒ Fast network connectivity check."""
        try:
            socket.create_connection(("8.8.8.8", 53), timeout=1.0)
            return True
        except (socket.error, socket.timeout):
            return False
    
    def _check_numpy_optimization(self) -> bool:
        """ğŸ” Check if NumPy is using optimized libraries."""
        if not HAS_NUMPY:
            return False
        try:
            config_info = np.__config__.show() if hasattr(np, "__config__") else ""
            return isinstance(config_info, str) and any(lib in config_info.lower() 
                                                     for lib in ["mkl", "openblas"])
        except (ImportError, AttributeError):
            return False
    
    def _get_performance_tier(self) -> int:
        """ğŸš„ Calculate system performance tier (0=low to 3=very high)."""
        # Dynamic scoring based on CPU and memory resources
        cpu_count = self.attributes["cpu_count"]
        memory_gb = self.attributes["memory_available"] / (1024 * 1024 * 1024) if self.attributes["memory_available"] else 2
        
        # Score calculation with elegant bounds
        cpu_score = 2 if cpu_count >= 16 else 1 if cpu_count >= 8 else -1 if cpu_count <= 2 else 0
        mem_score = 1 if memory_gb >= 16 else -1 if memory_gb <= 2 else 0
        
        return max(0, min(1 + cpu_score + mem_score, 3))
    
    def get_optimized_parameters(self) -> Dict[str, Any]:
        """âš¡ Provide context-aware optimized parameters for current environment."""
        tier = self.constraints["performance_tier"]
        return {
            "scale_factor": self.constraints["max_scale_factor"],
            "block_width": 4 if self.constraints["limited_width"] else 6 if tier >= 2 else 8,
            "block_height": 8 if self.constraints["limited_width"] else 6 if tier >= 2 else 8,
            "fps": self.constraints["default_fps"],
            "edge_mode": "simple" if tier == 0 else "enhanced",
            "color_mode": self.capabilities["can_display_color"],
            "animation_level": 0 if tier == 0 else 2 if tier >= 2 else 1,
            "max_width": self.constraints["max_art_width"],
            "max_height": self.constraints["max_art_height"],
        }

# Initialize system context at module load
SYSTEM_CONTEXT = SystemContext()

# Legacy compatibility
SYSTEM_INFO = SYSTEM_CONTEXT.attributes

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸ¨ Unicode Rendering Engine                                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class UnicodeRenderEngine:
    """âš™ï¸ High-precision multi-dimensional Unicode rendering system.
    
    Provides comprehensive character mapping, edge detection matrices,
    and adaptive rendering with intelligent dimension analysis.
    """
    # Singleton instance for global access with lazy initialization
    _instance = None
    
    @classmethod
    def get_instance(cls) -> 'UnicodeRenderEngine':
        """ğŸ”„ Access singleton with memory-optimized instantiation."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def __init__(self) -> None:
        """Initialize rendering engine with adaptive capabilities."""
        self.cache_dir = Path(os.path.expanduser("~/.cache/glyph_stream"))
        self.cache_file = self.cache_dir / "character_maps.json"
        self.character_maps = {}
        self._gradient_categories = {}
        self._edge_matrices = {}
        self._color_maps = {}
        
        # System-aware initialization
        self.supports_unicode = SYSTEM_CONTEXT.capabilities.get("can_display_unicode", True)
        self.supports_color = SYSTEM_CONTEXT.capabilities.get("can_display_color", True)
        
        # Initialize character sets
        self._load_or_generate_character_maps()
        
    def _load_or_generate_character_maps(self) -> None:
        """ğŸ“ Load cached character maps or generate optimized defaults."""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.character_maps = json.load(f)
                return
        except (json.JSONDecodeError, OSError):
            pass  # Continue to generation on any error
        
        # Generate maps from scratch
        self._generate_character_maps()
        self._save_character_maps()
    
    def _generate_character_maps(self) -> None:
        """ğŸ” Generate comprehensive character maps for all rendering modes."""
        # Core character categories
        self.character_maps = {
            # Density gradients - from solid to empty
            "gradient": {
                "standard": "â–ˆâ–“â–’â–‘ ",
                "enhanced": "â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–â–€ ",
                "blocks": "â–ˆâ–‰â–Šâ–‹â–Œâ–â–â– ",
                "dots": "â– â—â—‰â—â—‹â—ŒÂ· ",
                "squares": "â– â–¡â–£â–¤â–¥â–¦â–§â–¨â–©â–ªâ–«â—»â—¼â—˜â—™â—šâ—› ",
                "shapes": "â– â—†â—â—‰â—Œâ—‹â—â—â—â—‘â—’â—“â—”â—•â™¦â™¥â™£â™ â˜»â˜ºâ¬¢â¬¡â¬¤â¬ªâ¬«â­“â­”âšªâš«âš¬âš­âš®â˜˜â˜„",
                "symbols": "âœ¿â€ââ‚âœ£âœ¤âœ¥â„â…â†â‡âˆâ‰âŠâœ½âœ¾âœ¿â‹",
                "stars": "â˜…â˜†âœ©âœ«âœ¬âœ­âœ®âœ¯âœ¡âœ§âœ¦âœª",
                "braille": "â£¿â£·â£¯â£Ÿâ¡¿â¢¿â£»â£½â£¾ ",
                "ascii": "@%#*+=-:. "
            },
            
            # Edges by direction and intensity
            "edges": {
                "horizontal": {
                    "bold": "â”",
                    "standard": "â”€",
                    "light": "â•Œ",
                    "double": "â•",
                    "ascii": "-"
                },
                "vertical": {
                    "bold": "â”ƒ",
                    "standard": "â”‚",
                    "light": "â•",
                    "double": "â•‘",
                    "ascii": "|"
                },
                "diagonal_ne": {  # northeast (â†—)
                    "bold": "â•±",
                    "standard": "â•±",
                    "light": "â”„",
                    "ascii": "/"
                },
                "diagonal_nw": {  # northwest (â†–)
                    "bold": "â•²",
                    "standard": "â•²",
                    "light": "â”„",
                    "ascii": "\\"
                },
                "corners": {
                    "top_left": "â”Œ",
                    "top_right": "â”",
                    "bottom_left": "â””",
                    "bottom_right": "â”˜"
                },
                "junctions": {
                    "cross": "â”¼",
                    "t_up": "â”´",
                    "t_down": "â”¬",
                    "t_left": "â”¤",
                    "t_right": "â”œ"
                }
            },
            
            # Special rendering modes
            "modes": {
                "fallback": "â– â–¡â–ªâ–«â—»â—¼â—˜â—™â—šâ—›",
                "braille_matrix": self._generate_braille_matrix(),
                "shade_matrix": self._generate_shade_matrix()
            },
            
            # Complete gradient sequences for optimal rendering
            "full_gradients": {
                "standard": self._generate_enhanced_gradient_chars(),
                "minimal": "â–ˆâ–“â–’â–‘ ",
                "ascii_art": "@%#*+=-:. ",
                "braille": "â£¿â£·â£¯â£Ÿâ¡¿â¢¿â£»â£½â£¾ "
            }
        }
    
    def _save_character_maps(self) -> None:
        """ğŸ’¾ Cache character maps for future fast initialization."""
        try:
            self.cache_dir.mkdir(exist_ok=True, parents=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.character_maps, f, ensure_ascii=False, indent=2)
        except (OSError, PermissionError):
            # Silent degradation on save failure - still functional
            pass
    
    def _generate_braille_matrix(self) -> Dict[str, str]:
        """ğŸ”  Generate complete 2Ã—4 braille pattern matrix."""
        # Braille patterns follow a binary mapping pattern
        # Each dot position corresponds to a bit in a byte
        patterns = {}
        
        # Generate all 256 braille patterns (2^8 combinations)
        for i in range(256):
            # Convert to Unicode braille (starts at U+2800)
            char_code = 0x2800 + i
            patterns[f"{i:08b}"] = chr(char_code)
            
        return patterns
    
    def _generate_shade_matrix(self) -> Dict[str, str]:
        """ğŸ”² Generate block element shading matrix for sub-character precision."""
        # Block elements with different shading patterns
        return {
            "full": "â–ˆ",
            "seven_eighths": "â–‡",
            "three_quarters": "â–†",
            "five_eighths": "â–…",
            "half": "â–„",
            "three_eighths": "â–ƒ",
            "quarter": "â–‚",
            "eighth": "â–",
            "empty": " "
        }
    
    def _generate_enhanced_gradient_chars(self) -> str:
        """ğŸ”  Create perceptually-balanced character density gradient.
        
        Returns:
            String with characters from highest to lowest visual density
        """
        # Core block characters (guaranteed in most fonts)
        blocks = "â–ˆâ–“â–’â–‘"
        
        # Geometric shapes (common but may vary in rendering)
        shapes = "â–€â– â—†â—â—‰â—Œâ—‹â—â—â—â—‘â—’â—“â—”â—•"
        
        # Decorative symbols (may render differently across fonts)
        symbols = "â™¦â™¥â™£â™ â˜»â˜ºâ¬¢â¬¡â¬¤â¬ªâ¬«â­“â­”âšªâš«âš¬âš­âš®â˜˜â˜„âœ¿â€ââ‚âœ£âœ¤âœ¥â„â…â†â‡âˆâ‰âŠâœ½âœ¾âœ¿â‹ğŸ‘â˜…â˜†âœ©âœ«âœ¬âœ­âœ®âœ¯âœ¡âœ§âœ¦âœª"
        
        # Spacing and minimal characters
        spaces = "â‹âŠâ‰âˆâ‡â†â…â„âœ¥âœ¤âœ£â‚ââ€âœ¿â˜„ .    "
        
        # Full optimized gradient
        return blocks + shapes + symbols + spaces
    
    def get_ansi_color(self, r: int, g: int, b: int, 
                       background: bool = False) -> str:
        """ğŸŒˆ Generate 24-bit ANSI color sequence with caching.
        
        Args:
            r, g, b: RGB color components (0-255)
            background: Whether to set background instead of foreground
            
        Returns:
            ANSI escape code for color
        """
        # Fast path for non-color terminals
        if not self.supports_color:
            return ""
            
        # Cache key for performance
        key = f"{r}:{g}:{b}:{1 if background else 0}"
        
        # Memory-efficient caching with bounded size
        if key in self._color_maps:
            return self._color_maps[key]
            
        # Generate fresh color code
        code = f"\033[{48 if background else 38};2;{r};{g};{b}m"
        
        # Cache with size management (keep cache under 1024 entries)
        if len(self._color_maps) >= 1024:
            # Efficient single item removal instead of clear
            self._color_maps.pop(next(iter(self._color_maps.keys())))
        self._color_maps[key] = code
        
        return code
    
    def get_gradient(self, 
                   density: float, 
                   mode: str = "standard") -> str:
        """ğŸ“Š Get character representing specific density with adaptive mode.
        
        Args:
            density: Visual density from 0.0 (empty) to 1.0 (solid)
            mode: Gradient character set to use
            
        Returns:
            Unicode character matching requested density
        """
        # Parameter validation
        density = max(0.0, min(1.0, density))
        
        # Get gradient appropriate for terminal capabilities
        if not self.supports_unicode and mode != "ascii":
            mode = "ascii"
        
        # Select character set
        gradient = self.character_maps["full_gradients"].get(
            mode, self.character_maps["full_gradients"]["standard"])
            
        # Fast single-index lookup
        index = int(density * (len(gradient) - 1))
        
        return gradient[index]
    
    def get_edge_char(self, 
                     grad_x: float, 
                     grad_y: float, 
                     strength: float = 1.0, 
                     style: str = "standard") -> str:
        """ğŸ§© Select optimal Unicode edge representation.
        
        Args:
            grad_x: X gradient (-1.0 to 1.0)
            grad_y: Y gradient (-1.0 to 1.0)
            strength: Edge intensity factor (0.0-1.0)
            style: Edge style (bold, standard, light, double, ascii)
            
        Returns:
            Unicode character matching edge direction and intensity
        """
        # Parameter normalization
        strength = max(0.0, min(1.0, strength))
        
        # Fall back to ASCII if Unicode not supported
        if not self.supports_unicode:
            style = "ascii"
        
        # Flat region detection (early exit)
        if abs(grad_x) < 1e-6 and abs(grad_y) < 1e-6:
            return "Â·" if self.supports_unicode else "."
        
        # Apply strength to style selection
        if style == "standard" and strength > 0.8:
            style = "bold"
        elif style == "standard" and strength < 0.3:
            style = "light"
        
        # Angular sector mapping (0Â° = right, 90Â° = up)
        # Cached computation with vector angle optimization
        angle = math.degrees(math.atan2(grad_y, grad_x)) % 180
        
        # Direction classification with efficient boundary checks
        edges = self.character_maps["edges"]
        
        # Fast path dispatch using angle ranges
        if angle < 22.5 or angle >= 157.5:
            return edges["horizontal"][style]
        elif 67.5 <= angle < 112.5:
            return edges["vertical"][style]
        elif 22.5 <= angle < 67.5:
            return edges["diagonal_ne"][style]
        else:  # 112.5 <= angle < 157.5
            return edges["diagonal_nw"][style]
    
    def get_reset_code(self) -> str:
        """ğŸ”„ Get ANSI reset code with capability check."""
        return "\033[0m" if self.supports_color else ""
    
    def get_text_width(self, text: str) -> int:
        """ğŸ“ Calculate text width in terminal with Unicode awareness.
        
        Args:
            text: Input string
            
        Returns:
            Width in character cells (not bytes)
        """
        # Strip ANSI escape sequences which don't consume width
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        clean_text = ansi_escape.sub('', text)
        
        # Count width with Unicode awareness
        width = 0
        for char in clean_text:
            # East Asian full-width characters
            if unicodedata.east_asian_width(char) in ('F', 'W'):
                width += 2
            # Zero-width characters
            elif unicodedata.category(char) in ('Mn', 'Me', 'Cf'):
                continue
            # Normal-width characters
            else:
                width += 1
                
        return width
    
    def get_enhanced_gradient_chars(self) -> str:
        """ğŸ”  Get comprehensive gradient character set.
        
        Returns:
            Character sequence from dense to sparse
        """
        return self.character_maps["full_gradients"]["standard"]
    
    def apply_color(self, text: str, r: int, g: int, b: int) -> str:
        """ğŸ¨ Apply ANSI color with reset to text.
        
        Args:
            text: Text to colorize
            r, g, b: RGB color components
            
        Returns:
            Colorized text with reset code
        """
        if not self.supports_color:
            return text
            
        color_code = self.get_ansi_color(r, g, b)
        return f"{color_code}{text}{self.get_reset_code()}"


# Initialize the Unicode rendering engine as module-level singleton
UNICODE_ENGINE = UnicodeRenderEngine.get_instance()

# Legacy compatibility functions that delegate to the engine
def get_ansi_color(r: int, g: int, b: int) -> str:
    """ğŸŒˆ Generate 24-bit ANSI color sequence.
    
    Args:
        r, g, b: RGB color components (0-255)
        
    Returns:
        ANSI escape code for foreground color
    """
    return UNICODE_ENGINE.get_ansi_color(r, g, b)


def get_edge_char(avg_grad_x: float, avg_grad_y: float, strength: float = 1.0) -> str:
    """ğŸ§© Select optimal Unicode edge representation.
    
    Args:
        avg_grad_x: Average x gradient
        avg_grad_y: Average y gradient
        strength: Edge intensity factor (0.0-1.0)
        
    Returns:
        Unicode character matching edge direction and intensity
    """
    return UNICODE_ENGINE.get_edge_char(avg_grad_x, avg_grad_y, strength)


def get_enhanced_gradient_chars() -> str:
    """ğŸ”  Provide precise Unicode density gradient.
    
    Returns:
        Character sequence from dense to sparse
    """
    return UNICODE_ENGINE.get_enhanced_gradient_chars()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸ“ Text Processing Core                                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GlyphTextEngine:
    """ğŸ”  Dimensional text transmutation system with adaptive rendering.
    
    Multi-modal text processor with progressive enhancement capabilities,
    dynamic font selection, and contextual awareness for terminal dimensions.
    """
    
    # Singleton instance pattern for memory efficiency
    _instance = None
    
    @classmethod
    def get_instance(cls) -> 'GlyphTextEngine':
        """ğŸ”„ Access singleton with memory-optimized instantiation."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def __init__(self) -> None:
        """Initialize glyph engine with adaptive font detection."""
        # Cache structures for optimal performance
        self.render_cache = {}  # Memory-efficient result caching
        self.font_cache = {}    # Font rendering pipeline cache
        self.layout_cache = {}  # Layout optimization cache
        
        # Available fonts with categorization
        self.fonts = self._detect_available_fonts()
        self.font_categories = self._categorize_fonts()
        
        # Terminal-aware rendering constraints
        self.terminal_width = SYSTEM_CONTEXT.attributes.get("terminal_width", 80)
        self.supports_color = SYSTEM_CONTEXT.capabilities.get("can_display_color", True)
        self.supports_unicode = SYSTEM_CONTEXT.capabilities.get("can_display_unicode", True)
        
        # Unicode rendering engine for advanced colorization
        self.unicode_engine = UNICODE_ENGINE
    
    def _detect_available_fonts(self) -> List[str]:
        """ğŸ“‹ Detect available figlet fonts with validation."""
        try:
            # Direct access to font list through pyfiglet
            available_fonts = pyfiglet.FigletFont.getFonts()
            
            # Filter out problematic fonts that might cause rendering issues
            blacklist = {"eftichess", "eftifont", "eftirobot", "eftiwall"}
            return [font for font in available_fonts if font not in blacklist]
        except Exception:
            # Fallback to known good fonts
            return ["standard", "slant", "small", "big", "block", "digital"]
    
    def _categorize_fonts(self) -> Dict[str, List[str]]:
        """ğŸ—‚ï¸ Organize fonts into semantic categories for intelligent selection."""
        categories = {
            "standard": ["standard", "slant", "small", "mini"],
            "bold": ["block", "big", "chunky", "epic", "doom", "larry3d"],
            "script": ["script", "slscript", "cursive"],
            "simple": ["small", "mini", "lean", "sub-zero", "tiny"],
            "tech": ["digital", "binary", "bubble", "hex", "cyberlarge", "cybermedium"],
            "stylized": ["gothic", "graffiti", "crazy", "cosmic", "isometric"],
            "symbols": ["banner3-D", "ivrit", "weird", "starwars"],
            "decorative": ["bubble", "banner", "dotmatrix", "broadway"],
        }
        
        # Create "all" category containing validated fonts
        categories["all"] = self.fonts
        
        # Add ASCII-only category for limited terminals
        categories["ascii_only"] = [
            "standard", "small", "mini", "straight", "lean", 
            "banner", "dotmatrix", "bubble"
        ]
        
        return categories
    
    def text_to_figlet(self, 
                      text: str, 
                      font: str = "standard",
                      width: Optional[int] = None,
                      color: Optional[Tuple[int, int, int]] = None,
                      justify: Literal["left", "center", "right"] = "left",
                      direction: Literal["auto", "left-to-right", "right-to-left"] = "auto") -> List[str]:
        """ğŸ”  Convert text to FIGlet art with adaptive parameters.
        
        Args:
            text: Input text to convert
            font: FIGlet font name
            width: Maximum width (None=terminal width)
            color: RGB color tuple (None=no color)
            justify: Text alignment
            direction: Text direction
            
        Returns:
            List of strings with rendered FIGlet art
        """
        # Cache key for efficient reuse
        cache_key = f"{text}:{font}:{width}:{justify}:{direction}"
        if cache_key in self.render_cache:
            result = self.render_cache[cache_key]
            # If color is different, reapply color to cached result
            if color is not None:
                return self._apply_color_to_lines(result, color)
            return result
            
        # Parameter normalization with terminal awareness
        if width is None:
            width = self.terminal_width
            
        # Font fallback for unavailable fonts
        if font not in self.fonts:
            closest_match = self._find_similar_font(font)
            if HAS_RICH:
                CONSOLE.print(f"[yellow]Font '{font}' not found, using '{closest_match}' instead[/yellow]")
            font = closest_match
            
        # Core figlet rendering with error handling
        try:
            figlet = pyfiglet.Figlet(font=font, width=width, justify=justify, direction=direction)
            rendered = figlet.renderText(text)
            result = [line.rstrip() for line in rendered.splitlines()]
            
            # Cache result for future use (limit cache size)
            if len(self.render_cache) > 100:  # Prevent unbounded growth
                self.render_cache.pop(next(iter(self.render_cache)))
            self.render_cache[cache_key] = result
            
            # Apply colorization if specified
            if color is not None:
                return self._apply_color_to_lines(result, color)
                
            return result
        except Exception as e:
            # Fallback to standard font on error
            if HAS_RICH:
                CONSOLE.print(f"[red]Error rendering font '{font}': {str(e)}[/red]")
            figlet = pyfiglet.Figlet(font="standard", width=width)
            rendered = figlet.renderText(text)
            return [line.rstrip() for line in rendered.splitlines()]
    
    def _find_similar_font(self, requested_font: str) -> str:
        """ğŸ” Find closest matching font using fuzzy string matching.
        
        Args:
            requested_font: Font name to match
            
        Returns:
            Name of closest available font
        """
        # Fast path for direct category match
        if requested_font in self.font_categories:
            category = self.font_categories[requested_font]
            return category[0] if category else "standard"
            
        # Simple edit distance for font name matching
        best_match = "standard"
        best_score = float('inf')
        
        for available_font in self.fonts:
            # Calculate edit distance using Levenshtein distance
            score = self._levenshtein_distance(requested_font.lower(), available_font.lower())
            
            # Update if better match
            if score < best_score:
                best_score = score
                best_match = available_font
                
        return best_match
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """ğŸ“ Calculate edit distance between strings for matching."""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        # Fast paths
        if len(s2) == 0:
            return len(s1)
            
        # Initialize previous row
        previous_row = range(len(s2) + 1)
        
        # Calculate edit distance
        for i, c1 in enumerate(s1):
            # Initialize current row
            current_row = [i + 1]
            
            # Fill current row
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                
                # Calculate minimum edit distance
                current_row.append(min(insertions, deletions, substitutions))
                
            # Update previous row
            previous_row = current_row
            
        return previous_row[-1]
    
    def _apply_color_to_lines(self, lines: List[str], color: Tuple[int, int, int]) -> List[str]:
        """ğŸ¨ Apply color to text lines with ANSI support detection."""
        if not self.supports_color:
            return lines
            
        r, g, b = color
        colored_lines = []
        
        for line in lines:
            if not line:  # Skip empty lines
                colored_lines.append("")
                continue
                
            colored_line = self.unicode_engine.apply_color(line, r, g, b)
            colored_lines.append(colored_line)
            
        return colored_lines
    
    def get_font_preview(self, text: str = "abc ABC", category: Optional[str] = None) -> Dict[str, List[str]]:
        """ğŸ” Generate font preview with sample text.
        
        Args:
            text: Sample text to render
            category: Font category to preview (None=all)
            
        Returns:
            Dictionary mapping font names to rendered previews
        """
        preview = {}
        fonts_to_preview = (self.font_categories.get(category, self.fonts) 
                           if category else self.fonts)
            
        # Generate previews with limited selection for performance
        for font in fonts_to_preview[:20]:  # Limit to 20 fonts max
            try:
                preview[font] = self.text_to_figlet(text, font=font, width=self.terminal_width)
            except Exception:
                # Skip problematic fonts
                continue
                
        return preview
    
    def show_font_gallery(self, text: str = "Test", category: Optional[str] = None) -> None:
        """ğŸ–¼ï¸ Display interactive font gallery with rich formatting.
        
        Args:
            text: Sample text to render
            category: Font category to display
        """
        if not HAS_RICH:
            print("Font gallery requires rich library.")
            print("Install with: pip install rich")
            return
            
        preview = self.get_font_preview(text, category)
        
        title = f"ğŸ”  Font Gallery: {category or 'All'} ({len(preview)} fonts)"
        CONSOLE.print(Panel(title, style="cyan bold"))
        
        for font_name, lines in preview.items():
            font_panel = Panel(
                "\n".join(lines), 
                title=font_name,
                border_style="blue"
            )
            CONSOLE.print(font_panel)
            
            # Add pagination if many fonts
            if not CONSOLE.is_interactive:
                CONSOLE.print("--")
    
    def render_multiline_art(self, 
                           text: str,
                           font: str = "standard",
                           color: Optional[Tuple[int, int, int]] = None,
                           width: Optional[int] = None,
                           align: Literal["left", "center", "right"] = "left") -> List[str]:
        """ğŸ­ Render multi-line text with consistent formatting.
        
        Args:
            text: Multi-line text to render
            font: FIGlet font name
            color: RGB color tuple
            width: Maximum width
            align: Text alignment
            
        Returns:
            List of strings with rendered FIGlet art
        """
        lines = text.splitlines()
        result = []
        
        if not lines:
            return result
            
        # Process each line individually
        for line in lines:
            rendered = self.text_to_figlet(
                line, 
                font=font, 
                width=width, 
                color=color,
                justify=align
            )
            result.extend(rendered)
            result.append("")  # Add separator between lines
            
        # Remove trailing empty line
        if result and not result[-1]:
            result.pop()
            
        return result
            
    def get_random_font(self, category: Optional[str] = None) -> str:
        """ğŸ² Get random font name with optional category filtering.
        
        Args:
            category: Font category to select from
            
        Returns:
            Random font name
        """
        import random
        
        if category and category in self.font_categories:
            fonts = self.font_categories[category]
        else:
            fonts = self.fonts
            
        return random.choice(fonts) if fonts else "standard"
    
    def get_font_categories(self) -> List[str]:
        """ğŸ“‹ Get available font categories for selection."""
        return list(self.font_categories.keys())
    
    def get_font_information(self, font: str) -> Dict[str, Any]:
        """â„¹ï¸ Get detailed information about a specific font."""
        if font not in self.fonts:
            closest = self._find_similar_font(font)
            return {
                "name": closest,
                "found": False,
                "similar_to": font,
                "categories": [cat for cat, fonts in self.font_categories.items() 
                               if closest in fonts],
                "sample": self.text_to_figlet("Sample", font=closest)
            }
            
        # Get font information
        return {
            "name": font,
            "found": True,
            "categories": [cat for cat, fonts in self.font_categories.items() 
                           if font in fonts],
            "sample": self.text_to_figlet("Sample", font=font)
        }


# Initialize the text engine
TEXT_ENGINE = GlyphTextEngine.get_instance()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸ­ Text Transformation Functions                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def text_to_art(
    text: str,
    font: str = "standard",
    color: Optional[Tuple[int, int, int]] = None,
    width: Optional[int] = None,
    align: Literal["left", "center", "right"] = "left"
) -> List[str]:
    """ğŸ”  Convert text to ASCII/FIGlet art.
    
    Args:
        text: Text to convert
        font: FIGlet font name
        color: RGB color tuple
        width: Maximum width
        align: Text alignment
        
    Returns:
        List of strings with rendered art
    """
    return TEXT_ENGINE.text_to_figlet(
        text=text,
        font=font,
        width=width,
        color=color,
        justify=align
    )


def render_styled_text(
    text: str,
    font: str = "standard",
    color: Optional[Union[str, Tuple[int, int, int]]] = None,
    width: Optional[int] = None,
    align: Literal["left", "center", "right"] = "left",
    add_border: bool = False,
    padding: int = 0
) -> List[str]:
    """âœ¨ Render fully styled text with optional border and padding.
    
    Args:
        text: Text to render
        font: FIGlet font name
        color: Color name or RGB tuple
        width: Maximum width
        align: Text alignment
        add_border: Add decorative border
        padding: Padding around text
        
    Returns:
        List of strings with styled text art
    """
    # Convert color name to RGB if needed
    rgb_color = None
    if color is not None:
        if isinstance(color, str):
            # Map common color names to RGB
            color_map = {
                "red": (255, 0, 0),
                "green": (0, 255, 0),
                "blue": (0, 0, 255),
                "yellow": (255, 255, 0),
                "cyan": (0, 255, 255),
                "magenta": (255, 0, 255),
                "white": (255, 255, 255),
                "black": (0, 0, 0),
                "orange": (255, 165, 0),
                "purple": (128, 0, 128),
                "pink": (255, 192, 203),
                "gray": (128, 128, 128),
            }
            rgb_color = color_map.get(color.lower(), (255, 255, 255))
        else:
            rgb_color = color
    
    # Generate base text
    lines = TEXT_ENGINE.text_to_figlet(
        text=text,
        font=font,
        width=width,
        color=rgb_color,
        justify=align
    )
    
    # Apply padding if requested
    if padding > 0:
        padded_lines = []
        # Add vertical padding
        for _ in range(padding):
            padded_lines.append("")
            
        # Add padding to each line
        h_padding = " " * padding
        for line in lines:
            padded_lines.append(f"{h_padding}{line}{h_padding}")
            
        # Add bottom padding
        for _ in range(padding):
            padded_lines.append("")
            
        lines = padded_lines
    
    # Add border if requested
    if add_border:
        return add_unicode_border(lines, rgb_color)
        
    return lines


def show_all_fonts(text: str = "Sample", category: Optional[str] = None) -> None:
    """ğŸ”  Display gallery of all available fonts.
    
    Args:
        text: Sample text to render
        category: Font category filter
    """
    TEXT_ENGINE.show_font_gallery(text, category)


def list_font_categories() -> List[str]:
    """ğŸ“‹ List available font categories for reference."""
    categories = TEXT_ENGINE.get_font_categories()
    
    if HAS_RICH:
        table = Table(title="Available Font Categories")
        table.add_column("Category", style="cyan")
        table.add_column("Description", style="green")
        table.add_column("Fonts", style="dim")
        
        for category in categories:
            fonts = TEXT_ENGINE.font_categories.get(category, [])
            count = len(fonts)
            examples = ", ".join(fonts[:3]) + ("..." if count > 3 else "")
            
            description = {
                "standard": "Common readable fonts",
                "bold": "Heavy weight striking fonts",
                "script": "Flowing cursive-style fonts",
                "simple": "Minimal, space-efficient fonts",
                "tech": "Technology and computer themed",
                "stylized": "Highly decorative unique fonts",
                "symbols": "Special character based fonts",
                "decorative": "Ornamental display fonts",
                "ascii_only": "Compatible with limited terminals",
                "all": "Complete font collection"
            }.get(category, "")
            
            table.add_row(category, description, f"{count} fonts: {examples}")
            
        CONSOLE.print(table)
    else:
        for category in categories:
            fonts = TEXT_ENGINE.font_categories.get(category, [])
            print(f"{category}: {len(fonts)} fonts")
            
    return categories


def add_unicode_border(lines: List[str], color: Optional[Tuple[int, int, int]] = None) -> List[str]:
    """ğŸ”³ Add decorative Unicode border around text.
    
    Args:
        lines: Text lines to frame
        color: Border RGB color
        
    Returns:
        Bordered text lines
    """
    if not lines:
        return []
        
    # Calculate maximum line width
    width = max((TEXT_ENGINE.unicode_engine.get_text_width(line) for line in lines), default=0)
    
    # Define border characters
    top_left = "â•”"
    top_right = "â•—"
    bottom_left = "â•š"
    bottom_right = "â•"
    horizontal = "â•"
    vertical = "â•‘"
    
    # Create horizontal borders
    top_border = f"{top_left}{horizontal * (width + 2)}{top_right}"
    bottom_border = f"{bottom_left}{horizontal * (width + 2)}{bottom_right}"
    
    # Apply color if specified
    if color is not None and TEXT_ENGINE.supports_color:
        r, g, b = color
        top_border = TEXT_ENGINE.unicode_engine.apply_color(top_border, r, g, b)
        bottom_border = TEXT_ENGINE.unicode_engine.apply_color(bottom_border, r, g, b)
        vertical_colored = TEXT_ENGINE.unicode_engine.apply_color(vertical, r, g, b)
    else:
        vertical_colored = vertical
    
    # Create result with borders
    result = [top_border]
    
    for line in lines:
        # Calculate padding needed for alignment
        line_width = TEXT_ENGINE.unicode_engine.get_text_width(line)
        padding = " " * (width - line_width)
        
        # Add vertical borders
        result.append(f"{vertical_colored} {line}{padding} {vertical_colored}")
        
    result.append(bottom_border)
    return result


def generate_text_art(
    text: str,
    mode: Literal["simple", "styled", "rainbow", "random"] = "styled",
    font: Optional[str] = None,
    color: Optional[Union[str, Tuple[int, int, int]]] = None
) -> List[str]:
    """ğŸ¨ Generate text art with intelligent defaults and mode presets.
    
    Args:
        text: Text to render
        mode: Rendering style preset
        font: FIGlet font or category
        color: Text color name or RGB values
        
    Returns:
        Rendered text art lines
    """
    # Set defaults based on mode
    if mode == "simple":
        selected_font = font or "standard"
        border = False
        selected_color = None
        
    elif mode == "styled":
        selected_font = font or "slant"
        border = True
        selected_color = color or "cyan"
        
    elif mode == "rainbow":
        import random
        selected_font = font or TEXT_ENGINE.get_random_font("bold")
        border = True
        
        # Rainbow colors for letters
        rainbow_lines = []
        rendered = TEXT_ENGINE.text_to_figlet(text, font=selected_font)
        
        # Rainbow color application (hue rotation)
        for line_idx, line in enumerate(rendered):
            if not line.strip():
                rainbow_lines.append("")
                continue
                
            rainbow_line = []
            hue_shift = line_idx * 15  # Different starting hue per line
            
            for i, char in enumerate(line):
                if char.strip():
                    # Generate color from rotating hue
                    hue = (i * 10 + hue_shift) % 360
                    r, g, b = hsv_to_rgb(hue/360, 1.0, 1.0)
                    rainbow_line.append(TEXT_ENGINE.unicode_engine.apply_color(char, r, g, b))
                else:
                    rainbow_line.append(char)
                    
            rainbow_lines.append("".join(rainbow_line))
            
        # Add border
        return add_unicode_border(rainbow_lines, (255, 255, 255))
        
    elif mode == "random":
        import random
        # Random font from any category
        selected_font = font or TEXT_ENGINE.get_random_font()
        border = random.choice([True, False])
        
        # Random vibrant color
        if color is None:
            r = random.randint(100, 255)
            g = random.randint(100, 255)
            b = random.randint(100, 255)
            selected_color = (r, g, b)
        else:
            selected_color = color
    else:
        # Default to standard mode
        selected_font = font or "standard"
        border = False
        selected_color = color
    
    # Generate the art
    return render_styled_text(
        text=text,
        font=selected_font,
        color=selected_color,
        add_border=border,
        padding=1 if border else 0
    )


def hsv_to_rgb(h: float, s: float, v: float) -> Tuple[int, int, int]:
    """ğŸŒˆ Convert HSV color to RGB components.
    
    Args:
        h: Hue (0-1)
        s: Saturation (0-1)
        v: Value (0-1)
        
    Returns:
        RGB components (0-255)
    """
    if s == 0.0:
        return (int(v * 255), int(v * 255), int(v * 255))
        
    h *= 6.0
    i = int(h)
    f = h - i
    p = v * (1.0 - s)
    q = v * (1.0 - s * f)
    t = v * (1.0 - s * (1.0 - f))
    
    if i % 6 == 0:
        rgb = (v, t, p)
    elif i % 6 == 1:
        rgb = (q, v, p)
    elif i % 6 == 2:
        rgb = (p, v, t)
    elif i % 6 == 3:
        rgb = (p, q, v)
    elif i % 6 == 4:
        rgb = (t, p, v)
    else:
        rgb = (v, p, q)
        
    return (int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255))


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸ§  Image Processing Core                                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ImageProcessor:
    """ğŸ­ Adaptive dimensional image processing system.
    
    Provides multi-algorithm edge detection, context-aware parameter tuning,
    and dynamic precision control based on system capabilities.
    """
    # Singleton instance for global access with lazy initialization
    _instance = None
    
    @classmethod
    def get_instance(cls) -> 'ImageProcessor':
        """ğŸ”„ Access singleton with memory-optimized instantiation."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def __init__(self) -> None:
        """Initialize processor with environment-aware defaults."""
        # Performance-optimized cache structures
        self._cache = {}
        
        # Image enhancement kernel cache
        self._kernels = self._initialize_kernels()
        
        # System-aware processing parameters
        self.system_tier = SYSTEM_CONTEXT.constraints.get("performance_tier", 1)
        self.optimize_memory = self.system_tier <= 1
        self.max_dim = 4096 if self.system_tier >= 2 else 2048
        
        # Detection mode defaults
        self.edge_detection_algorithms = {
            "sobel": self._sobel_edge,
            "prewitt": self._prewitt_edge,
            "scharr": self._scharr_edge,
            "laplacian": self._laplacian_edge,
            "canny": self._canny_edge
        }
        
    def _initialize_kernels(self) -> Dict[str, np.ndarray]:
        """ğŸ”§ Generate optimized convolution kernels with pre-normalization."""
        return {
            "sobel_x": np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),
            "sobel_y": np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),
            "prewitt_x": np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),
            "prewitt_y": np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),
            "scharr_x": np.array([[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]]),
            "scharr_y": np.array([[-3, -10, -3], [0, 0, 0], [3, 10, 3]]),
            "laplacian": np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]]),
            "gaussian_3x3": np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16,
            "sharpen": np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
        }
        
    def supersample_image(self, image: Image.Image, scale_factor: int) -> Image.Image:
        """ğŸ”¬ Upscale image with adaptive quality based on system tier.
        
        Args:
            image: Source PIL image
            scale_factor: Integer multiplier for dimensions
            
        Returns:
            Upscaled image using optimal resampling method
        """
        # Enforce integer scale factor
        scale = max(1, int(scale_factor))
        
        # Cache key for reuse
        cache_key = f"supersample_{id(image)}_{scale}"
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # Dimension safety check (prevent OOM)
        new_width = int(image.width * scale)
        new_height = int(image.height * scale)
        
        if new_width > self.max_dim or new_height > self.max_dim:
            # Scale down to maximum safe dimensions
            scale_factor = min(self.max_dim / image.width, self.max_dim / image.height)
            new_width = int(image.width * scale_factor)
            new_height = int(image.height * scale_factor)
            
            if HAS_RICH:
                CONSOLE.print(f"[yellow]âš ï¸ Image dimensions reduced for memory safety "
                              f"({new_width}Ã—{new_height})[/yellow]")
        
        # Select best resampling method based on system capabilities
        if self.system_tier >= 2:
            # High-quality upscaling for powerful systems
            result = image.resize((new_width, new_height), Image.BICUBIC)
        else:
            # Faster upscaling for limited systems
            result = image.resize((new_width, new_height), Image.BILINEAR)
        
        # Cache if memory optimization is off
        if not self.optimize_memory:
            self._cache[cache_key] = result
            
        return result
        
    def rgb_to_gray(self, image_array: np.ndarray) -> np.ndarray:
        """ğŸ¨â¡ï¸âšª Convert RGB to perceptually accurate grayscale.
        
        Args:
            image_array: RGB numpy array (HÃ—WÃ—3)
            
        Returns:
            Grayscale numpy array (HÃ—W)
        """
        # Fast path for already grayscale images
        if len(image_array.shape) == 2:
            return image_array
            
        # ITU-R BT.601 standard for luminance (perceptual accuracy)
        return (0.2126 * image_array[:, :, 0] +  # Red
                0.7152 * image_array[:, :, 1] +  # Green
                0.0722 * image_array[:, :, 2]).astype(np.uint8)  # Blue
                
    def enhance_image(self, 
                     image_array: np.ndarray, 
                     contrast: float = 1.0,
                     brightness: float = 0.0,
                     denoise: bool = False) -> np.ndarray:
        """ğŸ”§ Apply adaptive image enhancements for improved edge detection.
        
        Args:
            image_array: Input image as numpy array
            contrast: Contrast adjustment factor (1.0 = no change)
            brightness: Brightness adjustment (-128 to +128)
            denoise: Apply noise reduction
            
        Returns:
            Enhanced image array
        """
        result = image_array.copy()
        
        # Apply brightness adjustment
        if brightness != 0:
            brightness = np.clip(brightness, -128, 128)
            result = np.clip(result + brightness, 0, 255).astype(np.uint8)
            
        # Apply contrast adjustment
        if contrast != 1.0:
            contrast = np.clip(contrast, 0.5, 2.0)
            f = 259 * (contrast * 255 + 255) / (255 * (259 - contrast * 255))
            result = np.clip(f * (result.astype(np.float32) - 128) + 128, 0, 255).astype(np.uint8)
            
        # Apply noise reduction
        if denoise and self.system_tier >= 1:
            # Simple 3x3 gaussian filter if OpenCV isn't available
            kernel = self._kernels["gaussian_3x3"]
            
            # Fast convolution for single grayscale channel
            if len(result.shape) == 2:
                # Edge-padded boundary handling
                padded = np.pad(result, ((1, 1), (1, 1)), mode='edge')
                windows = np.lib.stride_tricks.sliding_window_view(padded, (3, 3))
                result = np.tensordot(windows, kernel, axes=([2, 3], [0, 1]))
            # For RGB images (process each channel)
            elif len(result.shape) == 3:
                for c in range(result.shape[2]):
                    padded = np.pad(result[:,:,c], ((1, 1), (1, 1)), mode='edge')
                    windows = np.lib.stride_tricks.sliding_window_view(padded, (3, 3))
                    result[:,:,c] = np.tensordot(windows, kernel, axes=([2, 3], [0, 1]))
                    
        return result.astype(np.uint8)
                
    def detect_edges(self, 
                    gray_array: np.ndarray, 
                    algorithm: str = "sobel",
                    threshold: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ğŸ”ª Multi-algorithm edge detection with adaptive thresholding.
        
        Args:
            gray_array: Grayscale image as numpy array
            algorithm: Edge detection algorithm
            threshold: Manual sensitivity threshold (None=auto)
            
        Returns:
            Tuple of (gradient magnitude, x gradient, y gradient)
        """
        # Parameter normalization
        algorithm = algorithm.lower()
        
        # Auto-threshold selection
        if threshold is None:
            # Dynamic thresholding based on image statistics
            # More detail in bright images, less in dark/noisy ones
            mean_value = gray_array.mean()
            threshold = int(40 + (mean_value / 5))
        
        # Algorithm dispatch with fallback
        if algorithm in self.edge_detection_algorithms:
            return self.edge_detection_algorithms[algorithm](gray_array, threshold)
        else:
            # Fallback to Sobel
            if HAS_RICH:
                CONSOLE.print(f"[yellow]Unknown edge algorithm '{algorithm}', using sobel[/yellow]")
            return self._sobel_edge(gray_array, threshold)
            
    def _sobel_edge(self, 
                   gray_array: np.ndarray, 
                   threshold: int = 50) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ğŸ”ª Extract directional edges using tensor-accelerated Sobel.
        
        Args:
            gray_array: Grayscale image as numpy array
            threshold: Sensitivity threshold
            
        Returns:
            Tuple of (gradient magnitude, x gradient, y gradient)
        """
        # Get cached kernels
        Kx = self._kernels["sobel_x"]
        Ky = self._kernels["sobel_y"]
        
        # Edge-padded boundary handling
        padded = np.pad(gray_array, ((1, 1), (1, 1)), mode='edge')
        
        # Memory-efficient sliding windows
        windows = np.lib.stride_tricks.sliding_window_view(padded, (3, 3))
        
        # Vectorized tensor operations (4-8Ã— faster than loops)
        grad_x = np.tensordot(windows, Kx, axes=([2, 3], [0, 1]))
        grad_y = np.tensordot(windows, Ky, axes=([2, 3], [0, 1]))
        
        # Numerically stable gradient magnitude
        grad = np.hypot(grad_x, grad_y)
        
        # Zero-safe normalization with threshold control
        if grad.max() > 0:
            # Apply threshold before normalization (detail control)
            grad = np.clip((grad / grad.max()) * 255, 0, 255)
            # Enhance edges based on threshold
            grad = np.where(grad < threshold, 0, grad)
            # Renormalize after thresholding
            if grad.max() > 0:
                grad = (grad / grad.max()) * 255
            
        return grad.astype(np.uint8), grad_x, grad_y
        
    def _prewitt_edge(self, 
                     gray_array: np.ndarray, 
                     threshold: int = 50) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ğŸ”ª Extract directional edges using Prewitt operator.
        
        Better for high-contrast images with less noise sensitivity than Sobel.
        
        Args:
            gray_array: Grayscale image as numpy array
            threshold: Sensitivity threshold
            
        Returns:
            Tuple of (gradient magnitude, x gradient, y gradient)
        """
        # Get cached kernels
        Kx = self._kernels["prewitt_x"]
        Ky = self._kernels["prewitt_y"]
        
        # Edge-padded boundary handling
        padded = np.pad(gray_array, ((1, 1), (1, 1)), mode='edge')
        
        # Memory-efficient sliding windows
        windows = np.lib.stride_tricks.sliding_window_view(padded, (3, 3))
        
        # Vectorized operations
        grad_x = np.tensordot(windows, Kx, axes=([2, 3], [0, 1]))
        grad_y = np.tensordot(windows, Ky, axes=([2, 3], [0, 1]))
        
        # Gradient magnitude
        grad = np.hypot(grad_x, grad_y)
        
        # Normalization with threshold
        if grad.max() > 0:
            grad = np.clip((grad / grad.max()) * 255, 0, 255)
            grad = np.where(grad < threshold, 0, grad)
            if grad.max() > 0:
                grad = (grad / grad.max()) * 255
            
        return grad.astype(np.uint8), grad_x, grad_y
        
    def _scharr_edge(self, 
                    gray_array: np.ndarray, 
                    threshold: int = 50) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ğŸ”ª Extract directional edges using Scharr operator.
        
        Better rotational symmetry than Sobel for enhanced diagonal edges.
        
        Args:
            gray_array: Grayscale image as numpy array
            threshold: Sensitivity threshold
            
        Returns:
            Tuple of (gradient magnitude, x gradient, y gradient)
        """
        # Get cached kernels
        Kx = self._kernels["scharr_x"]
        Ky = self._kernels["scharr_y"]
        
        # Edge-padded boundary handling
        padded = np.pad(gray_array, ((1, 1), (1, 1)), mode='edge')
        
        # Memory-efficient sliding windows
        windows = np.lib.stride_tricks.sliding_window_view(padded, (3, 3))
        
        # Vectorized operations
        grad_x = np.tensordot(windows, Kx, axes=([2, 3], [0, 1]))
        grad_y = np.tensordot(windows, Ky, axes=([2, 3], [0, 1]))
        
        # Gradient magnitude
        grad = np.hypot(grad_x, grad_y)
        
        # Normalization with threshold
        if grad.max() > 0:
            grad = np.clip((grad / grad.max()) * 255, 0, 255)
            grad = np.where(grad < threshold, 0, grad)
            if grad.max() > 0:
                grad = (grad / grad.max()) * 255
            
        return grad.astype(np.uint8), grad_x, grad_y
        
    def _laplacian_edge(self, 
                       gray_array: np.ndarray, 
                       threshold: int = 30) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ğŸ”ª Extract edges using Laplacian operator.
        
        Detects edges in all directions simultaneously, good for detailed images.
        
        Args:
            gray_array: Grayscale image as numpy array
            threshold: Sensitivity threshold
            
        Returns:
            Tuple of (gradient magnitude, x gradient, y gradient)
        """
        # Get laplacian kernel
        K = self._kernels["laplacian"]
        
        # Preprocessing for Laplacian (optional denoise)
        if self.system_tier >= 2:
            # Apply minimal Gaussian blur to reduce noise
            padded_blur = np.pad(gray_array, ((1, 1), (1, 1)), mode='edge')
            windows_blur = np.lib.stride_tricks.sliding_window_view(padded_blur, (3, 3))
            gray_array = np.tensordot(windows_blur, self._kernels["gaussian_3x3"], axes=([2, 3], [0, 1])).astype(np.uint8)
            
        # Edge-padded boundary handling
        padded = np.pad(gray_array, ((1, 1), (1, 1)), mode='edge')
        
        # Memory-efficient sliding windows
        windows = np.lib.stride_tricks.sliding_window_view(padded, (3, 3))
        
        # Apply laplacian
        grad = np.abs(np.tensordot(windows, K, axes=([2, 3], [0, 1])))
        
        # For laplacian, we need to simulate the x and y gradients
        # Use Sobel for directional components while keeping laplacian magnitude
        Kx = self._kernels["sobel_x"]
        Ky = self._kernels["sobel_y"]
        grad_x = np.tensordot(windows, Kx, axes=([2, 3], [0, 1]))
        grad_y = np.tensordot(windows, Ky, axes=([2, 3], [0, 1]))
        
        # Normalization with threshold
        if grad.max() > 0:
            grad = np.clip((grad / grad.max()) * 255, 0, 255)
            grad = np.where(grad < threshold, 0, grad)
            if grad.max() > 0:
                grad = (grad / grad.max()) * 255
            
        return grad.astype(np.uint8), grad_x, grad_y
        
    def _canny_edge(self, 
                   gray_array: np.ndarray, 
                   threshold: int = 50) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ğŸ”ª Extract edges using simplified Canny-inspired approach.
        
        Approximates Canny edge detection with non-maximum suppression.
        
        Args:
            gray_array: Grayscale image as numpy array
            threshold: Sensitivity threshold (equivalently split for high/low)
            
        Returns:
            Tuple of (gradient magnitude, x gradient, y gradient)
        """
        # Start with Sobel edge detection
        grad, grad_x, grad_y = self._sobel_edge(gray_array, 0)  # No thresholding yet
        
        # Calculate gradient directions (in radians)
        theta = np.arctan2(grad_y, grad_x)
        
        # Quantize directions to 0, 45, 90, 135 degrees
        angle = np.round(theta * (4/np.pi)) % 4
        
        # Non-maximum suppression
        suppressed = np.zeros_like(grad)
        height, width = grad.shape
        
        # Memory-optimized approach (avoids full copy operations)
        for i in range(1, height-1):
            for j in range(1, width-1):
                if grad[i, j] == 0:  # Skip zero gradients
                    continue
                    
                # Check neighbors along gradient direction
                if angle[i, j] == 0:  # Horizontal
                    neighbors = [grad[i, j-1], grad[i, j+1]]
                elif angle[i, j] == 1:  # Diagonal (45Â°)
                    neighbors = [grad[i+1, j-1], grad[i-1, j+1]]
                elif angle[i, j] == 2:  # Vertical
                    neighbors = [grad[i-1, j], grad[i+1, j]]
                else:  # Diagonal (135Â°)
                    neighbors = [grad[i-1, j-1], grad[i+1, j+1]]
                    
                # Keep pixel if it's the maximum in its direction
                if grad[i, j] >= max(neighbors[0], neighbors[1]):
                    suppressed[i, j] = grad[i, j]
        
        # Hysteresis thresholding (simplified)
        high_threshold = threshold
        low_threshold = threshold // 2
        
        # Strong edges
        strong = suppressed >= high_threshold
        
        # Weak edges
        weak = (suppressed >= low_threshold) & (suppressed < high_threshold)
        
        # Keep strong edges and weak edges connected to strong edges
        result = np.zeros_like(suppressed)
        result[strong] = 255
        
        # Simplified connectivity check for weak edges
        for i in range(1, height-1):
            for j in range(1, width-1):
                if weak[i, j]:
                    # Check if connected to a strong edge
                    if (strong[i-1:i+2, j-1:j+2]).any():
                        result[i, j] = 255
        
        # Return normalized result with simulated gradients
        return result, grad_x, grad_y
        
    def clear_cache(self) -> None:
        """ğŸ§¹ Clear internal caches to free memory."""
        self._cache.clear()


# Initialize the global processor instance
IMAGE_PROCESSOR = ImageProcessor.get_instance()

# Legacy compatibility functions that delegate to the processor
def supersample_image(image: Image.Image, scale_factor: int) -> Image.Image:
    """ğŸ”¬ Upscale image for enhanced detail extraction.
    
    Args:
        image: Source PIL image
        scale_factor: Integer multiplier for dimensions
        
    Returns:
        Upscaled image using bicubic interpolation
    """
    return IMAGE_PROCESSOR.supersample_image(image, scale_factor)


def rgb_to_gray(image_array: np.ndarray) -> np.ndarray:
    """ğŸ¨â¡ï¸âšª Convert RGB to perceptually accurate grayscale.
    
    Args:
        image_array: RGB numpy array (HÃ—WÃ—3)
        
    Returns:
        Grayscale numpy array (HÃ—W)
    """
    return IMAGE_PROCESSOR.rgb_to_gray(image_array)


def sobel_edge(gray_array: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """ğŸ”ª Extract directional edges using tensor-accelerated Sobel.
    
    Args:
        gray_array: Grayscale image as numpy array
        
    Returns:
        Tuple of (gradient magnitude, x gradient, y gradient)
    """
    return IMAGE_PROCESSOR._sobel_edge(gray_array)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸï¸ Streaming Intelligence                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class StreamEngine:
    """ğŸ¬ Multi-dimensional adaptive stream processing system.
    
    Contextually-aware video handling with dynamic performance optimization,
    temporal coherence management, and network-resilient stream acquisition.
    """
    # Cache for extracted stream URLs with TTL expiration
    _stream_cache: Dict[str, Tuple[str, float]] = {}
    _cache_ttl = 3600  # 1 hour cache validity
    
    @classmethod
    def extract_stream_url(cls, youtube_url: str, resolution: Optional[int] = None) -> str:
        """ğŸ“¡ Extract adaptive-quality stream URL with resilient retry logic.
        
        Args:
            youtube_url: YouTube video URL or ID
            resolution: Preferred vertical resolution (None=auto)
            
        Returns:
            Direct streaming URL with optimal format selection
            
        Raises:
            SystemExit: If extraction fails after recovery attempts
        """
        # Dependency verification
        if not HAS_YT_DLP:
            print("ğŸš« Missing: yt_dlp library")
            print("ğŸ’¡ Install with: pip install yt-dlp")
            sys.exit(1)
        
        # Cache lookup for performance (avoid repeated API calls)
        cache_key = f"{youtube_url}:{resolution}"
        now = time.time()
        if cache_key in cls._stream_cache:
            url, timestamp = cls._stream_cache[cache_key]
            if now - timestamp < cls._cache_ttl:
                return url
        
        # Auto-select resolution based on system capabilities
        if resolution is None:
            system_tier = SYSTEM_CONTEXT.constraints.get("performance_tier", 1)
            terminal_height = SYSTEM_CONTEXT.attributes.get("terminal_height", 24)
            
            # Calculate optimal resolution (higher for powerful systems)
            if system_tier >= 2 and terminal_height > 40:
                resolution = 720
            else:
                resolution = 480
        
        # Format selection based on ideal performance
        format_spec = f'best[height<={resolution}][ext=mp4]'
        
        # YouTube-DL options with fallback formats
        ydl_opts = {
            'format': format_spec,
            'quiet': True,
            'skip_download': True,
            'no_warnings': True,
            'socket_timeout': 10,
        }
        
        # Progressive retry logic with failure analysis
        retry_count = 0
        max_retries = 2
        
        while retry_count <= max_retries:
            try:
                if HAS_RICH:
                    status_message = "ğŸ” Extracting stream data" + ("." * retry_count)
                    with CONSOLE.status(status_message, spinner="dots"):
                        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                            info = ydl.extract_info(youtube_url, download=False)
                            video_url = info.get("url", None)
                            title = info.get("title", "Unknown video")
                            duration = info.get("duration")
                            
                            # Information display
                            duration_str = f" ({duration//60}:{duration%60:02d})" if duration else ""
                            CONSOLE.log(f"âœ“ Found: {title}{duration_str}")
                else:
                    print("ğŸ” Extracting stream...", end="", flush=True)
                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                        info = ydl.extract_info(youtube_url, download=False)
                        video_url = info.get("url", None)
                        print(" âœ“")
                    
                if not video_url:
                    # Try format fallbacks on failure
                    if retry_count < max_retries:
                        # Progressive format fallback strategy
                        if retry_count == 0:
                            ydl_opts['format'] = 'best[height<=360]'
                        else:
                            ydl_opts['format'] = 'worst'
                        retry_count += 1
                        continue
                    
                    print("ğŸš« Stream extraction failed")
                    print("ğŸ’¡ Check if video is private or geo-restricted")
                    sys.exit(1)
                
                # Cache successful extraction for performance
                cls._stream_cache[cache_key] = (video_url, now)
                
                # Clean old cache entries
                cls._clean_cache()
                
                return video_url
                
            except Exception as e:
                # Error analysis with informative diagnostics
                if retry_count < max_retries:
                    if "429" in str(e):
                        error_msg = "Rate limit exceeded"
                    elif "403" in str(e):
                        error_msg = "Access forbidden"
                    elif "Blocked by YouTube" in str(e):
                        error_msg = "Regional restriction"
                    elif "not available in your country" in str(e):
                        error_msg = "Geo-restricted content"
                    else:
                        error_msg = f"Connection error: {e}"
                    
                    # Silent retry with adjusted parameters
                    retry_count += 1
                    # Exponential backoff
                    time.sleep(1 * (2**retry_count))
                    continue
                else:
                    print(f"ğŸš« YouTube error: {e}")
                    print("ğŸ’¡ Check network connection and URL validity")
                    sys.exit(1)
    
    @classmethod
    def _clean_cache(cls) -> None:
        """ğŸ§¹ Clear expired cache entries with minimal overhead."""
        now = time.time()
        # Only clean if cache is becoming large
        if len(cls._stream_cache) > 20:
            expired_keys = [k for k, (_, ts) in cls._stream_cache.items() 
                          if now - ts > cls._cache_ttl]
            for k in expired_keys:
                del cls._stream_cache[k]
                
    @classmethod
    def process_video_stream(
        cls,
        source: Union[str, int],
        scale_factor: int = 2,
        block_width: int = 8,
        block_height: int = 8,
        edge_threshold: int = 50,
        gradient_str: Optional[str] = None,
        color: bool = True,
        fps: int = 15,
        enhanced_edges: bool = True,
        show_stats: bool = True,
        adaptive_quality: bool = True,
        border: bool = True
    ) -> None:
        """ğŸ¬ Real-time Unicode stream processor with adaptive performance.
        
        Args:
            source: File path, YouTube URL, or camera index
            scale_factor: Detail enhancement factor
            block_width: Character cell width
            block_height: Character cell height
            edge_threshold: Edge sensitivity
            gradient_str: Custom character gradient
            color: Enable ANSI colors
            fps: Target frames per second
            enhanced_edges: Use advanced edge characters
            show_stats: Display performance overlay
            adaptive_quality: Auto-adjust quality for performance
            border: Add decorative frame border
        """
        # Dependency validation
        if not HAS_CV2:
            print("ğŸš« Missing: OpenCV library")
            print("ğŸ’¡ Install with: pip install opencv-python")
            sys.exit(1)
        
        # Parameter validation and normalization
        if isinstance(source, str) and source.startswith("http") and (
           "youtube.com" in source or "youtu.be" in source):
            stream_url = cls.extract_stream_url(source)
        else:
            stream_url = source

        # Initialize capture with progressive retry
        capture_success = False
        retry_attempts = 0
        
        while not capture_success and retry_attempts < 3:
            if HAS_RICH:
                with CONSOLE.status("ğŸ”Œ Initializing stream...", spinner="dots"):
                    cap = cv2.VideoCapture(stream_url)
                    if cap.isOpened():
                        capture_success = True
                        # Get video metadata with validation
                        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        video_fps = cap.get(cv2.CAP_PROP_FPS)
                        if video_fps <= 0:  # Handle invalid FPS
                            video_fps = 30.0  # Sensible default
                        CONSOLE.log(f"âœ“ Stream ready: {width}Ã—{height} at {video_fps:.1f}fps")
                    else:
                        retry_attempts += 1
                        if retry_attempts < 3:
                            CONSOLE.log(f"âš ï¸ Connection attempt {retry_attempts} failed, retrying...")
                            time.sleep(1)
                        else:
                            CONSOLE.print("âŒ Stream initialization failed", style="bold red")
                            sys.exit(1)
            else:
                print(f"ğŸ”Œ Connecting to stream...{' ' * retry_attempts}", end="", flush=True)
                cap = cv2.VideoCapture(stream_url)
                if cap.isOpened():
                    capture_success = True
                    print(" âœ“")
                else:
                    retry_attempts += 1
                    if retry_attempts < 3:
                        print(" âš ï¸")
                        print(f"Retrying ({retry_attempts}/3)...")
                        time.sleep(1)
                    else:
                        print(" âŒ")
                        print("ğŸš« Stream connection failed")
                        print("ğŸ’¡ Check path, codec support, and permissions")
                        sys.exit(1)

        # Get optimal rendering parameters from system context
        system_params = SYSTEM_CONTEXT.get_optimized_parameters()
        terminal_width = SYSTEM_CONTEXT.attributes.get("terminal_width", 80)
        terminal_height = SYSTEM_CONTEXT.attributes.get("terminal_height", 24)
        
        # Adjust settings based on system capabilities if adaptive
        if adaptive_quality:
            # Apply system-specific optimizations
            scale_factor = min(scale_factor, system_params.get("scale_factor", scale_factor))
            if SYSTEM_CONTEXT.constraints.get("performance_tier", 1) <= 1:
                block_width = max(block_width, 10)  # Larger blocks for lower-end systems
                block_height = max(block_height, 10)

        # Performance tracking with ring buffer for stable metrics
        max_tracking_samples = 30
        frame_duration = 1.0 / fps
        frames_processed = 0
        start_time = time.time()
        dropped_frames = 0
        
        # Ring buffer for smoother statistics
        render_times = collections.deque(maxlen=max_tracking_samples)
        fps_values = collections.deque(maxlen=max_tracking_samples)
        
        # Initialize gradient with fallback
        if gradient_str is None:
            gradient_str = get_enhanced_gradient_chars()
        
        # Prepare display area
        display_width = terminal_width
        display_height = terminal_height
        
        # Border characters and style
        if border and UNICODE_ENGINE.supports_unicode:
            top_left = "â•”"
            top_right = "â•—"
            bottom_left = "â•š"
            bottom_right = "â•"
            horizontal = "â•"
            vertical = "â•‘"
        else:
            # ASCII fallback
            border = False  # Disable for ASCII terminals
        
        # Informative header
        if HAS_RICH:
            CONSOLE.print("ğŸ¬ [cyan]Dimensional stream active[/cyan] â€¢ [bold]Ctrl+C to exit[/bold]")
        else:
            print("ğŸ¬ Dimensional stream active â€¢ Ctrl+C to exit")
        
        # Calculate frame dimensions
        aspect_ratio = cap.get(cv2.CAP_PROP_FRAME_WIDTH) / cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
        frame_width = min(terminal_width - (4 if border else 0), 
                          int((terminal_height - (6 if show_stats else 0)) * aspect_ratio * 0.45))
        
        # Calculate optimal block dimensions for aspect ratio preservation
        optimal_width = min(block_width, max(4, int(frame_width / (cap.get(cv2.CAP_PROP_FRAME_WIDTH) / block_width))))
        optimal_height = min(block_height, max(2, int(frame_width / aspect_ratio / (cap.get(cv2.CAP_PROP_FRAME_HEIGHT) / block_height))))
        
        # Dynamic adaptive quality management
        adaptive_params = {
            'scale': scale_factor,
            'width': optimal_width,
            'height': optimal_height,
            'threshold': edge_threshold,
            'frames_since_adjustment': 0,
            'quality_level': 2,  # 0=lowest, 4=highest
        }
        
        # Quality adjustment thresholds (ms/frame)
        quality_thresholds = {
            'reduce': frame_duration * 1000 * 0.9,  # 90% of frame budget
            'improve': frame_duration * 1000 * 0.6,  # 60% of frame budget
        }
        
        try:
            # Process frames in main loop
            last_fps_update = time.time()
            current_fps = fps
            adjust_interval = 15  # Frames between quality adjustments
            
            while True:
                frame_start = time.time()
                
                # Read frame with timeout and error handling
                read_success = False
                try:
                    ret, frame = cap.read()
                    read_success = ret
                except Exception as e:
                    # Log error but continue if possible
                    if HAS_RICH:
                        CONSOLE.log(f"âš ï¸ Frame read error: {e}")
                    read_success = False
                
                if not read_success:
                    # Handle end of stream or read error
                    if isinstance(source, str) and source.startswith("http"):
                        # Might be a temporary network issue, retry
                        time.sleep(0.5)
                        cap.release()
                        cap = cv2.VideoCapture(stream_url)
                        if not cap.isOpened():
                            break  # Give up if reconnection fails
                        continue
                    else:
                        break  # End of file or unrecoverable error
                
                # Convert colorspace with error handling
                try:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(frame_rgb)
                except Exception as e:
                    if HAS_RICH and frames_processed > 0:
                        CONSOLE.log(f"âš ï¸ Frame conversion error: {e}")
                    continue  # Skip problematic frame
                
                # Generate art with timing
                render_start = time.time()
                
                # Apply current adaptive parameters
                unicode_art = image_to_unicode_art(
                    pil_image,
                    scale_factor=adaptive_params['scale'],
                    block_width=adaptive_params['width'],
                    block_height=adaptive_params['height'],
                    edge_threshold=adaptive_params['threshold'],
                    gradient_str=gradient_str,
                    color=color,
                    enhanced_edges=enhanced_edges
                )
                
                render_time = time.time() - render_start
                render_times.append(render_time * 1000)  # Store in ms
                
                # Build display frame with optional border and stats
                display_frame = []
                frame_width = len(unicode_art[0]) if unicode_art else 0
                
                # Add top border if enabled
                if border:
                    # Title bar with stream info
                    title = f" {os.path.basename(str(source))} "
                    padding = frame_width - len(title)
                    left_pad = padding // 2
                    right_pad = padding - left_pad
                    
                    top_border = f"{top_left}{horizontal * left_pad}{title}{horizontal * right_pad}{top_right}"
                    display_frame.append(top_border)
                
                # Add unicode art content
                display_frame.extend(unicode_art)
                
                # Add performance stats if enabled
                if show_stats and frames_processed > 0:
                    # Calculate stats for overlay
                    avg_render = sum(render_times) / len(render_times)
                    elapsed_total = time.time() - start_time
                    actual_fps = current_fps
                    
                    # Stats display
                    if border:
                        # Bottom border with embedded stats
                        display_frame.append(f"{bottom_left}{horizontal * frame_width}{bottom_right}")
                        
                        stats_line = (
                            f"FPS: {actual_fps:.1f} | "
                            f"Render: {avg_render:.1f}ms | "
                            f"Quality: {adaptive_params['quality_level']}/4 | "
                            f"{'â—' * adaptive_params['quality_level']}{'â—‹' * (4-adaptive_params['quality_level'])}"
                        )
                        
                        if len(stats_line) > frame_width:
                            stats_line = stats_line[:frame_width-3] + "..."
                        else:
                            stats_line = stats_line + " " * (frame_width - len(stats_line))
                            
                        display_frame.append(f"{vertical} {stats_line} {vertical}")
                        display_frame.append(f"{bottom_left}{horizontal * (frame_width+2)}{bottom_right}")
                    else:
                        # Simple stats footer without border
                        stats_line = f"FPS: {actual_fps:.1f} | Render: {avg_render:.1f}ms | Q: {adaptive_params['quality_level']}/4"
                        display_frame.append(stats_line)
                
                # Clear screen and display
                sys.stdout.write("\033[H\033[J")  # Clear screen with ANSI escape
                
                # Optimize writes (single operation is faster)
                sys.stdout.write("\n".join(display_frame) + "\n")
                sys.stdout.flush()
                
                # Performance tracking
                frames_processed += 1
                adaptive_params['frames_since_adjustment'] += 1
                
                # Update FPS calculation once per second for stability
                if time.time() - last_fps_update >= 1.0:
                    fps_interval = time.time() - last_fps_update
                    frames_in_interval = max(1, adaptive_params['frames_since_adjustment'])
                    current_fps = frames_in_interval / fps_interval
                    fps_values.append(current_fps)
                    last_fps_update = time.time()
                    adaptive_params['frames_since_adjustment'] = 0
                
                # Adaptive quality management - adjust based on performance
                if adaptive_quality and frames_processed > 5 and adaptive_params['frames_since_adjustment'] >= adjust_interval:
                    avg_render_time = sum(render_times) / len(render_times)
                    
                    # Adjust quality based on render time vs frame budget
                    if avg_render_time > quality_thresholds['reduce'] and adaptive_params['quality_level'] > 0:
                        # Reduce quality - we're not meeting frame timing
                        adaptive_params['quality_level'] = max(0, adaptive_params['quality_level'] - 1)
                        
                        # Apply quality reduction strategy
                        if adaptive_params['quality_level'] <= 1:
                            adaptive_params['scale'] = max(1, scale_factor - 1)
                            adaptive_params['width'] = optimal_width + 2
                            adaptive_params['height'] = optimal_height + 1
                        elif adaptive_params['quality_level'] == 2:
                            adaptive_params['width'] = optimal_width + 1
                            adaptive_params['height'] = optimal_height
                            
                    elif avg_render_time < quality_thresholds['improve'] and adaptive_params['quality_level'] < 4:
                        # Increase quality - we have performance headroom
                        adaptive_params['quality_level'] = min(4, adaptive_params['quality_level'] + 1)
                        
                        # Apply quality improvement strategy
                        if adaptive_params['quality_level'] >= 3:
                            adaptive_params['scale'] = min(scale_factor + 1, 4)
                            adaptive_params['width'] = max(4, optimal_width - 1)
                            adaptive_params['height'] = max(2, optimal_height - 1)
                        elif adaptive_params['quality_level'] == 2:
                            adaptive_params['width'] = optimal_width
                            adaptive_params['height'] = optimal_height
                    
                    # Reset adjustment counter
                    adaptive_params['frames_since_adjustment'] = 0
                
                # Frame timing control with drift compensation
                elapsed = time.time() - frame_start
                sleep_time = frame_duration - elapsed
                
                if sleep_time > 0:
                    time.sleep(sleep_time)
                elif sleep_time < -0.1:  # >100ms behind
                    dropped_frames += 1
                    
        except KeyboardInterrupt:
            # Clean exit with full stats report
            sys.stdout.write("\033[H\033[J")  # Clear screen
            print("\nğŸ‘‹ Stream terminated")
            
            # Detailed performance report
            duration = time.time() - start_time
            actual_fps = frames_processed / duration
            avg_render = sum(render_times) / len(render_times) if render_times else 0
            
            print(f"\nğŸ“Š Performance summary:")
            print(f"  â€¢ Frames processed: {frames_processed}")
            print(f"  â€¢ Runtime: {duration:.2f}s")
            print(f"  â€¢ Effective rate: {actual_fps:.2f} fps")
            print(f"  â€¢ Average render: {avg_render:.1f}ms/frame")
            print(f"  â€¢ Final quality level: {adaptive_params['quality_level']}/4")
            
            if dropped_frames > 0:
                drop_percentage = (dropped_frames/frames_processed*100)
                print(f"  â€¢ Dropped frames: {dropped_frames} ({drop_percentage:.1f}%)")
                if drop_percentage > 10:
                    print(f"  ğŸ’¡ Tip: For smoother playback, try:")
                    print(f"     - Reducing scale_factor (current: {scale_factor})")
                    print(f"     - Increasing block size (current: {block_width}Ã—{block_height})")
                    print(f"     - Using a lower resolution source")
            
        except Exception as e:
            # Structured error report with contextual information
            import traceback
            sys.stdout.write("\033[H\033[J")  # Clear screen
            print(f"\n\nğŸš« Stream error: {str(e)}")
            
            if HAS_RICH:
                CONSOLE.print_exception(show_locals=False, word_wrap=True, width=terminal_width)
            else:
                traceback.print_exc()
                
            print(f"\nğŸ’¡ Context: Processing frame {frames_processed}")
            
        finally:
            # Resource cleanup with verification
            try:
                cap.release()
            except:
                pass


# Legacy compatibility function wrappers
def get_video_stream_url(youtube_url: str) -> str:
    """ğŸ“º Extract direct video stream URL.
    
    Args:
        youtube_url: YouTube video URL
        
    Returns:
        Direct streaming URL
        
    Raises:
        SystemExit: If extraction fails
    """
    return StreamEngine.extract_stream_url(youtube_url)


def process_video_stream(
    source: Union[str, int],
    scale_factor: int = 2,
    block_width: int = 8,
    block_height: int = 8,
    edge_threshold: int = 50,
    gradient_str: Optional[str] = None,
    color: bool = True,
    fps: int = 15,
    enhanced_edges: bool = True,
    show_stats: bool = True,
    adaptive_quality: bool = True,
    border: bool = True
) -> None:
    """ğŸ¬ Real-time Unicode video renderer with dimensional awareness.
    
    Args:
        source: File path, YouTube URL, or camera index
        scale_factor: Detail enhancement factor
        block_width: Character cell width in pixels
        block_height: Character cell height in pixels
        edge_threshold: Edge sensitivity threshold
        gradient_str: Custom character gradient sequence
        color: Enable ANSI terminal colors
        fps: Target frames per second
        enhanced_edges: Use advanced directional edge characters
        show_stats: Display performance metrics overlay
        adaptive_quality: Dynamically adjust parameters for target FPS
        border: Add decorative frame border around output
    """
    return StreamEngine.process_video_stream(
        source=source,
        scale_factor=scale_factor,
        block_width=block_width,
        block_height=block_height,
        edge_threshold=edge_threshold,
        gradient_str=gradient_str,
        color=color,
        fps=fps,
        enhanced_edges=enhanced_edges,
        show_stats=show_stats,
        adaptive_quality=adaptive_quality,
        border=border
    )


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸŒ€ Dimensional Transmutation Engine                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def image_to_unicode_art(
    pil_image: Image.Image,
    scale_factor: int = 2,
    block_width: int = 8,
    block_height: int = 8,
    edge_threshold: int = 50,
    gradient_str: Optional[str] = None,
    color: bool = True,
    enhanced_edges: bool = True,
    algorithm: str = "sobel",
    dithering: bool = False
) -> List[str]:
    """ğŸ­ Transform image into dimensional Unicode art.
    
    Args:
        pil_image: Source PIL image
        scale_factor: Supersampling multiplier
        block_width: Character cell width in pixels
        block_height: Character cell height in pixels
        edge_threshold: Edge detection sensitivity (0-255)
        gradient_str: Custom character density gradient
        color: Apply ANSI color to output
        enhanced_edges: Use advanced edge representation
        algorithm: Edge detection algorithm
        dithering: Apply error diffusion dithering
        
    Returns:
        List of strings with Unicode art rows
    """
    # Default gradient with adaptive selection
    if gradient_str is None:
        # Choose appropriate gradient based on terminal capabilities
        if SYSTEM_CONTEXT.capabilities.get("can_display_unicode", True):
            gradient_str = get_enhanced_gradient_chars()
        else:
            # ASCII-only fallback
            gradient_str = UNICODE_ENGINE.character_maps["full_gradients"]["ascii_art"]

    # Detail preservation via supersampling
    image_sup = supersample_image(pil_image, scale_factor)
    image_array = np.array(image_sup)
    
    # Apply contrast enhancement if enabled
    if SYSTEM_CONTEXT.constraints.get("performance_tier", 1) >= 2:
        # High-end systems get enhanced preprocessing
        image_array = IMAGE_PROCESSOR.enhance_image(
            image_array, 
            contrast=1.2,
            brightness=5.0,
            denoise=True
        )
    
    # Edge detection pipeline with algorithm flexibility
    gray_array = rgb_to_gray(image_array)
    edge_result = IMAGE_PROCESSOR.detect_edges(
        gray_array, 
        algorithm=algorithm.lower(), 
        threshold=edge_threshold
    )
    edge_magnitude, grad_x, grad_y = edge_result
    
    # Output grid dimensions with boundary check
    height, width = gray_array.shape
    cols = max(1, width // block_width)
    rows = max(1, height // block_height)
    
    # Prepare dithering state if enabled
    dither_errors = np.zeros((height, width)) if dithering else None
    
    # Generate art line by line
    output_lines = []
    for i in range(rows):
        line = []
        for j in range(cols):
            # Extract current block with bounds checking
            y_start = i * block_height
            y_end = min((i + 1) * block_height, height)
            x_start = j * block_width
            x_end = min((j + 1) * block_width, width)
            
            # Color extraction for colorization
            color_block = image_array[y_start:y_end, x_start:x_end, :]
            avg_color = color_block.mean(axis=(0, 1))
            r, g, b = avg_color.astype(int)
            
            # Perceptual luminance calculation (ITU-R BT.709)
            brightness = 0.2126 * r + 0.7152 * g + 0.0722 * b
            
            # Apply dithering if enabled
            if dithering and dither_errors is not None:
                # Extract errors for this block
                error_block = dither_errors[y_start:y_end, x_start:x_end]
                # Apply accumulated error to brightness (with limits)
                error_avg = error_block.mean()
                brightness = np.clip(brightness + error_avg, 0, 255)
            
            # Edge analysis with threshold adaptation
            edge_block = edge_magnitude[y_start:y_end, x_start:x_end]
            avg_edge = edge_block.mean()
            
            # Character selection with edge intelligence
            if avg_edge > edge_threshold:
                # Edge detected - use directional character
                block_grad_x = grad_x[y_start:y_end, x_start:x_end]
                block_grad_y = grad_y[y_start:y_end, x_start:x_end]
                avg_grad_x = block_grad_x.mean()
                avg_grad_y = block_grad_y.mean()
                
                edge_strength = min(avg_edge / 255, 1.0)
                char = get_edge_char(avg_grad_x, avg_grad_y, 
                                    edge_strength if enhanced_edges else 0.5)
            else:
                # No edge - use gradient character
                normalized_brightness = brightness / 255
                
                # Update dithering error if enabled
                if dithering and dither_errors is not None:
                    # Calculate quantization error
                    index = min(int((1.0 - normalized_brightness) * (len(gradient_str) - 1)), len(gradient_str) - 1)
                    quantized = (1.0 - index / (len(gradient_str) - 1)) * 255
                    quant_error = brightness - quantized
                    
                    # Distribute error (Floyd-Steinberg)
                    if j < cols - 1:  # Right
                        dither_errors[y_start:y_end, x_end:min(x_end+block_width, width)] += quant_error * 0.4375
                    if i < rows - 1:  # Down
                        dither_errors[y_end:min(y_end+block_height, height), x_start:x_end] += quant_error * 0.3125
                    if i < rows - 1 and j > 0:  # Down-left
                        dither_errors[y_end:min(y_end+block_height, height), 
                                    max(0, x_start-block_width):x_start] += quant_error * 0.1875
                    if i < rows - 1 and j < cols - 1:  # Down-right
                        dither_errors[y_end:min(y_end+block_height, height), 
                                    x_end:min(x_end+block_width, width)] += quant_error * 0.0625
                
                # Select character from gradient based on brightness
                index = min(int((1.0 - normalized_brightness) * (len(gradient_str) - 1)), len(gradient_str) - 1)
                char = gradient_str[index]
            
            # Apply color with system adaptation
            if color and SYSTEM_CONTEXT.capabilities.get("can_display_color", True):
                # Efficient color construction
                line.append(f"{get_ansi_color(r, g, b)}{char}\033[0m")
            else:
                line.append(char)
        
        output_lines.append("".join(line))
    
    return output_lines


def generate_unicode_art(
    image_path: Union[str, Path],
    scale_factor: int = 2,
    block_width: int = 8,
    block_height: int = 8,
    edge_threshold: int = 50,
    gradient_str: Optional[str] = None,
    color: bool = True,
    enhanced_edges: bool = True,
    algorithm: str = "sobel",
    dithering: bool = False,
    auto_scale: bool = True
) -> List[str]:
    """ğŸ“¸ Process image into dimensional Unicode art.
    
    Args:
        image_path: Path to source image
        scale_factor: Detail enhancement factor
        block_width: Character cell width
        block_height: Character cell height
        edge_threshold: Edge sensitivity
        gradient_str: Custom character gradient
        color: Enable ANSI colors
        enhanced_edges: Use advanced edge characters
        algorithm: Edge detection algorithm
        dithering: Apply error diffusion dithering
        auto_scale: Automatically adapt output to terminal
        
    Returns:
        List of strings with Unicode art rows
        
    Raises:
        SystemExit: If image loading fails
    """
    start_time = time.time()
    
    # Adaptive system parameter optimization
    if auto_scale:
        # Get system-optimal parameters
        params = SYSTEM_CONTEXT.get_optimized_parameters()
        scale_factor = min(scale_factor, params.get("scale_factor", scale_factor))
        
        # Terminal size adaptation
        terminal_width = SYSTEM_CONTEXT.attributes.get("terminal_width", 80)
        # Each character is roughly half as tall as wide in most terminals
        aspect_ratio_factor = 0.5
        
        # Apply automatic size limitation
        if terminal_width < 100:
            # Small terminal adjustment
            block_width = max(block_width, 10)
            block_height = max(block_height, int(block_width * aspect_ratio_factor))
    
    # Load and process image
    try:
        if HAS_RICH:
            with CONSOLE.status("ğŸ“¥ Loading image...", spinner="dots"):
                # Error handling and format normalization
                try:
                    image = Image.open(image_path).convert("RGB")
                    CONSOLE.log(f"âœ“ Image loaded: {image.width}Ã—{image.height} px")
                except (IOError, FileNotFoundError) as e:
                    CONSOLE.print(f"[red]ğŸš« Error opening image:[/red] {e}")
                    sys.exit(1)
                except Exception as e:
                    CONSOLE.print(f"[red]ğŸš« Unexpected error:[/red] {e}")
                    sys.exit(1)
        else:
            print(f"ğŸ“¥ Loading image: {image_path}...")
            try:
                image = Image.open(image_path).convert("RGB")
                print(f"âœ“ Image loaded: {image.width}Ã—{image.height} px")
            except Exception as e:
                print(f"ğŸš« Image error: {e}")
                print("ğŸ’¡ Check path, permissions, and format support")
                sys.exit(1)
    except Exception as e:
        print(f"ğŸš« Image error: {e}")
        print("ğŸ’¡ Check path, permissions, and format support")
        sys.exit(1)
    
    # Image size safety check
    max_dim = 8192 if SYSTEM_CONTEXT.constraints.get("performance_tier", 1) >= 2 else 4096
    if image.width > max_dim or image.height > max_dim:
        if HAS_RICH:
            CONSOLE.print(f"[yellow]âš ï¸ Image is very large, resizing for safety[/yellow]")
        else:
            print("âš ï¸ Image is very large, resizing for safety")
            
        # Maintain aspect ratio
        aspect = image.width / image.height
        if image.width > image.height:
            new_width = max_dim
            new_height = int(new_width / aspect)
        else:
            new_height = max_dim
            new_width = int(new_height * aspect)
            
        image = image.resize((new_width, new_height), Image.LANCZOS)
    
    # Process image
    if HAS_RICH:
        with CONSOLE.status("ğŸ§  Processing...", spinner="dots"):
            result = image_to_unicode_art(
                image,
                scale_factor=scale_factor,
                block_width=block_width,
                block_height=block_height,
                edge_threshold=edge_threshold,
                gradient_str=gradient_str,
                color=color,
                enhanced_edges=enhanced_edges,
                algorithm=algorithm,
                dithering=dithering
            )
            elapsed = time.time() - start_time
            CONSOLE.log(f"âœ¨ Generated in {elapsed:.2f}s")
    else:
        print("ğŸ§  Processing image...")
        result = image_to_unicode_art(
            image,
            scale_factor=scale_factor,
            block_width=block_width,
            block_height=block_height,
            edge_threshold=edge_threshold,
            gradient_str=gradient_str,
            color=color,
            enhanced_edges=enhanced_edges,
            algorithm=algorithm,
            dithering=dithering
        )
        elapsed = time.time() - start_time
        print(f"âœ¨ Generated in {elapsed:.2f}s")
        
    return result


class ArtTransformer:
    """ğŸ¨ Multi-dimensional art transformation pipeline.
    
    Provides fluent interface for progressive image transformations
    with context-aware parameter tuning and optimization paths.
    """
    
    def __init__(self, source: Union[str, Path, Image.Image]) -> None:
        """Initialize transformer with source image.
        
        Args:
            source: Image path or PIL Image object
        """
        self.image = None
        self.options = {
            "scale_factor": 2,
            "block_width": 8,
            "block_height": 8, 
            "edge_threshold": 50,
            "gradient_str": None,
            "color": SYSTEM_CONTEXT.capabilities.get("can_display_color", True),
            "enhanced_edges": True,
            "algorithm": "sobel",
            "dithering": False,
            "output_format": "ansi"
        }
        
        # Load image source
        if isinstance(source, Image.Image):
            self.image = source
        else:
            try:
                self.image = Image.open(source).convert("RGB")
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[red]ğŸš« Error loading image:[/red] {e}")
                else:
                    print(f"ğŸš« Error loading image: {e}")
                raise
    
    def with_scale(self, factor: int) -> 'ArtTransformer':
        """Set supersampling scale factor.
        
        Args:
            factor: Detail enhancement factor (1-4)
            
        Returns:
            Self for chaining
        """
        self.options["scale_factor"] = max(1, min(4, factor))
        return self
    
    def with_block_size(self, width: int, height: Optional[int] = None) -> 'ArtTransformer':
        """Set character block dimensions.
        
        Args:
            width: Block width in pixels
            height: Block height (defaults to width)
            
        Returns:
            Self for chaining
        """
        self.options["block_width"] = max(2, width)
        self.options["block_height"] = max(2, height or width)
        return self
    
    def with_edge_detection(self, 
                          threshold: int = 50, 
                          algorithm: str = "sobel",
                          enhanced: bool = True) -> 'ArtTransformer':
        """Configure edge detection parameters.
        
        Args:
            threshold: Edge sensitivity (0-255)
            algorithm: Detection algorithm
            enhanced: Use detailed edge characters
            
        Returns:
            Self for chaining
        """
        self.options["edge_threshold"] = threshold
        self.options["algorithm"] = algorithm
        self.options["enhanced_edges"] = enhanced
        return self
    
    def with_gradient(self, gradient: str) -> 'ArtTransformer':
        """Set custom gradient character sequence.
        
        Args:
            gradient: Character sequence from dense to sparse
            
        Returns:
            Self for chaining
        """
        self.options["gradient_str"] = gradient
        return self
    
    def with_preset(self, preset: Literal["default", "detailed", "fast", "minimal"]) -> 'ArtTransformer':
        """Apply predefined parameter preset.
        
        Args:
            preset: Named parameter configuration
            
        Returns:
            Self for chaining
        """
        if preset == "detailed":
            self.options.update({
                "scale_factor": 3,
                "block_width": 4,
                "block_height": 4,
                "edge_threshold": 40,
                "algorithm": "scharr",
                "enhanced_edges": True,
                "dithering": True
            })
        elif preset == "fast":
            self.options.update({
                "scale_factor": 1,
                "block_width": 12,
                "block_height": 12,
                "edge_threshold": 60,
                "algorithm": "sobel",
                "enhanced_edges": False,
                "dithering": False
            })
        elif preset == "minimal":
            self.options.update({
                "scale_factor": 1,
                "block_width": 8,
                "block_height": 8,
                "edge_threshold": 80,
                "algorithm": "sobel",
                "enhanced_edges": False,
                "color": False,
                "dithering": False,
                "gradient_str": " .:;+=xX$&#@"
            })
            
        return self
    
    def with_color(self, enabled: bool = True) -> 'ArtTransformer':
        """Enable or disable ANSI color output.
        
        Args:
            enabled: Color state
            
        Returns:
            Self for chaining
        """
        self.options["color"] = enabled and SYSTEM_CONTEXT.capabilities.get("can_display_color", True)
        return self
    
    def with_dithering(self, enabled: bool = True) -> 'ArtTransformer':
        """Enable or disable Floyd-Steinberg dithering.
        
        Args:
            enabled: Dithering state
            
        Returns:
            Self for chaining
        """
        self.options["dithering"] = enabled
        return self
    
    def optimize_for_terminal(self) -> 'ArtTransformer':
        """Adapt parameters to current terminal dimensions.
        
        Returns:
            Self for chaining
        """
        # Get terminal dimensions
        terminal_width = SYSTEM_CONTEXT.attributes.get("terminal_width", 80)
        terminal_height = SYSTEM_CONTEXT.attributes.get("terminal_height", 24)
        
        # Calculate optimal block size to fit image in terminal
        # Account for aspect ratio (terminal characters are taller than wide)
        char_aspect = 0.5  # approximate character height/width ratio
        
        # Calculate dimensions that would fit in terminal
        target_cols = max(20, terminal_width - 4)  # Allow border space
        target_rows = max(10, terminal_height - 4)
        
        if self.image:
            image_width, image_height = self.image.size
            image_aspect = image_width / image_height
            
            # Calculate block size that fits image in terminal
            # with appropriate aspect ratio correction
            width_from_cols = max(2, image_width // target_cols)
            height_from_rows = max(2, image_height // target_rows)
            
            # Adjust to preserve aspect ratio
            adjusted_height = int(width_from_cols / (image_aspect * char_aspect))
            adjusted_width = int(height_from_rows * image_aspect / char_aspect)
            
            self.options["block_width"] = min(width_from_cols, adjusted_width)
            self.options["block_height"] = min(height_from_rows, adjusted_height)
            
            # Adjust scale factor based on terminal size
            if terminal_width < 80 or terminal_height < 24:
                self.options["scale_factor"] = 1
            
        return self
    
    def render(self) -> List[str]:
        """Generate Unicode art with current settings.
        
        Returns:
            List of strings with art lines
        """
        if not self.image:
            return ["Error: No image loaded"]
            
        return image_to_unicode_art(
            self.image,
            **self.options
        )
    
    def save_to_file(self, path: Union[str, Path]) -> None:
        """Save rendered art to text file.
        
        Args:
            path: Output file path
        """
        result = self.render()
        
        try:
            with open(path, 'w', encoding='utf-8') as f:
                for line in result:
                    # Strip ANSI color codes for file output
                    if self.options["output_format"] != "ansi":
                        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
                        line = ansi_escape.sub('', line)
                    f.write(line + '\n')
                    
            if HAS_RICH:
                CONSOLE.print(f"[green]âœ“ Art saved to:[/green] {path}")
            else:
                print(f"âœ“ Art saved to: {path}")
        except Exception as e:
            if HAS_RICH:
                CONSOLE.print(f"[red]ğŸš« Error saving file:[/red] {e}")
            else:
                print(f"ğŸš« Error saving file: {e}")
    
    def display(self) -> None:
        """Render and display art in terminal."""
        result = self.render()
        
        if HAS_RICH:
            # Add a title panel
            title = "âœ¨ Unicode Art Rendering âœ¨"
            CONSOLE.print(Panel(title, style="bold blue"))
            
            # Print each line
            for line in result:
                CONSOLE.print(line)
        else:
            # Simple display
            print("\n".join(result))


# Legacy compatibility wrapper
def transform_image(
    image_path: Union[str, Path], 
    preset: Optional[str] = None
) -> List[str]:
    """ğŸ¨ Transform image file with fluent parameter configuration.
    
    Args:
        image_path: Path to image file
        preset: Optional preset name ("default", "detailed", "fast", "minimal")
        
    Returns:
        List of strings containing Unicode art
    """
    transformer = ArtTransformer(image_path)
    
    if preset:
        transformer.with_preset(preset)
        
    return transformer.render()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸ–¼ï¸ Virtual Display Capture And Management                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class VirtualDisplayEngine:
    """ğŸ”® Dimensional context bridge for GUI-to-terminal transmutation.
    
    Creates and manages virtual displays (Xvfb), captures their visual state,
    and streams real-time GUI content to terminals through dimensional
    compression with adaptive performance tuning and intelligent fallbacks.
    """
    # Singleton instance with lazy initialization
    _instance = None
    
    @classmethod
    def get_instance(cls) -> 'VirtualDisplayEngine':
        """ğŸ“¡ Get or create the singleton display engine."""
        if cls._instance is None:
            cls._instance = VirtualDisplayEngine()
        return cls._instance

    def __init__(self) -> None:
        """Initialize with dynamic capability detection."""
        self._virtual_display = None
        self._display_process = None
        self._current_display_id = 99  # Default virtual display number
        self._display_size = (1280, 720)  # Default resolution
        
        # Capability detection with progressive enhancement
        self._capabilities = self._detect_capabilities()
        
        # Screenshot backend selection based on system capabilities
        self._capture_backend = self._select_capture_backend()
        
        # Rendering optimizations cache
        self._capture_cache = {}
        self._capture_cache_enabled = SYSTEM_CONTEXT.constraints.get("performance_tier", 1) >= 2
        
        # Status tracking for resource management
        self._active_streams = set()
        self._last_capture_time = 0

    def _detect_capabilities(self) -> Dict[str, bool]:
        """ğŸ•µï¸ Detect available virtual display and capture mechanisms."""
        capabilities = {
            "has_xvfb": False,
            "has_pyvirtualdisplay": False,
            "has_mss": False,
            "has_pil_grab": False,
            "has_x11_utils": False,
            "can_create_display": False,
            "platform_headless_support": False
        }
        
        # Check for PyVirtualDisplay
        try:
            import pyvirtualdisplay
            capabilities["has_pyvirtualdisplay"] = True
        except ImportError:
            pass
            
        # Check for MSS (fast cross-platform screenshots)
        try:
            import mss
            capabilities["has_mss"] = True
        except ImportError:
            pass
            
        # Check for PIL ImageGrab
        try:
            from PIL import ImageGrab
            capabilities["has_pil_grab"] = True
        except (ImportError, AttributeError):
            pass
            
        # Check for X11 utilities
        if SYSTEM_CONTEXT.attributes.get("platform") == "Linux":
            for utility in ["xvfb-run", "xwd", "import"]:
                if shutil.which(utility):
                    capabilities["has_x11_utils"] = True
                    if utility == "xvfb-run":
                        capabilities["has_xvfb"] = True
            
        # Determine if we can create virtual displays
        capabilities["can_create_display"] = (
            capabilities["has_xvfb"] or 
            capabilities["has_pyvirtualdisplay"]
        )
            
        # Platform-specific capability checks
        if SYSTEM_CONTEXT.attributes.get("platform") == "Linux":
            capabilities["platform_headless_support"] = True
        elif SYSTEM_CONTEXT.attributes.get("platform") == "Darwin":
            capabilities["platform_headless_support"] = capabilities["has_pil_grab"]
        else:  # Windows
            capabilities["platform_headless_support"] = capabilities["has_mss"]
            
        return capabilities

    def _select_capture_backend(self) -> str:
        """âš™ï¸ Select optimal screenshot backend based on capabilities."""
        if self._capabilities["has_mss"]:
            return "mss"  # Fast and cross-platform
        elif self._capabilities["has_x11_utils"] and SYSTEM_CONTEXT.attributes.get("platform") == "Linux":
            return "xwd"  # X11 specific but reliable
        elif self._capabilities["has_pil_grab"]:
            return "pillow"  # Decent fallback
        else:
            return "none"  # No suitable backend

    def create_virtual_display(self, 
                             width: int = 1280, 
                             height: int = 720, 
                             color_depth: int = 24,
                             visible: bool = False) -> bool:
        """ğŸ–¥ï¸ Create new virtual display or take over existing one.
        
        Args:
            width: Display width in pixels
            height: Display height in pixels
            color_depth: Display color depth
            visible: Whether to make display visible (for debugging)
            
        Returns:
            Success status
        """
        # Abort if capabilities not available
        if not self._capabilities["can_create_display"]:
            if HAS_RICH:
                CONSOLE.print("[red]âš ï¸ Cannot create virtual display - missing dependencies[/red]")
                CONSOLE.print("[yellow]Install pyvirtualdisplay or ensure Xvfb is available[/yellow]")
            else:
                print("âš ï¸ Cannot create virtual display - missing dependencies")
                print("ğŸ’¡ Install pyvirtualdisplay or ensure Xvfb is available")
            return False
            
        # Clean up any existing display
        self.destroy_virtual_display()
        
        # Store new display dimensions
        self._display_size = (width, height)
        
        # Try pyvirtualdisplay first (more robust)
        if self._capabilities["has_pyvirtualdisplay"]:
            try:
                import pyvirtualdisplay
                self._virtual_display = pyvirtualdisplay.Display(
                    visible=1 if visible else 0,
                    size=(width, height),
                    color_depth=color_depth
                )
                self._virtual_display.start()
                os.environ["DISPLAY"] = self._virtual_display.display
                self._current_display_id = int(self._virtual_display.display.replace(':', ''))
                
                if HAS_RICH:
                    CONSOLE.print(f"[green]âœ“[/green] Virtual display created at [bold]{self._virtual_display.display}[/bold]")
                else:
                    print(f"âœ“ Virtual display created at {self._virtual_display.display}")
                return True
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[red]Error creating virtual display with PyVirtualDisplay: {e}[/red]")
                else:
                    print(f"Error creating virtual display with PyVirtualDisplay: {e}")
        
        # Fall back to direct Xvfb if available
        if self._capabilities["has_xvfb"]:
            try:
                # Find an available display number
                for display_id in range(99, 110):
                    if not os.path.exists(f"/tmp/.X{display_id}-lock"):
                        self._current_display_id = display_id
                        break
                
                # Start Xvfb process
                display_str = f":{self._current_display_id}"
                cmd = [
                    "Xvfb", display_str, "-screen", "0", 
                    f"{width}x{height}x{color_depth}", "-nolisten", "tcp"
                ]
                if not visible:
                    cmd.append("-ac")
                    
                self._display_process = subprocess.Popen(
                    cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
                )
                
                # Wait for display to become available
                time.sleep(1)
                
                # Set environment variable for applications
                os.environ["DISPLAY"] = display_str
                
                if HAS_RICH:
                    CONSOLE.print(f"[green]âœ“[/green] Virtual display created at [bold]{display_str}[/bold]")
                else:
                    print(f"âœ“ Virtual display created at {display_str}")
                return True
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[red]Error creating virtual display with Xvfb: {e}[/red]")
                else:
                    print(f"Error creating virtual display with Xvfb: {e}")
                    
        return False

    def destroy_virtual_display(self) -> None:
        """ğŸ§¹ Clean up virtual display resources."""
        # Stop all active streams
        self._active_streams.clear()
        
        # Clean up pyvirtualdisplay
        if self._virtual_display is not None:
            try:
                self._virtual_display.stop()
            except:
                pass
            self._virtual_display = None
            
        # Clean up direct Xvfb process
        if self._display_process is not None:
            try:
                self._display_process.terminate()
                self._display_process.wait(timeout=3)
            except:
                try:
                    self._display_process.kill()
                except:
                    pass
            self._display_process = None

    def capture_screenshot(self) -> Optional[Image.Image]:
        """ğŸ“¸ Capture screenshot from current display with optimal backend.
        
        Returns:
            PIL Image of current screen or None on failure
        """
        # Rate limiting to prevent excessive captures
        current_time = time.time()
        if current_time - self._last_capture_time < 0.01:  # Max 100 FPS
            time.sleep(0.01)
            
        self._last_capture_time = current_time
        
        # Try different capture methods in order of preference
        if self._capture_backend == "mss":
            try:
                import mss
                with mss.mss() as sct:
                    # Get the monitor to capture (first monitor or virtual display)
                    monitor = sct.monitors[0]  # Capturing the entire display
                    
                    # Capture the screen
                    sct_img = sct.grab(monitor)
                    
                    # Convert to PIL Image
                    return Image.frombytes("RGB", sct_img.size, sct_img.rgb)
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[yellow]MSS capture failed: {e}[/yellow]")
                else:
                    print(f"MSS capture failed: {e}")
                    
        elif self._capture_backend == "xwd":
            try:
                # Use xwd to capture the screen
                display = os.environ.get("DISPLAY", f":{self._current_display_id}")
                cmd = ["xwd", "-root", "-display", display, "-silent"]
                
                process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                stdout, stderr = process.communicate(timeout=5)
                
                if process.returncode == 0:
                    # Convert xwd output to PIL Image
                    return Image.open(io.BytesIO(stdout))
                else:
                    if HAS_RICH:
                        CONSOLE.print(f"[yellow]xwd capture failed: {stderr.decode()}[/yellow]")
                    else:
                        print(f"xwd capture failed: {stderr.decode()}")
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[yellow]X11 capture failed: {e}[/yellow]")
                else:
                    print(f"X11 capture failed: {e}")
                    
        elif self._capture_backend == "pillow":
            try:
                from PIL import ImageGrab
                return ImageGrab.grab()
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[yellow]PIL capture failed: {e}[/yellow]")
                else:
                    print(f"PIL capture failed: {e}")
        
        # If all capture methods failed, try one more fallback
        try:
            # ImageMagick's import command (often available on Linux)
            if shutil.which("import"):
                temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)
                temp_file.close()
                
                cmd = ["import", "-window", "root", temp_file.name]
                process = subprocess.run(cmd, timeout=5)
                
                if process.returncode == 0:
                    img = Image.open(temp_file.name)
                    os.unlink(temp_file.name)
                    return img
                
                os.unlink(temp_file.name)
        except:
            pass
            
        return None

    def stream_display_to_terminal(self,
                                 scale_factor: int = 2,
                                 block_width: int = 8,
                                 block_height: int = 8,
                                 edge_threshold: int = 50,
                                 gradient_str: Optional[str] = None,
                                 color: bool = True,
                                 fps: int = 15,
                                 enhanced_edges: bool = True,
                                 max_frames: Optional[int] = None,
                                 algorithm: str = "sobel",
                                 show_stats: bool = True,
                                 adaptive_quality: bool = True) -> None:
        """ğŸ¬ Stream virtual display content to terminal as Unicode art.
        
        Creates real-time visualization of GUI applications in the terminal
        using adaptive quality parameters and system-aware optimizations.
        
        Args:
            scale_factor: Detail enhancement factor
            block_width: Character cell width
            block_height: Character cell height
            edge_threshold: Edge sensitivity
            gradient_str: Custom character gradient
            color: Enable ANSI colors
            fps: Target frames per second
            enhanced_edges: Use advanced edge characters
            max_frames: Maximum frames to capture (None for continuous)
            algorithm: Edge detection algorithm
            show_stats: Display performance metrics
            adaptive_quality: Auto-adjust quality for performance
        """
        # Generate unique stream ID for tracking
        stream_id = str(uuid.uuid4())
        self._active_streams.add(stream_id)
        
        # Get system-aware parameters
        system_params = SYSTEM_CONTEXT.get_optimized_parameters()
        terminal_width = SYSTEM_CONTEXT.attributes.get("terminal_width", 80)
        terminal_height = SYSTEM_CONTEXT.attributes.get("terminal_height", 24)
        
        # Adjust parameters based on system capabilities if adaptive
        if adaptive_quality:
            # Reduce quality on slower systems
            if SYSTEM_CONTEXT.constraints.get("performance_tier", 1) <= 1:
                scale_factor = 1
                edge_threshold = 70
                algorithm = "sobel"  # Fastest algorithm
                
            # Terminal size adaptation
            if terminal_width < 100:
                block_width = max(block_width, 10)
                block_height = max(block_height, int(block_width * 0.5))
                
        # Fallback gradient based on terminal capabilities
        if gradient_str is None:
            if UNICODE_ENGINE.supports_unicode:
                gradient_str = get_enhanced_gradient_chars()
            else:
                gradient_str = UNICODE_ENGINE.character_maps["full_gradients"]["ascii_art"]
                
        # Performance tracking
        max_tracking_samples = 30
        frame_duration = 1.0 / fps
        frames_processed = 0
        dropped_frames = 0
        start_time = time.time()
        
        # Performance metrics with ring buffer
        render_times = collections.deque(maxlen=max_tracking_samples)
        fps_values = collections.deque(maxlen=max_tracking_samples)
        capture_times = collections.deque(maxlen=max_tracking_samples)
        
        # Adaptive quality parameters
        adaptive_params = {
            'scale': scale_factor,
            'width': block_width,
            'height': block_height,
            'threshold': edge_threshold,
            'frames_since_adjustment': 0,
            'quality_level': 2,
        }
        
        # Quality adjustment thresholds (ms/frame)
        quality_thresholds = {
            'reduce': frame_duration * 1000 * 0.9,  # If processing takes >90% of frame time
            'improve': frame_duration * 1000 * 0.6,  # If processing takes <60% of frame time
        }
        
        try:
            # Prepare terminal for streaming display
            if HAS_RICH:
                with CONSOLE.screen():
                    CONSOLE.print(Panel.fit(
                        f"ğŸ–¥ï¸ [bold cyan]Virtual Display Stream[/bold cyan]\n"
                        f"[dim]Resolution: {self._display_size[0]}Ã—{self._display_size[1]} â€¢ "
                        f"Target: {fps} FPS â€¢ "
                        f"Quality: {adaptive_params['quality_level']}/4[/dim]",
                        border_style="blue"
                    ))
                    
                    # Main streaming loop
                    frame_count = 0
                    while (max_frames is None or frame_count < max_frames) and stream_id in self._active_streams:
                        frame_start = time.time()
                        
                        # â±ï¸ Capture timing
                        capture_start = time.time()
                        screenshot = self.capture_screenshot()
                        capture_time = (time.time() - capture_start) * 1000
                        capture_times.append(capture_time)
                        
                        if screenshot is None:
                            # If capture failed, skip this frame
                            time.sleep(0.1)
                            dropped_frames += 1
                            continue
                            
                        # â±ï¸ Processing timing
                        process_start = time.time()
                        frame = image_to_unicode_art(
                            screenshot,
                            scale_factor=adaptive_params['scale'],
                            block_width=adaptive_params['width'],
                            block_height=adaptive_params['height'],
                            edge_threshold=adaptive_params['threshold'],
                            gradient_str=gradient_str,
                            color=color,
                            enhanced_edges=enhanced_edges,
                            algorithm=algorithm
                        )
                        process_time = (time.time() - process_start) * 1000
                        render_times.append(process_time)
                        
                        # Display the frame with stats if enabled
                        CONSOLE.clear()
                        
                        if show_stats:
                            stats_panel = Table.grid()
                            stats_panel.add_column(style="cyan")
                            stats_panel.add_column(style="green")
                            stats_panel.add_column(style="magenta")
                            stats_panel.add_column(style="yellow")
                            
                            # Calculate metrics
                            avg_render = sum(render_times) / len(render_times) if render_times else 0
                            avg_capture = sum(capture_times) / len(capture_times) if capture_times else 0
                            actual_fps = frames_processed / (time.time() - start_time) if frames_processed > 0 else 0
                            
                            stats_panel.add_row(
                                f"Frame: {frame_count}",
                                f"FPS: {actual_fps:.1f}",
                                f"Render: {avg_render:.1f}ms",
                                f"Quality: {adaptive_params['quality_level']}/4"
                            )
                            
                            CONSOLE.print(stats_panel)
                            
                        # Display frame
                        for line in frame:
                            CONSOLE.print(line)
                            
                        frames_processed += 1
                        frame_count += 1
                        
                        # Adaptive quality management
                        adaptive_params['frames_since_adjustment'] += 1
                        
                        if adaptive_quality and adaptive_params['frames_since_adjustment'] >= 5:
                            total_time = avg_render + avg_capture
                            
                            # Reduce quality if we're struggling to meet FPS
                            if total_time > quality_thresholds['reduce'] and adaptive_params['quality_level'] > 0:
                                adaptive_params['quality_level'] -= 1
                                adaptive_params['scale'] = max(1, adaptive_params['scale'] - 1)
                                adaptive_params['width'] = min(16, adaptive_params['width'] + 2)
                                adaptive_params['height'] = min(16, adaptive_params['height'] + 2)
                                adaptive_params['threshold'] = min(80, adaptive_params['threshold'] + 10)
                                adaptive_params['frames_since_adjustment'] = 0
                                
                            # Improve quality if we're exceeding FPS target
                            elif total_time < quality_thresholds['improve'] and adaptive_params['quality_level'] < 4:
                                adaptive_params['quality_level'] += 1
                                adaptive_params['scale'] = min(4, adaptive_params['scale'] + 1)
                                adaptive_params['width'] = max(4, adaptive_params['width'] - 2)
                                adaptive_params['height'] = max(2, adaptive_params['height'] - 1)
                                adaptive_params['threshold'] = max(30, adaptive_params['threshold'] - 10)
                                adaptive_params['frames_since_adjustment'] = 0
                        
                        # Control frame rate
                        elapsed = time.time() - frame_start
                        sleep_time = frame_duration - elapsed
                        
                        if sleep_time > 0:
                            time.sleep(sleep_time)
                        else:
                            dropped_frames += 1
                            
                        # Collect metrics
                        actual_frame_duration = time.time() - frame_start
                        fps_values.append(1.0 / actual_frame_duration if actual_frame_duration > 0 else fps)
            else:
                # Simpler non-Rich version with ANSI control codes
                print("\033[2J\033[H", end="")  # Clear screen and move to home
                print(f"ğŸ–¥ï¸ Virtual Display Stream - Target: {fps} FPS - Quality: {adaptive_params['quality_level']}/4")
                
                # Main streaming loop
                frame_count = 0
                while (max_frames is None or frame_count < max_frames) and stream_id in self._active_streams:
                    frame_start = time.time()
                    
                    # Capture screenshot
                    capture_start = time.time()
                    screenshot = self.capture_screenshot()
                    capture_time = (time.time() - capture_start) * 1000
                    capture_times.append(capture_time)
                    
                    if screenshot is None:
                        # If capture failed, skip this frame
                        time.sleep(0.1)
                        dropped_frames += 1
                        continue
                        
                    # Process image
                    process_start = time.time()
                    frame = image_to_unicode_art(
                        screenshot,
                        scale_factor=adaptive_params['scale'],
                        block_width=adaptive_params['width'],
                        block_height=adaptive_params['height'],
                        edge_threshold=adaptive_params['threshold'],
                        gradient_str=gradient_str,
                        color=color,
                        enhanced_edges=enhanced_edges,
                        algorithm=algorithm
                    )
                    process_time = (time.time() - process_start) * 1000
                    render_times.append(process_time)
                    
                    # Display the frame
                    print("\033[2J\033[H", end="")  # Clear screen and move to home
                    
                    if show_stats:
                        avg_render = sum(render_times) / len(render_times) if render_times else 0
                        avg_capture = sum(capture_times) / len(capture_times) if capture_times else 0
                        actual_fps = frames_processed / (time.time() - start_time) if frames_processed > 0 else 0
                        
                        print(f"Frame: {frame_count} | FPS: {actual_fps:.1f} | "
                              f"Render: {avg_render:.1f}ms | Quality: {adaptive_params['quality_level']}/4")
                    
                    # Display frame
                    for line in frame:
                        print(line)
                        
                    frames_processed += 1
                    frame_count += 1
                    
                    # Adaptive quality management
                    adaptive_params['frames_since_adjustment'] += 1
                    
                    if adaptive_quality and adaptive_params['frames_since_adjustment'] >= 5:
                        total_time = avg_render + avg_capture
                        
                        # Adjust quality based on performance
                        if total_time > quality_thresholds['reduce'] and adaptive_params['quality_level'] > 0:
                            adaptive_params['quality_level'] -= 1
                            adaptive_params['scale'] = max(1, adaptive_params['scale'] - 1)
                            adaptive_params['width'] = min(16, adaptive_params['width'] + 2)
                            adaptive_params['height'] = min(16, adaptive_params['height'] + 2)
                            adaptive_params['threshold'] = min(80, adaptive_params['threshold'] + 10)
                            adaptive_params['frames_since_adjustment'] = 0
                        elif total_time < quality_thresholds['improve'] and adaptive_params['quality_level'] < 4:
                            adaptive_params['quality_level'] += 1
                            adaptive_params['scale'] = min(4, adaptive_params['scale'] + 1)
                            adaptive_params['width'] = max(4, adaptive_params['width'] - 2)
                            adaptive_params['height'] = max(2, adaptive_params['height'] - 1)
                            adaptive_params['threshold'] = max(30, adaptive_params['threshold'] - 10)
                            adaptive_params['frames_since_adjustment'] = 0
                    
                    # Control frame rate
                    elapsed = time.time() - frame_start
                    sleep_time = frame_duration - elapsed
                    
                    if sleep_time > 0:
                        time.sleep(sleep_time)
                    else:
                        dropped_frames += 1
                        
                    # Collect metrics
                    actual_frame_duration = time.time() - frame_start
                    fps_values.append(1.0 / actual_frame_duration if actual_frame_duration > 0 else fps)
                    
        except KeyboardInterrupt:
            if HAS_RICH:
                CONSOLE.print("\n[bold green]ğŸ‘‹ Stream terminated by user[/bold green]")
            else:
                print("\nğŸ‘‹ Stream terminated by user")
        except Exception as e:
            if HAS_RICH:
                CONSOLE.print(f"[bold red]ğŸš« Stream error: {str(e)}[/bold red]")
                CONSOLE.print_exception()
            else:
                print(f"\nğŸš« Stream error: {str(e)}")
                traceback.print_exc()
        finally:
            # Clean up stream resources
            if stream_id in self._active_streams:
                self._active_streams.remove(stream_id)
            
            # Display final statistics
            duration = time.time() - start_time
            actual_fps = frames_processed / duration if duration > 0 else 0
            avg_render = sum(render_times) / len(render_times) if render_times else 0
            
            if show_stats:
                if HAS_RICH:
                    CONSOLE.print("\n[bold]ğŸ“Š Performance summary:[/bold]")
                    stats_table = Table()
                    stats_table.add_column("Metric", style="cyan")
                    stats_table.add_column("Value", style="green")
                    
                    stats_table.add_row("Frames processed", str(frames_processed))
                    stats_table.add_row("Runtime", f"{duration:.2f}s")
                    stats_table.add_row("Effective rate", f"{actual_fps:.2f} fps")
                    stats_table.add_row("Average render", f"{avg_render:.1f}ms/frame")
                    stats_table.add_row("Final quality level", f"{adaptive_params['quality_level']}/4")
                    if dropped_frames > 0:
                        stats_table.add_row("Dropped frames", f"{dropped_frames} ({dropped_frames/max(1, frames_processed)*100:.1f}%)")
                    
                    CONSOLE.print(stats_table)
                else:
                    print("\nğŸ“Š Performance summary:")
                    print(f"  â€¢ Frames processed: {frames_processed}")
                    print(f"  â€¢ Runtime: {duration:.2f}s")
                    print(f"  â€¢ Effective rate: {actual_fps:.2f} fps")
                    print(f"  â€¢ Average render: {avg_render:.1f}ms/frame")
                    print(f"  â€¢ Final quality level: {adaptive_params['quality_level']}/4")
                    if dropped_frames > 0:
                        print(f"  â€¢ Dropped frames: {dropped_frames} ({dropped_frames/max(1, frames_processed)*100:.1f}%)")

    def launch_gui_application(self, 
                             command: Union[str, List[str]],
                             stream_to_terminal: bool = True,
                             fps: int = 15,
                             timeout: Optional[int] = None,
                             scale_factor: int = 2,
                             color: bool = True) -> subprocess.Popen:
        """ğŸš€ Launch GUI application on virtual display and optionally stream to terminal.
        
        Executes a graphical application on the virtual display and can simultaneously
        stream its visual output to the terminal as Unicode art.
        
        Args:
            command: Application command or list of arguments
            stream_to_terminal: Whether to capture and display the GUI
            fps: Frames per second for streaming
            timeout: Maximum runtime in seconds (None for no limit)
            scale_factor: Detail level for streaming
            color: Enable color output
            
        Returns:
            Process handle for the launched application
        """
        # Ensure we have a virtual display
        if not self._virtual_display and self._display_process is None:
            created = self.create_virtual_display()
            if not created:
                raise RuntimeError("Failed to create virtual display")
                
        # Ensure DISPLAY environment variable is set
        if "DISPLAY" not in os.environ:
            os.environ["DISPLAY"] = f":{self._current_display_id}"
        # Launch the application
        if isinstance(command, str):
            command = shlex.split(command)
        process = subprocess.Popen(
            command,
            env=os.environ,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        if stream_to_terminal:
            # Stream the application to the terminal
            self.stream_display_to_terminal(
                scale_factor=scale_factor,
                fps=fps,
                color=color,
                max_frames=timeout,
                adaptive_quality=True
            )
            return process
        else:
            # Just launch the application without streaming
            return process

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸŒŸ Interactive Mode & File Selection                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def show_interactive_menu() -> Dict[str, Any]:
    """ğŸ® Present interactive interface for GlyphStream.
    
    Provides contextually-relevant options with multi-stage rendering
    based on system capabilities and user choices.
    
    Returns:
        Dictionary of user-selected options with validated parameters
    """
    if not HAS_RICH:
        print("ğŸ”® Installing rich is recommended for best experience:")
        print("   pip install rich")
        
        # Basic input in non-rich mode
        print("\nâ•”â•â•â•â•â•â•â•â• GlyphStream Interactive â•â•â•â•â•â•â•â•â•—")
        print("â•‘ Transform reality through Unicode art    â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        source_type = input("ğŸ“Œ Select source type (1=Image, 2=Video, 3=YouTube, 4=Webcam, 5=Virtual Display): ")
        source = input("ğŸ” Enter path or URL: ")
        scale = input("ğŸ”¬ Detail level (1-4, default=2): ")
        color = input("ğŸ¨ Use color? (y/n, default=y): ")
        
        # System-aware defaults with Eidosian integration
        system_params = SYSTEM_CONTEXT.get_optimized_parameters()
        default_scale = min(2, system_params.get("scale_factor", 2))
        
        options = {
            "source": source,
            "scale": int(scale) if scale.isdigit() and 1 <= int(scale) <= 4 else default_scale,
            "color": color.lower() != 'n',
            "video": source_type in ('2', '3', '4', '5'),
            "virtual_display": source_type == '5',
            "fps": system_params.get("fps", 15),
            "edge_threshold": 50,
            "enhanced_edges": True,
            "algorithm": "sobel",
            "gradient_set": "standard",
            "dithering": False,
            "render_engine": "unicode"  # Default rendering engine
        }
        
        if source_type == '3' and not source.startswith(("http", "www")):
            options["source"] = f"https://youtu.be/{source}"
        elif source_type in ('4', '5'):
            options["source"] = int(source) if source.isdigit() else 0
            
        print("\nâœ¨ Processing with Neuroforge transmutation engine...\n")
        return options
    
    # Rich-enhanced interface with Eidosian flair
    title_art = Text("âœ¨ GLYPH STREAM âœ¨", style="bold cyan")
    subtitle = Text("Dimensional Unicode Transmutation Engine", style="italic")
    
    # Create stylish header with author information
    header_panel = Panel(
        Group(
            Align.center(title_art),
            Align.center(subtitle),
            Align.center(Text(f"by {AUTHOR_INFO['name']} â€¢ {AUTHOR_INFO['org']}", style="dim"))
        ),
        border_style="blue",
        box=rich.box.ROUNDED
    )
    
    CONSOLE.print(header_panel)
    
    # Get system-optimized parameters for intelligent defaults
    system_params = SYSTEM_CONTEXT.get_optimized_parameters()
    
    # Source type selection with visual guidance and personality
    source_table = Table(show_header=False, box=rich.box.SIMPLE)
    source_table.add_column("â„–", style="cyan", no_wrap=True)
    source_table.add_column("Type", style="green", no_wrap=True)
    source_table.add_column("Description", style="white")
    
    source_table.add_row("1", "ğŸ–¼ï¸ [bold]Image[/bold]", "Convert still images into dimensional art")
    source_table.add_row("2", "ğŸ¬ [bold]Video[/bold]", "Transform local video files into streams")
    source_table.add_row("3", "ğŸ“º [bold]YouTube[/bold]", "Transmute online videos in real-time")
    source_table.add_row("4", "ğŸ“· [bold]Webcam[/bold]", "See yourself through the dimensional lens")
    source_table.add_row("5", "ğŸ–¥ï¸ [bold]Display[/bold]", "Capture virtual display output")
    
    CONSOLE.print(Panel(source_table, title="ğŸ“¦ Source Selection", border_style="cyan"))
    source_type = Prompt.ask("âš¡ Choose input type", choices=["1", "2", "3", "4", "5"], default="1")
    
    # Source path/URL with contextual prompting
    if source_type == "1":
        CONSOLE.print("ğŸ–¼ï¸ [bold]Image Transmutation Mode[/bold]")
        source = Prompt.ask("ğŸ“‚ Enter image path")
        
        # Engine selection with visual indicators
        engine_table = Table(show_header=False, box=rich.box.SIMPLE)
        engine_table.add_column("Engine", style="cyan")
        engine_table.add_column("Characteristics", style="white")
        
        engine_table.add_row("unicode", "âš›ï¸ Standard - Edge-aware dimensional rendering")
        engine_table.add_row("text", "ğŸ”¤ Typographic - ASCII art with font variety")
        engine_table.add_row("transformer", "âœ¨ Advanced - Multi-stage processing pipeline")
        
        CONSOLE.print(Panel(engine_table, title="ğŸ§  Rendering Engines", border_style="magenta"))
        render_engine = Prompt.ask(
            "ğŸ§ª Select rendering engine",
            choices=["unicode", "text", "transformer"],
            default="unicode"
        )
        
        # Show preset options with visual indicators
        preset_table = Table(show_header=False, box=rich.box.SIMPLE)
        preset_table.add_column("Preset", style="cyan")
        preset_table.add_column("Description", style="white")
        
        preset_table.add_row("standard", "Balanced quality and performance")
        preset_table.add_row("detailed", "âœ¨ High-fidelity edge detection and dithering")
        preset_table.add_row("fast", "ğŸš€ Optimized for speed on any system")
        preset_table.add_row("minimal", "ğŸ§© ASCII-compatible, resource-efficient")
        
        CONSOLE.print(Panel(preset_table, title="ğŸ›ï¸ Processing Presets", border_style="green"))
        preset_choice = Prompt.ask(
            "ğŸšï¸ Select processing mode",
            choices=["standard", "detailed", "fast", "minimal"],
            default="standard"
        )
    elif source_type == "2":
        CONSOLE.print("ğŸ¬ [bold]Video Stream Mode[/bold]")
        source = Prompt.ask("ğŸ“‚ Enter video file path")
        render_engine = "stream"
    elif source_type == "3":
        CONSOLE.print("ğŸ“º [bold]YouTube Stream Mode[/bold]")
        youtube_input = Prompt.ask("ğŸ”— Enter YouTube URL or video ID")
        if not (youtube_input.startswith("http") or youtube_input.startswith("www")):
            source = f"https://youtu.be/{youtube_input}"
        else:
            source = youtube_input
        render_engine = "stream"
    elif source_type == "4":
        CONSOLE.print("ğŸ“· [bold]Realtime Capture Mode[/bold]")
        webcam_id = Prompt.ask("ğŸ¯ Enter device ID (usually 0)", default="0")
        source = int(webcam_id)
        render_engine = "stream"
    else:  # Virtual Display
        CONSOLE.print("ğŸ–¥ï¸ [bold]Virtual Display Capture Mode[/bold]")
        display_engine = VirtualDisplayEngine.get_instance()
        
        if not display_engine._capabilities.get("can_create_display", False):
            CONSOLE.print("[bold red]âš ï¸ Virtual display capabilities not detected[/bold red]")
            CONSOLE.print("[yellow]Install dependencies: pip install pyvirtualdisplay[/yellow]")
        
        virtual_display_dims = Prompt.ask(
            "ğŸ“ Display dimensions (WIDTHxHEIGHT)", 
            default="1280x720"
        )
        width, height = map(int, virtual_display_dims.lower().split('x'))
        source = 0  # Default display ID
        render_engine = "virtual"
    
    # Configuration with visual styling
    config_panel = Panel(
        Text("Adjust these parameters to control the dimensional transmutation process",
             style="italic"),
        title="âš™ï¸ Configuration",
        border_style="cyan"
    )
    CONSOLE.print(config_panel)
    
    # Detail level with system-aware defaults and visual cues
    detail_table = Table(show_header=False, box=rich.box.SIMPLE, expand=True)
    detail_table.add_column("Level", style="cyan", width=8)
    detail_table.add_column("Impact", style="white")
    
    detail_table.add_row("1", "ğŸš€ Fastest - minimal detail")
    detail_table.add_row("2", "âš¡ Balanced - standard detail")
    detail_table.add_row("3", "âœ¨ Enhanced - higher detail")
    detail_table.add_row("4", "ğŸ’ Maximum - highest detail (slower)")
    
    CONSOLE.print(Panel(detail_table, title="Detail Levels", border_style="blue"))
    
    default_scale = str(min(2, system_params.get("scale_factor", 2)))
    scale = int(Prompt.ask(
        "ğŸ” Detail level", 
        choices=["1", "2", "3", "4"],
        default=default_scale
    ))
    
    # Color support with system capability check
    color = Confirm.ask(
        "ğŸ¨ Enable dimensional color", 
        default=UNICODE_ENGINE.supports_color
    )
    
    # Character set selection with UNICODE_ENGINE integration
    if source_type == "1" and render_engine == "unicode":
        gradient_table = Table(show_header=False, box=rich.box.SIMPLE)
        gradient_table.add_column("Set", style="cyan")
        gradient_table.add_column("Characteristics", style="white")
        
        gradient_table.add_row("standard", "â–ˆâ–“â–’â–‘ Standard balanced set")
        gradient_table.add_row("enhanced", "â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â– Extended gradients")
        gradient_table.add_row("braille", "â£¿â£·â£¯â£Ÿâ¡¿â¢¿â£»â£½â£¾ Braille patterns")
        gradient_table.add_row("ascii", "@%#*+=-:. ASCII compatible")
        
        CONSOLE.print(Panel(gradient_table, title="ğŸ”£ Gradient Character Sets", border_style="blue"))
        
        gradient_set = Prompt.ask(
            "ğŸ”  Select character set",
            choices=["standard", "enhanced", "braille", "ascii"],
            default="standard"
        )
    elif source_type == "1" and render_engine == "text":
        # Font category selection for text engine
        font_categories = TEXT_ENGINE.get_font_categories()
        
        if font_categories:
            categories_str = ", ".join(font_categories)
            CONSOLE.print(f"ğŸ“ Available font categories: [cyan]{categories_str}[/cyan]")
            font_category = Prompt.ask("ğŸ”¤ Select font category", default="standard")
            font_name = TEXT_ENGINE.get_random_font(font_category)
        else:
            font_name = "standard"
    else:
        gradient_set = "standard"
    
    # Prepare options dictionary with base settings
    options = {
        "source": source,
        "scale": scale,
        "color": color,
        "video": source_type in ("2", "3", "4", "5"),
        "virtual_display": source_type == "5",
        "render_engine": render_engine if "render_engine" in locals() else "unicode",
    }
    
    # Add engine-specific options
    if "render_engine" in locals():
        if render_engine == "text" and "font_name" in locals():
            options.update({
                "font": font_name
            })
        elif render_engine == "unicode" and "gradient_set" in locals():
            options.update({
                "gradient_set": gradient_set
            })
        elif render_engine == "virtual":
            options.update({
                "display_width": width if "width" in locals() else 1280,
                "display_height": height if "height" in locals() else 720
            })
    
    # Apply preset configurations with system awareness
    if source_type == "1" and "preset_choice" in locals():
        if preset_choice == "detailed":
            # High-fidelity settings optimized for detailed edge rendering
            options.update({
                "scale": min(3, system_params.get("scale_factor", 3)),
                "block_width": 4,
                "block_height": 4,
                "edge_threshold": 40,
                "algorithm": "scharr",
                "enhanced_edges": True,
                "dithering": True
            })
        elif preset_choice == "fast":
            # Performance-optimized settings
            options.update({
                "scale": 1,
                "block_width": 12,
                "block_height": 12,
                "edge_threshold": 60,
                "enhanced_edges": False,
                "dithering": False
            })
        elif preset_choice == "minimal":
            # Resource-efficient mode
            options.update({
                "scale": 1,
                "block_width": 8,
                "block_height": 8,
                "edge_threshold": 80,
                "color": False,
                "enhanced_edges": False,
                "dithering": False
            })
    
    # Video-specific options with intelligent defaults
    if source_type in ("2", "3", "4", "5"):
        # Dynamic FPS based on system capabilities
        perf_tier = SYSTEM_CONTEXT.constraints.get("performance_tier", 1)
        default_fps = system_params.get("default_fps", 15)
        
        fps_table = Table(show_header=False, box=rich.box.SIMPLE, expand=True)
        fps_table.add_column("FPS", style="cyan", width=8)
        fps_table.add_column("Description", style="white")
        
        fps_table.add_row("10", "ğŸ¢ Smooth on low-end systems")
        fps_table.add_row("15", "âš¡ Balanced performance")
        fps_table.add_row("30", "ğŸš€ Fluid motion on high-end systems")
        
        CONSOLE.print(Panel(fps_table, title="ğŸï¸ Frame Rate Options", border_style="blue"))
        
        fps = int(Prompt.ask(
            "ğŸï¸ Target frames per second",
            choices=["10", "15", "20", "30"],
            default=str(default_fps)
        ))
        
        # Auto-quality with visual explanation
        CONSOLE.print(Panel(
            "Adaptive quality automatically adjusts parameters to maintain target FPS",
            title="ğŸ¤– Adaptive Engine",
            border_style="green"
        ))
        
        adaptive_quality = Confirm.ask(
            "ğŸ§  Enable adaptive quality", 
            default=True
        )
        
        options.update({
            "fps": fps,
            "adaptive_quality": adaptive_quality,
            "show_stats": Confirm.ask("ğŸ“Š Show performance metrics", default=True),
            "border": Confirm.ask("ğŸ”² Add dimensional frame", default=True)
        })
    
    # Advanced options with visual collapse pattern
    if Confirm.ask("ğŸ”¬ Configure advanced parameters", default=False):
        CONSOLE.print(Panel("Fine-tune the dimensional transmutation engine", 
                          title="âš™ï¸ Advanced Configuration", 
                          border_style="magenta"))
        
        # Block size with visual guidance
        block_width = int(Prompt.ask(
            "â¬œ Block width (smaller=more detail, slower)",
            default=str(system_params.get("block_width", 8))
        ))
        
        block_height = int(Prompt.ask(
            "â¬œ Block height",
            default=str(system_params.get("block_height", 8))
        ))
        
        # Edge detection with visual guidance
        edge_table = Table(show_header=False, box=rich.box.SIMPLE, expand=True)
        edge_table.add_column("Value", style="cyan", width=8)
        edge_table.add_column("Effect", style="white")
        
        edge_table.add_row("30", "ğŸ” Sensitive - detects subtle edges")
        edge_table.add_row("50", "âš–ï¸ Balanced - standard sensitivity")
        edge_table.add_row("70", "ğŸ” Selective - only pronounced edges")
        
        CONSOLE.print(Panel(edge_table, title="Edge Detection Sensitivity", border_style="blue"))
        
        edge_threshold = int(Prompt.ask(
            "ğŸ”ª Edge threshold",
            default="50"
        ))
        
        enhanced_edges = Confirm.ask(
            "âœ¨ Use enhanced directional edges", 
            default=system_params.get("edge_mode", "enhanced") == "enhanced"
        )
        
        # Algorithm selection with visual guidance for images
        if source_type == "1" and "render_engine" in locals() and render_engine == "unicode":
            algo_table = Table(show_header=False, box=rich.box.SIMPLE)
            algo_table.add_column("Algorithm", style="cyan")
            algo_table.add_column("Characteristics", style="white")
            
            algo_table.add_row("sobel", "âš¡ Fast, balanced edge detection")
            algo_table.add_row("prewitt", "ğŸ§  Less noise sensitivity")
            algo_table.add_row("scharr", "âœ¨ Better diagonal edge detection")
            algo_table.add_row("laplacian", "ğŸ” Detects edges in all directions")
            algo_table.add_row("canny", "ğŸ’ Advanced multi-stage detection")
            
            CONSOLE.print(Panel(algo_table, title="ğŸ”¬ Edge Detection Algorithms", border_style="blue"))
            
            algorithm = Prompt.ask(
                "ğŸ§ª Edge detection algorithm",
                choices=["sobel", "prewitt", "scharr", "laplacian", "canny"],
                default="sobel"
            )
            
            dithering = Confirm.ask(
                "ğŸ”¢ Apply error diffusion dithering", 
                default=False
            )
            
            options.update({
                "algorithm": algorithm,
                "dithering": dithering
            })
            
        # Text engine specific options
        if "render_engine" in locals() and render_engine == "text" and source_type == "1":
            options.update({
                "text_align": Prompt.ask(
                    "ğŸ“ Text alignment", 
                    choices=["left", "center", "right"],
                    default="center"
                ),
                "add_border": Confirm.ask("ğŸ”² Add text border", default=False)
            })
            
        # Virtual display specific options
        if "render_engine" in locals() and render_engine == "virtual":
            options.update({
                "launch_application": Confirm.ask(
                    "ğŸš€ Launch application in virtual display", 
                    default=False
                )
            })
            
            if options.get("launch_application"):
                options["application_command"] = Prompt.ask(
                    "ğŸ’» Enter application command"
                )
        
        options.update({
            "block_width": block_width,
            "block_height": block_height,
            "edge_threshold": edge_threshold,
            "enhanced_edges": enhanced_edges
        })
    else:
        # Apply intelligent system-aware defaults
        options.update({
            "block_width": system_params.get("block_width", 8),
            "block_height": system_params.get("block_height", 8),
            "edge_threshold": 50,
            "enhanced_edges": system_params.get("edge_mode", "enhanced") == "enhanced",
            "algorithm": "sobel",
            "dithering": False
        })
    
    # Transformer pipeline configuration for advanced processing
    if "render_engine" in locals() and render_engine == "transformer" and source_type == "1":
        CONSOLE.print(Panel(
            "The Art Transformer provides a pipeline of sequential operations",
            title="ğŸ”„ Transformation Pipeline",
            border_style="magenta"
        ))
        
        # Select transformations to apply
        options["transformations"] = []
        
        transform_table = Table(show_header=False, box=rich.box.SIMPLE)
        transform_table.add_column("Transform", style="cyan")
        transform_table.add_column("Effect", style="white")
        
        transform_table.add_row("optimize", "ğŸ”§ Auto-optimize for terminal display")
        transform_table.add_row("edge", "ğŸ”ª Apply edge detection with current algorithm")
        transform_table.add_row("dither", "ğŸ”¢ Apply dithering for better gradients")
        transform_table.add_row("invert", "ğŸ”„ Invert image colors")
        
        CONSOLE.print(Panel(transform_table, title="Available Transformations", border_style="blue"))
        
        if Confirm.ask("ğŸ”§ Add auto-optimization", default=True):
            options["transformations"].append("optimize")
        
        if Confirm.ask("ğŸ”ª Add edge detection", default=True):
            options["transformations"].append("edge")
            
        if Confirm.ask("ğŸ”¢ Add dithering", default=False):
            options["transformations"].append("dither")
            
        if Confirm.ask("ğŸ”„ Add color inversion", default=False):
            options["transformations"].append("invert")
    
    # Configuration summary with visual confirmation
    CONSOLE.print("[bold green]âœ“[/bold green] Dimensional configuration locked in!")
    
    # Optional config summary with styled table
    if Confirm.ask("ğŸ‘ï¸ View configuration details", default=False):
        summary_table = Table(title="ğŸ§© Processing Matrix", box=rich.box.ROUNDED)
        summary_table.add_column("Parameter", style="cyan")
        summary_table.add_column("Value", style="green")
        
        for key, value in options.items():
            if key != "source":  # Don't show source path in summary
                summary_table.add_row(key, str(value))
                
        CONSOLE.print(Panel(summary_table, border_style="blue"))
    
    CONSOLE.print("\n[bold blue]ğŸŒ€ Initializing dimensional transmutation...[/bold blue]\n")
    return options


def parse_command_args() -> Dict[str, Any]:
    """ğŸ§© Parse command-line arguments with intelligent defaults.
    
    Returns:
        Dictionary of parsed and validated command options
    """
    import argparse
    import shutil
    import os
    from rich.panel import Panel
    
    # System-aware default parameters
    sys_params = SYSTEM_CONTEXT.get_optimized_parameters()
    
    # Create parser with Eidosian styling
    parser = argparse.ArgumentParser(
        description="ğŸŒŸ GlyphStream - Dimensional Unicode Art Transmutation Engine",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸ”® Examples:                                       â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ glyph_stream                     # Interactive mode â•‘
â•‘ glyph_stream image.jpg           # Process image    â•‘
â•‘ glyph_stream video.mp4 --fps 30  # Process video    â•‘
â•‘ glyph_stream https://youtu.be/ID # Stream YouTube   â•‘
â•‘ glyph_stream --webcam            # Use webcam       â•‘
â•‘ glyph_stream --virtual-display   # Capture display  â•‘
â•‘ glyph_stream --text "Hello"      # Generate text    â•‘
â•‘ glyph_stream --help              # Show this help   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Neuroforge Glyph Dimensional Engine v1.0
{AUTHOR_INFO["name"]} <{AUTHOR_INFO["email"]}> â€¢ {AUTHOR_INFO["org"]}
        """
    )
    
    # Input source options (mutually exclusive)
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument('source', nargs='?', help='ğŸ“‚ Input file path or ğŸ”— URL')
    input_group.add_argument('--webcam', '-w', action='store_true', help='ğŸ“· Use webcam as input')
    input_group.add_argument('--virtual-display', '-vd', action='store_true', help='ğŸ–¥ï¸ Capture virtual display')
    input_group.add_argument('--text', '-tx', help='ğŸ”  Generate text art')
    
    # Rendering engine selection
    parser.add_argument('--engine', '-e', choices=['unicode', 'text', 'transformer', 'stream', 'virtual'],
                        help='ğŸ§  Rendering engine (default: auto-selected based on input)')
    
    # Processing presets with Eidosian descriptions
    parser.add_argument('--preset', '-p', choices=['standard', 'detailed', 'fast', 'minimal'],
                        help='ğŸ§© Processing preset (standard=balanced, detailed=high-quality, fast=performance, minimal=ASCII)')
    
    # Core parameters with emoji indicators
    parser.add_argument('--scale', '-s', type=int, choices=range(1, 5),
                        default=sys_params.get('scale_factor', 2),
                        help='ğŸ” Detail enhancement factor (1-4)')
    parser.add_argument('--block-width', '-bw', type=int, default=sys_params.get('block_width', 8),
                        help='â¬œ Character cell width')
    parser.add_argument('--block-height', '-bh', type=int, default=sys_params.get('block_height', 8),
                        help='â¬œ Character cell height')
    parser.add_argument('--edge-threshold', '-et', type=int, default=50,
                        help='ğŸ”ª Edge detection threshold (0-255)')
    
    # Feature flags with Eidosian styling
    parser.add_argument('--no-color', action='store_true',
                        help='âš« Disable dimensional color')
    parser.add_argument('--no-enhanced-edges', action='store_true',
                        help='â– Use simplified edge characters')
    parser.add_argument('--dithering', '-d', action='store_true',
                        help='ğŸ”¢ Apply error diffusion dithering')
    
    # Video-specific options with emoji indicators
    parser.add_argument('--fps', '-f', type=int, default=sys_params.get('fps', 15),
                        help='ğŸï¸ Target frames per second for video')
    parser.add_argument('--no-adaptive', action='store_true',
                        help='ğŸ”’ Disable adaptive quality adjustments')
    parser.add_argument('--no-stats', action='store_true',
                        help='ğŸš« Hide performance statistics')
    parser.add_argument('--no-border', action='store_true',
                        help='â¬œ Disable dimensional frame')
    
    # Advanced options with technical descriptions
    parser.add_argument('--algorithm', '-a', choices=['sobel', 'prewitt', 'scharr', 'laplacian', 'canny'],
                        default='sobel', help='ğŸ§ª Edge detection algorithm')
    parser.add_argument('--webcam-id', type=int, default=0,
                        help='ğŸ¯ Webcam device ID (usually 0)')
    parser.add_argument('--display-size', 
                        help='ğŸ“ Virtual display size (WIDTHxHEIGHT), e.g. 1280x720')
    
    # Text engine specific options
    parser.add_argument('--font', 
                        help='ğŸ”  Text font name or category')
    parser.add_argument('--align', choices=['left', 'center', 'right'], default='center',
                        help='ğŸ“ Text alignment')
    
    # Character set selection integrated with UnicodeRenderEngine
    parser.add_argument('--gradient-set', choices=['standard', 'enhanced', 'braille', 'ascii'],
                        default='standard', help='ğŸ”£ Character gradient set to use')
    
    # Transformation pipeline parameters
    parser.add_argument('--transform', '-t', action='append', choices=['optimize', 'edge', 'dither', 'invert'],
                        help='ğŸ”„ Apply transformation (can be used multiple times)')
    
    # Output options with file handling
    parser.add_argument('--save', '-o', metavar='FILE',
                        help='ğŸ’¾ Save output to specified file')
    parser.add_argument('--format', choices=['ansi', 'plain', 'html', 'svg', 'png'],
                        default='ansi', help='ğŸ“„ Output format when saving')
    
    # System options
    parser.add_argument('--debug', action='store_true',
                        help='ğŸ› Enable debug mode with additional diagnostics')
    parser.add_argument('--launch-app', 
                        help='ğŸš€ Launch application in virtual display (with --virtual-display)')
    parser.add_argument('--benchmark', action='store_true',
                        help='â±ï¸ Run benchmark and show detailed performance metrics')
    
    # Parse args with error handling
    try:
        args = parser.parse_args()
    except SystemExit as e:
        # Clean exit after help display
        return {}
    
    # Translate to options dictionary with intelligent source detection
    options = {
        'scale': args.scale,
        'block_width': args.block_width,
        'block_height': args.block_height,
        'edge_threshold': args.edge_threshold,
        'color': not args.no_color,
        'enhanced_edges': not args.no_enhanced_edges,
        'dithering': args.dithering,
        'algorithm': args.algorithm,
        'fps': args.fps,
        'adaptive_quality': not args.no_adaptive,
        'show_stats': not args.no_stats,
        'border': not args.no_border,
        'gradient_set': args.gradient_set,
        'debug': args.debug,
        'benchmark': args.benchmark,
    }
    
    # Engine selection with contextual intelligence
    if args.engine:
        options['render_engine'] = args.engine
    
    # Text processor options
    if args.text:
        options['source'] = args.text
        options['text_content'] = args.text
        options['font'] = args.font or 'standard'
        options['align'] = args.align
        options['render_engine'] = 'text'
        options['video'] = False
    
    # Source determination with validation and intelligent classification
    if args.webcam:
        options['source'] = args.webcam_id
        options['video'] = True
        options['render_engine'] = options.get('render_engine', 'stream')
    elif args.virtual_display:
        # Parse display size with validation
        if args.display_size:
            try:
                width, height = map(int, args.display_size.lower().split('x'))
                options['display_width'] = width
                options['display_height'] = height
            except ValueError:
                if HAS_RICH:
                    CONSOLE.print("[red]Invalid display size format. Using default 1280x720.[/red]")
                options['display_width'] = 1280
                options['display_height'] = 720
        else:
            options['display_width'] = 1280
            options['display_height'] = 720
            
        options['source'] = 0  # Default display ID
        options['video'] = True
        options['virtual_display'] = True
        options['render_engine'] = options.get('render_engine', 'virtual')
        
        # Application launch in virtual display
        if args.launch_app:
            options['launch_application'] = True
            options['application_command'] = args.launch_app
    elif args.source:
        # YouTube URL detection with multi-format support
        if any(domain in args.source for domain in ['youtube.com', 'youtu.be', 'yt.be']):
            options['source'] = args.source
            options['video'] = True
            options['render_engine'] = options.get('render_engine', 'stream')
        # YouTube ID detection (11 chars)
        elif args.source.strip() and len(args.source) == 11 and '/' not in args.source:
            options['source'] = f'https://youtu.be/{args.source}'
            options['video'] = True
            options['render_engine'] = options.get('render_engine', 'stream')
        # File detection with MIME-type awareness
        else:
            # Check if source is a video file
            video_extensions = {'.mp4', '.mkv', '.avi', '.mov', '.webm', '.flv', '.wmv', '.m4v', '.3gp'}
            _, ext = os.path.splitext(args.source.lower())
            options['video'] = ext in video_extensions
            options['source'] = args.source
            
            if options['video']:
                options['render_engine'] = options.get('render_engine', 'stream')
            else:
                options['render_engine'] = options.get('render_engine', 'unicode')
    else:
        # No source specified, fall back to interactive mode
        return {}
    
    # Apply presets with system-aware parameter adaptation
    if args.preset:
        if args.preset == 'detailed':
            options.update({
                'scale': min(3, sys_params.get('scale_factor', 3)),
                'block_width': 4,
                'block_height': 4,
                'edge_threshold': 40,
                'algorithm': 'scharr',
                'enhanced_edges': True,
                'dithering': True
            })
        elif args.preset == 'fast':
            options.update({
                'scale': 1,
                'block_width': 12,
                'block_height': 12,
                'edge_threshold': 60,
                'enhanced_edges': False,
                'dithering': False
            })
        elif args.preset == 'minimal':
            options.update({
                'scale': 1,
                'block_width': 8,
                'block_height': 8,
                'edge_threshold': 80,
                'color': False,
                'enhanced_edges': False,
                'dithering': False,
                'gradient_set': 'ascii'
            })
    
    # Transformation pipeline setup
    if args.transform:
        options['transformations'] = args.transform
        
        # Ensure transformer engine for pipelines
        if 'render_engine' not in options or options['render_engine'] == 'unicode':
            options['render_engine'] = 'transformer'
    
    # File output handling with format determination
    if args.save:
        options['save_path'] = args.save
        options['output_format'] = args.format
        
        # Validate format compatibility
        if options['output_format'] == 'ansi' and not SYSTEM_CONTEXT.capabilities.get("can_display_color", True):
            if HAS_RICH:
                CONSOLE.print("[yellow]Warning: ANSI format selected but terminal doesn't support color. Using 'plain' instead.[/yellow]")
            options['output_format'] = 'plain'
            
        # File format auto-selection from extension
        if '.' in args.save:
            ext = os.path.splitext(args.save)[1].lower()
            if ext in ('.png', '.jpg', '.jpeg') and options['output_format'] == 'ansi':
                options['output_format'] = 'png'
            elif ext == '.svg' and options['output_format'] == 'ansi':
                options['output_format'] = 'svg'
            elif ext == '.html' and options['output_format'] == 'ansi':
                options['output_format'] = 'html'
    
    return options


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ ğŸš€ Entry Point & Command Interface                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main() -> None:
    """ğŸ® GlyphStream command interface.
    
    Provides both CLI and interactive modes with dynamic parameter selection,
    system-aware optimization, and comprehensive error handling.
    """
    try:
        # Check for command-line arguments
        if len(sys.argv) > 1:
            options = parse_command_args()
            # If options is empty, user specified help or invalid args
            if not options:
                return
        else:
            # Interactive mode
            options = show_interactive_menu()
        
        # Process based on source type
        if options["video"]:
            # Handle video source (file, YouTube, or webcam)
            process_video_stream(
                options["source"],
                scale_factor=options["scale"],
                block_width=options.get("block_width", 8),
                block_height=options.get("block_height", 8),
                edge_threshold=options.get("edge_threshold", 50),
                color=options["color"],
                fps=options.get("fps", 15),
                enhanced_edges=options.get("enhanced_edges", True),
                show_stats=options.get("show_stats", True),
                adaptive_quality=options.get("adaptive_quality", True),
                border=options.get("border", True)
            )
        else:
            # Handle image source
            unicode_art = generate_unicode_art(
                options["source"],
                scale_factor=options["scale"],
                block_width=options.get("block_width", 8),
                block_height=options.get("block_height", 8),
                edge_threshold=options.get("edge_threshold", 50),
                color=options["color"],
                enhanced_edges=options.get("enhanced_edges", True),
                algorithm=options.get("algorithm", "sobel"),
                dithering=options.get("dithering", False),
                auto_scale=True
            )
            
            # Handle output destination
            if "save_path" in options:
                # Save to file
                try:
                    with open(options["save_path"], 'w', encoding='utf-8') as f:
                        for line in unicode_art:
                            # Strip ANSI codes if saving to file
                            if not options["color"]:
                                ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
                                line = ansi_escape.sub('', line)
                            f.write(line + '\n')
                    
                    if HAS_RICH:
                        CONSOLE.print(f"[green]âœ“ Art saved to:[/green] {options['save_path']}")
                    else:
                        print(f"âœ“ Art saved to: {options['save_path']}")
                except Exception as e:
                    if HAS_RICH:
                        CONSOLE.print(f"[red]ğŸš« Error saving file:[/red] {str(e)}")
                    else:
                        print(f"ğŸš« Error saving file: {str(e)}")
            else:
                # Print to console
                for line in unicode_art:
                    print(line)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ Exiting GlyphStream")
        sys.exit(0)
    except Exception as e:
        if HAS_RICH:
            CONSOLE.print(f"\n[bold red]ğŸš« Error:[/bold red] {str(e)}")
            CONSOLE.print("[yellow]ğŸ’¡ For troubleshooting, run with --help for usage information[/yellow]")
        else:
            print(f"\nğŸš« Error: {str(e)}")
            print("ğŸ’¡ Check input parameters and try again")
        sys.exit(1)


if __name__ == "__main__":
    main()