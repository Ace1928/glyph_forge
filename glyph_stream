#!/usr/bin/env python3
"""
Glyph Stream - Dimensional Unicode Art Transmutation Engine.

A hyper-dimensional terminal rendering system for transforming visual content
into prismatic Unicode art. Features adaptive quality, edge detection,
multi-algorithm processing, and realtime streaming capabilities.

Attributes:
    THREAD_POOL: Global executor for parallel operations
    CONSOLE: Rich console for enhanced terminal output
    HAS_RICH: Flag indicating if rich library is available
    HAS_CV2: Flag indicating if OpenCV is available
    HAS_YT_DLP: Flag indicating if youtube-dl is available
    SYSTEM_CONTEXT: Global environment context and capabilities
"""

import collections
import io
import json
import math
import os
import platform
import re
import shlex
import shutil
import socket
import statistics
import argparse
import subprocess
import sys
import tempfile
import threading
import time
import traceback
import unicodedata
import uuid
from concurrent.futures import Future, ThreadPoolExecutor
from dataclasses import dataclass
from datetime import datetime
from enum import Enum, IntEnum, auto
from functools import lru_cache
from pathlib import Path
from typing import (Any, Callable, Dict, List, Literal, NamedTuple, Optional, 
                   Set, Tuple, TypedDict, TypeVar, Union, cast)

from enum import Enum, auto
from typing import Dict, List, Literal, Mapping, Optional, Tuple, Union, cast
import random
from typing import Dict, Literal, Optional, Protocol, Tuple, TypedDict, Union, cast, overload
from enum import Enum, auto
import numpy as np
from PIL import Image
import functools


class EdgeDetector(Enum):
    """Edge detection algorithms with optimal spatial characteristics."""
    SOBEL = auto()    # Balanced sensitivity, good general purpose
    PREWITT = auto()  # Enhanced noise stability, cleaner on high-contrast
    SCHARR = auto()   # Superior rotational symmetry for diagonal edges
    LAPLACIAN = auto()# Omnidirectional, detail-preserving, noise-sensitive
    CANNY = auto()    # Maximum precision with hysteresis thresholding


class GradientResult(TypedDict):
    """Edge detection result with normalized components."""
    magnitude: np.ndarray  # Normalized edge magnitude [0-255]
    gradient_x: np.ndarray # X-component of gradient vector
    gradient_y: np.ndarray # Y-component of gradient vector
    direction: np.ndarray  # Gradient direction in radians (optional)


class TextStyle(Enum):
    """Standardized text rendering presets for consistent application."""
    SIMPLE = auto()    # Basic text without decoration
    STYLED = auto()    # Enhanced with color and borders
    RAINBOW = auto()   # Multi-color gradient effect
    RANDOM = auto()    # Randomized styling parameters


# Color name mapping with standardized RGB values
COLOR_MAP: Mapping[str, Tuple[int, int, int]] = {
    "red": (255, 0, 0),
    "green": (0, 255, 0),
    "blue": (0, 0, 255),
    "yellow": (255, 255, 0),
    "cyan": (0, 255, 255),
    "magenta": (255, 0, 255),
    "white": (255, 255, 255),
    "black": (0, 0, 0),
    "orange": (255, 165, 0),
    "purple": (128, 0, 128),
    "pink": (255, 192, 203),
    "gray": (128, 128, 128),
}


# Type variables for generic functions
T = TypeVar('T')

# Type aliases for semantic clarity
Milliseconds = float
Seconds = float
Density = float  # 0.0-1.0 normalized value

class QualityLevel(IntEnum):
    """Discrete quality levels with semantic meaning for adaptive rendering."""
    MINIMAL = 0    # Lowest quality, maximum performance
    LOW = 1        # Reduced quality for constrained systems
    STANDARD = 2   # Balanced quality and performance
    HIGH = 3       # Enhanced quality for capable systems
    MAXIMUM = 4    # Highest quality, performance intensive


class VideoInfo(NamedTuple):
    """Immutable video metadata with validated fields."""
    url: Optional[str] = None
    title: str = "Unknown"
    duration: Optional[int] = None
    format: str = "unknown"
    width: Optional[int] = None
    height: Optional[int] = None
    fps: Optional[float] = None

    @classmethod
    def from_capture(cls, capture, source_name: str, stream_format: str) -> 'VideoInfo':
        """Factory constructor to create VideoInfo from capture device.
        
        Args:
            capture: OpenCV capture object
            source_name: Name of the source (URL/device)
            stream_format: Format identifier
        
        Returns:
            Validated VideoInfo instance
        """
        width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = capture.get(cv2.CAP_PROP_FPS)
        
        # Normalize values with validation
        fps = 30.0 if fps <= 0 or fps > 1000 else fps
        
        return cls(
            title=str(source_name),
            format=stream_format,
            width=width if width > 0 else None,
            height=height if height > 0 else None,
            fps=fps
        )


class PerformanceStats(TypedDict):
    """Rich statistical performance metrics."""
    avg_render_time: float
    avg_fps: float
    effective_fps: float
    total_frames: int
    dropped_frames: int
    drop_ratio: float
    stability: float  # 0-1 rating of render time consistency


# Core parallel execution engine with optimal thread count
THREAD_POOL = ThreadPoolExecutor(max_workers=min(32, (os.cpu_count() or 4) * 2))

# Module import cache with thread-safe access
_MODULE_CACHE: Dict[str, Any] = {}
_IMPORT_LOCK = threading.RLock()

def import_module(module_name: str, package: Optional[str] = None) -> Any:
    """Dynamically import modules with intelligent caching and error handling.
    
    Provides thread-safe, cached module imports with timeout protection and
    graceful error recovery for optional dependencies.
    
    Args:
        module_name: Name of module to import
        package: Specific package from module to import
        
    Returns:
        Imported module or None if import fails
    """
    cache_key = f"{module_name}.{package}" if package else module_name
    
    # Fast path for cached modules with thread safety
    with _IMPORT_LOCK:
        if cache_key in _MODULE_CACHE:
            return _MODULE_CACHE[cache_key]
    
    # Perform actual import with error handling
    try:
        if package:
            module = __import__(module_name, fromlist=[package])
            result = getattr(module, package)
        else:
            result = __import__(module_name)
            
        # Cache successful result
        with _IMPORT_LOCK:
            _MODULE_CACHE[cache_key] = result
        return result
    except (ImportError, AttributeError, ModuleNotFoundError):
        # Cache failed import as None
        with _IMPORT_LOCK:
            _MODULE_CACHE[cache_key] = None
        return None

# Core module imports with parallel initialization
numpy = import_module("numpy")
PIL_Image = import_module("PIL.Image", "Image")
PIL_ImageDraw = import_module("PIL.ImageDraw", "ImageDraw")
PIL_ImageFont = import_module("PIL.ImageFont", "ImageFont")
PIL_ImageOps = import_module("PIL.ImageOps", "ImageOps")
cv2 = import_module("cv2")
pyfiglet = import_module("pyfiglet")
yt_dlp = import_module("yt_dlp")
colorama = import_module("colorama")
rich = import_module("rich")
psutil = import_module("psutil")
pyvirtualdisplay = import_module("pyvirtualdisplay")

# Feature availability flags
HAS_NUMPY = numpy is not None
HAS_PIL = PIL_Image is not None
HAS_CV2 = cv2 is not None
HAS_PYFIGLET = pyfiglet is not None
HAS_YT_DLP = yt_dlp is not None
HAS_RICH = rich is not None

# Initialize colorama for cross-platform color support
if colorama:
    colorama.init(strip=False, convert=True)

# Rich console initialization with fail-safe behavior
if HAS_RICH:
    try:
        from rich.console import Console, Group
        from rich.panel import Panel
        from rich.table import Table
        from rich.text import Text
        from rich.align import Align
        from rich.prompt import Prompt, Confirm
        CONSOLE = Console(highlight=True)
    except ImportError:
        CONSOLE = None
else:
    CONSOLE = None

# System context singleton for global environment awareness
class SystemContext:
    """Global system context with environment detection and capability analysis.
    
    Provides unified access to system capabilities, terminal characteristics,
    and performance metrics with intelligent caching and platform awareness.
    """
    _instance = None
    
    @classmethod
    def get_instance(cls) -> 'SystemContext':
        """Access the singleton instance with lazy initialization."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def __init__(self) -> None:
        """Initialize system context with comprehensive capability detection."""
        # Terminal and environment detection
        self.attributes = self._detect_environment()
        self.capabilities = self._analyze_capabilities()
        self.constraints = self._determine_constraints()
    
    def _detect_environment(self) -> Dict[str, Any]:
        """Detect terminal and system environment with resilient fallbacks."""
        # Terminal dimensions with robust error handling
        try:
            dims = shutil.get_terminal_size()
            terminal_width, terminal_height = dims.columns, dims.lines
        except (AttributeError, OSError):
            terminal_width, terminal_height = 80, 24
        
        return {
            "terminal_width": terminal_width,
            "terminal_height": terminal_height,
            "interactive": sys.stdout.isatty(),
            "platform": platform.system(),
            "python_version": platform.python_version(),
            "cpu_count": os.cpu_count() or 2,
            "has_ipython": "IPython" in sys.modules,
            "timestamp": datetime.now()
        }
    
    def _analyze_capabilities(self) -> Dict[str, bool]:
        """Analyze system capabilities with progressive feature detection."""
        # Unicode detection
        encoding = sys.stdout.encoding or "ascii"
        supports_unicode = "utf" in encoding.lower() and not any(
            k in os.environ for k in ('NO_UNICODE', 'ASCII_ONLY')
        )
        
        # Color support detection
        supports_color = (
            bool(colorama) or 
            sys.platform != 'win32' or
            "ANSICON" in os.environ or
            os.environ.get('COLORTERM') in ('truecolor', '24bit') or
            os.environ.get('TERM', '').endswith(('color', '256color'))
        )
        
        # Network connectivity check
        has_network = False
        try:
            socket.create_connection(("1.1.1.1", 53), timeout=0.5)
            has_network = True
        except (socket.error, socket.timeout):
            pass
        
        # Performance tier calculation
        perf_tier = self._calculate_performance_tier()
        
        return {
            "can_display_unicode": supports_unicode,
            "can_display_color": supports_color,
            "has_numpy": HAS_NUMPY,
            "has_cv2": HAS_CV2,
            "has_pil": HAS_PIL,
            "has_rich": HAS_RICH,
            "has_network": has_network,
            "performance_tier": perf_tier,
            "hardware_acceleration": self._check_hardware_acceleration(),
        }
    
    def _check_hardware_acceleration(self) -> bool:
        """Check for hardware acceleration capabilities."""
        # CUDA detection via OpenCV
        if HAS_CV2 and hasattr(cv2, 'cuda') and hasattr(cv2.cuda, 'getCudaEnabledDeviceCount'):
            try:
                return cv2.cuda.getCudaEnabledDeviceCount() > 0
            except Exception:
                pass
        
        # Metal detection on macOS
        if platform.system() == "Darwin":
            return True
            
        return False
    
    def _calculate_performance_tier(self) -> int:
        """Calculate system performance tier (0-3) for adaptive optimization."""
        # Base score from CPU count
        cpu_count = os.cpu_count() or 2
        cpu_score = 0
        
        if cpu_count >= 16:
            cpu_score = 3
        elif cpu_count >= 8:
            cpu_score = 2
        elif cpu_count >= 4:
            cpu_score = 1
        
        # Memory analysis if available
        mem_score = 0
        if psutil:
            try:
                mem = psutil.virtual_memory()
                mem_gb = mem.total / (1024**3)
                
                if mem_gb >= 16:
                    mem_score = 2
                elif mem_gb >= 8:
                    mem_score = 1
            except Exception:
                pass
                
        # Hardware acceleration bonus
        accel_bonus = 1 if self._check_hardware_acceleration() else 0
        
        # Calculate overall tier with bounds
        tier = (cpu_score + mem_score + accel_bonus) // 2
        return max(0, min(tier, 3))
    
    def _determine_constraints(self) -> Dict[str, Any]:
        """Determine system constraints for adaptive rendering."""
        term_width = self.attributes["terminal_width"]
        term_height = self.attributes["terminal_height"]
        perf_tier = self.capabilities["performance_tier"]
        
        return {
            "limited_width": term_width < 60,
            "limited_height": term_height < 20,
            "max_art_width": term_width - (2 if term_width < 60 else 4),
            "max_art_height": term_height - (4 if term_height < 20 else 6),
            "max_scale_factor": min(4, max(1, perf_tier + 1)),
            "default_fps": 5 if perf_tier == 0 else 10 if perf_tier == 1 else 15,
            "performance_tier": perf_tier
        }
    
    @lru_cache(maxsize=8)
    def get_optimized_parameters(self, operation: str = "general") -> Dict[str, Any]:
        """Get context-aware optimized parameters for specific operations.
        
        Args:
            operation: Operation type ("general", "video", "image", "text")
            
        Returns:
            Dictionary of optimized parameters for current system
        """
        tier = self.capabilities["performance_tier"]
        limited = self.constraints["limited_width"]
        
        # Base parameters with progressive enhancement
        params = {
            "scale_factor": self.constraints["max_scale_factor"],
            "block_width": 4 if limited else 6 if tier >= 2 else 8,
            "block_height": 8 if limited else 6 if tier >= 2 else 8,
            "fps": self.constraints["default_fps"],
            "edge_mode": "enhanced" if tier > 0 else "simple",
            "color_mode": self.capabilities["can_display_color"],
            "animation_level": min(tier, 2),
            "max_width": self.constraints["max_art_width"],
            "max_height": self.constraints["max_art_height"],
        }
        
        # Operation-specific enhancements
        if operation == "video":
            params.update({
                "buffer_frames": 2 if tier <= 1 else 4,
                "preprocessing": tier >= 2,
                "parallel_decode": tier >= 1
            })
        elif operation == "image":
            params.update({
                "dithering": tier >= 2,
                "edge_threshold": 60 if tier == 0 else 50 if tier == 1 else 40,
                "algorithm": "sobel" if tier <= 1 else "scharr"
            })
        elif operation == "text":
            params.update({
                "font_cache_size": 4 if tier == 0 else 8 if tier == 1 else 16,
                "enable_effects": tier >= 1
            })
            
        return params

# Initialize system context singleton for global access
SYSTEM_CONTEXT = SystemContext.get_instance()

# ╔══════════════════════════════════════════════════════════════╗
# ║ 🌌 Hyperdimensional Environment & System Intelligence        ║
# ╚══════════════════════════════════════════════════════════════╝

# Core modules mapping with standardized capability detection
CORE_MODULES = {
    "numpy": ("numpy", "NumPy tensor operations"),
    "pillow": ("PIL_Image", "Image processing"),
    "opencv": ("cv2", "Computer vision operations"),
    "pyfiglet": ("pyfiglet", "ASCII art generation"),
    "yt_dlp": ("yt_dlp", "Media streaming"),
    "rich": ("rich", "Terminal rendering"),
    "psutil": ("psutil", "System monitoring")
}

class EnvContext:
    """Unified environment intelligence with multidimensional awareness.
    
    A thread-safe singleton providing comprehensive system intelligence
    with parallel detection, optimized caching, and dynamic configuration.
    Automatically adapts parameters based on available resources.
    
    Attributes:
        terminal (Dict[str, Any]): Terminal dimensions and capabilities
        runtime (Dict[str, Any]): Python runtime environment details
        hardware (Dict[str, Any]): System hardware specifications
        network (Dict[str, bool]): Network connectivity status
        modules (Dict[str, bool]): Available module capabilities
        capabilities (Dict[str, Any]): Synthesized capability flags
        constraints (Dict[str, Any]): System-aware operational constraints
    """
    _instance: Optional['EnvContext'] = None
    _lock: threading.RLock = threading.RLock()
    
    @classmethod
    def get(cls) -> 'EnvContext':
        """Access thread-safe singleton instance with lazy initialization.
        
        Returns:
            EnvContext: The global environment context
        """
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance
    
    def __init__(self) -> None:
        """Initialize environment with parallel capability detection."""
        # Terminal properties with fast detection
        self.terminal = self._detect_terminal()
        
        # Execute analysis tasks concurrently
        futures = {
            "runtime": THREAD_POOL.submit(self._analyze_runtime),
            "hardware": THREAD_POOL.submit(self._analyze_hardware),
            "network": THREAD_POOL.submit(self._analyze_network)
        }
        
        # Gather results with timeout protection
        self.runtime = self._safely_get(futures["runtime"], {})
        self.hardware = self._safely_get(futures["hardware"], {})
        self.network = self._safely_get(futures["network"], {"connected": False})
        
        # Module detection
        self.modules = {name: bool(globals()[module_name]) 
                       for name, (module_name, _) in CORE_MODULES.items()}
        
        # Calculate derived properties
        self._update_capabilities()
        self._update_constraints()

    def _detect_terminal(self) -> Dict[str, Any]:
        """Detect terminal properties with robust fallbacks."""
        try:
            dims = shutil.get_terminal_size()
            width, height = dims.columns, dims.lines
        except (AttributeError, OSError):
            width, height = 80, 24
            
        # Comprehensive environment-aware capability detection
        supports_unicode = (
            sys.stdout.encoding and 
            'utf' in sys.stdout.encoding.lower() and 
            not any(k in os.environ for k in ('NO_UNICODE', 'ASCII_ONLY'))
        )
        
        supports_color = (
            bool(colorama) or 
            sys.platform != 'win32' or 
            any(k in os.environ for k in ('ANSICON', 'WT_SESSION', 'FORCE_COLOR')) or
            os.environ.get('TERM_PROGRAM') in ('vscode', 'iTerm.app') or
            os.environ.get('COLORTERM') in ('truecolor', '24bit') or
            os.environ.get('TERM', '').endswith(('color', '256color'))
        )
        
        return {
            "width": width,
            "height": height,
            "interactive": sys.stdout.isatty(),
            "supports_unicode": supports_unicode,
            "supports_color": supports_color
        }
    
    def _analyze_runtime(self) -> Dict[str, Any]:
        """Analyze Python runtime environment and platform capabilities."""
        return {
            "platform": platform.system(),
            "architecture": platform.machine(),
            "python_version": tuple(map(int, platform.python_version().split('.'))),
            "hostname": platform.node(),
            "has_metal": platform.system() == "Darwin",
            "is_64bit": sys.maxsize > 2**32,
            "is_debug": hasattr(sys, 'gettrace') and sys.gettrace() is not None,
            "is_frozen": getattr(sys, 'frozen', False)
        }
    
    def _analyze_hardware(self) -> Dict[str, Any]:
        """Analyze hardware capabilities with parallel detection."""
        result = {
            "memory_total": 0,
            "memory_available": 0,
            "cpu_count": os.cpu_count() or 2,
            "cpu_physical": max(1, (os.cpu_count() or 2) // 2),
            "has_cuda": False,
            "numpy_optimized": False
        }
        
        # Memory analysis with psutil
        if psutil:
            try:
                mem = psutil.virtual_memory()
                result.update({
                    "memory_total": mem.total,
                    "memory_available": mem.available,
                    "cpu_count": psutil.cpu_count(logical=True) or result["cpu_count"],
                    "cpu_physical": psutil.cpu_count(logical=False) or result["cpu_physical"]
                })
            except Exception:
                pass
                
        # GPU acceleration detection
        if cv2 and hasattr(cv2, 'cuda'):
            try:
                result["has_cuda"] = cv2.cuda.getCudaEnabledDeviceCount() > 0
            except Exception:
                pass
                
        # NumPy optimization detection
        if numpy:
            try:
                config_info = str(numpy.show_config()) if hasattr(numpy, "show_config") else ""
                result["numpy_optimized"] = any(lib in config_info.lower() 
                                              for lib in ["mkl", "openblas", "accelerate", "blas"])
            except Exception:
                pass
                
        return result
    
    def _analyze_network(self) -> Dict[str, bool]:
        """Check network connectivity with fast timeout."""
        # Redundant check with multiple services for reliability
        for host, port in [("1.1.1.1", 53), ("8.8.8.8", 53)]:
            try:
                socket.create_connection((host, port), timeout=0.5).close()
                return {"connected": True}
            except (socket.error, socket.timeout):
                continue
        return {"connected": False}
    
    def _safely_get(self, future: Future, default: T) -> T:
        """Retrieve future results with timeout protection."""
        try:
            return future.result(timeout=0.5)
        except (TimeoutError, Exception):
            return default
    
    def _update_capabilities(self) -> None:
        """Synthesize capability flags from detected attributes."""
        perf_tier = self._calculate_performance_tier()
        
        self.capabilities = {
            "unicode": self.terminal["supports_unicode"],
            "color": self.terminal["supports_color"],
            "interactive": self.terminal["interactive"],
            "animations": self.terminal["interactive"] and self.terminal["supports_color"],
            "high_performance": perf_tier >= 2,
            "network": self.network.get("connected", False),
            "hardware_accel": self.hardware.get("has_cuda", False) or 
                            self.runtime.get("has_metal", False),
            "performance_tier": perf_tier,
            "has_rich": self.modules.get("rich", False),
            "has_numpy": self.modules.get("numpy", False),
            "has_cv2": self.modules.get("opencv", False),
            "has_yt_dlp": self.modules.get("yt_dlp", False)
        }
    
    def _update_constraints(self) -> None:
        """Calculate operational constraints based on capabilities."""
        term_width = self.terminal["width"]
        term_height = self.terminal["height"]
        perf_tier = self.capabilities["performance_tier"]
        
        self.constraints = {
            "limited_width": term_width < 60,
            "limited_height": term_height < 20,
            "max_art_width": term_width - (2 if term_width < 60 else 4),
            "max_art_height": term_height - (4 if term_height < 20 else 6),
            "max_scale_factor": min(4, max(1, perf_tier + 1)),
            "default_fps": 5 if perf_tier == 0 else 10 if perf_tier == 1 else 15,
            "max_memory_usage": min(1.0, 0.5 + (perf_tier * 0.2)),
            "parallel_tasks": max(2, min(8, self.hardware.get("cpu_count", 2)))
        }
    
    def _calculate_performance_tier(self) -> int:
        """Calculate system performance tier for adaptive optimization.
        
        Analyzes CPU, memory and acceleration capabilities to determine
        the appropriate performance tier for dynamic parameter scaling.
        
        Returns:
            int: Performance tier (0=minimal, 3=high-end)
        """
        cpu_count = self.hardware.get("cpu_count", 2)
        memory_gb = self.hardware.get("memory_available", 0) / (1024**3) if self.hardware.get("memory_available", 0) > 0 else 2
        
        cpu_score = 2 if cpu_count >= 16 else 1 if cpu_count >= 8 else -1 if cpu_count <= 2 else 0
        mem_score = 1 if memory_gb >= 16 else -1 if memory_gb <= 2 else 0
        accel_bonus = 1 if self.hardware.get("has_cuda", False) or self.runtime.get("has_metal", False) else 0
        
        return max(0, min(1 + cpu_score + mem_score + accel_bonus, 3))
    
    @lru_cache(maxsize=8)
    def get_optimal_params(self, operation: str = "general") -> Dict[str, Any]:
        """Get context-aware optimized parameters for specific operations.
        
        Provides intelligent default parameters based on current system
        capabilities and terminal constraints for different operations.
        
        Args:
            operation: Operation type ("general", "video", "image", "text")
            
        Returns:
            Dict[str, Any]: Optimized parameters for specified operation
        """
        tier = self.capabilities["performance_tier"]
        limited = self.constraints["limited_width"]
        
        # Base parameters with progressive enhancement
        params = {
            "scale_factor": self.constraints["max_scale_factor"],
            "block_width": 4 if limited else 6 if tier >= 2 else 8,
            "block_height": 8 if limited else 6 if tier >= 2 else 8,
            "fps": self.constraints["default_fps"],
            "edge_mode": "enhanced" if tier > 0 else "simple",
            "color_mode": self.capabilities["color"],
            "animation_level": min(tier, 2),
            "max_width": self.constraints["max_art_width"],
            "max_height": self.constraints["max_art_height"],
            "cache_ttl": 300 * (tier + 1)
        }
        
        # Operation-specific parameter optimization
        if operation == "video":
            params.update({
                "buffer_frames": 2 if tier <= 1 else 4,
                "preprocessing": tier >= 2,
                "parallel_decode": tier >= 1,
                "adaptive_quality": True,
                "quality_headroom": 0.1 * (tier + 1)
            })
        elif operation == "image":
            params.update({
                "dithering": tier >= 2,
                "edge_threshold": 60 if tier == 0 else 50 if tier == 1 else 40,
                "algorithm": "sobel" if tier <= 1 else "scharr",
                "denoise": tier >= 2,
                "contrast_boost": 1.0 + (0.1 * tier)
            })
        elif operation == "text":
            params.update({
                "font_cache_size": 4 if tier == 0 else 8 if tier == 1 else 16,
                "enable_effects": tier >= 1,
                "max_width_ratio": 0.8 + (0.05 * tier),
                "alignment": "center" if not limited else "left"
            })
            
        return params

# Initialize environment with thread safety
ENV = EnvContext.get()

# Project metadata with dynamic capability detection
AUTHOR_INFO = {
    "name": "Lloyd Handyside",
    "email": "ace1928@gmail.com",
    "org": "Neuroforge",
    "org_email": "lloyd.handyside@neuroforge.io",
    "contributors": ["Eidos <syntheticeidos@gmail.com>", "Prismatic Architect <prism@neuroforge.io>"],
    "version": "1.0.1",
    "updated": "2025-03-16",
    "codename": "Prismatic Cipher",
    "release_stage": "stable",
    "license": "MIT",
    "repository": "github.com/Ace1928/glyph_forge",
    "documentation": "https://neuroforge.io/docs/glyph_stream",
    "support": "https://neuroforge.io/support",
    "keywords": [
        "unicode-art", "terminal-graphics", "dimensional-rendering", 
        "reality-transmutation", "prismatic-encoding", "visual-transcendence"
    ],
    "capabilities": ENV.modules,
    "preferences": {
        "banner_style": "cosmic",
        "color_scheme": "prismatic",
        "edge_detection": "adaptive",
        "default_scale": 2,
        "show_banner_on_import": True
    }
}

# ╔══════════════════════════════════════════════════════════════╗
# ║ 🌠 Unified Dimensional Banner System                        ║
# ╚══════════════════════════════════════════════════════════════╝

class BannerEngine:
    """Dimensional banner system with adaptive rendering capabilities.
    
    Provides contextually-aware banner generation with intelligent style selection,
    caching, and terminal-adaptive layouts. Automatically adjusts to environment
    constraints while maintaining visual consistency.
    
    Attributes:
        terminal_width (int): Current terminal width in characters
        terminal_height (int): Current terminal height in lines
        supports_unicode (bool): Whether terminal supports Unicode characters
        supports_color (bool): Whether terminal supports ANSI color codes
        symbols (Dict[str, Dict[str, str]]): Symbol registry with unicode/ascii variants
    """
    _instance: Optional['BannerEngine'] = None
    _lock: threading.RLock = threading.RLock()
    _cache: Dict[str, Tuple[str, float]] = {}  # (content, creation_timestamp)
    _cache_ttl: float = 3600.0  # Cache lifetime in seconds
    _max_cache_entries: int = 16  # Bounded cache size
    
    @classmethod
    def get_instance(cls) -> 'BannerEngine':
        """Get singleton instance with thread-safe lazy initialization.
        
        Returns:
            BannerEngine: Thread-safe singleton instance
        """
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance
    
    def __init__(self) -> None:
        """Initialize banner engine with environment-aware configuration."""
        # Terminal capabilities detection
        self.terminal_width: int = ENV.terminal["width"]
        self.terminal_height: int = ENV.terminal["height"]
        self.supports_unicode: bool = ENV.capabilities["unicode"]
        self.supports_color: bool = ENV.capabilities["color"]
        self.has_figlet: bool = "pyfiglet" in ENV.modules
        
        # Create symbol registry with unicode/ascii variants
        self.symbols: Dict[str, Dict[str, str]] = {
            "unicode": {
                # Borders and structure
                "top_left": "╔", "top_right": "╗", "bottom_left": "╚", "bottom_right": "╝",
                "horizontal": "═", "vertical": "║", "inner_h": "━", "inner_v": "┃",
                # Decorative elements
                "cosmic": "⚝", "dimension": "⟁", "star": "✧", "star_alt": "✦", 
                "star_filled": "✫", "diamond": "◈", "bullet": "•", "circle": "○",
                # Semantic groupings
                "stars": "✧✦✫", "corners": "╔╗╚╝", "lines": "═║",
                "stylized_bullet": "◆", "stylized_star": "★",
            },
            "ascii": {
                # ASCII fallbacks
                "top_left": "+", "top_right": "+", "bottom_left": "+", "bottom_right": "+",
                "horizontal": "-", "vertical": "|", "inner_h": "=", "inner_v": "|",
                "cosmic": "#", "dimension": "+", "star": "*", "star_alt": "*",
                "star_filled": "*", "diamond": "<>", "bullet": "*", "circle": "O",
                "stars": "*-*", "corners": "++++", "lines": "-|",
                "stylized_bullet": "+", "stylized_star": "*",
            }
        }
    
    def display(self, style: str = "auto", color: bool = True, 
                width: Optional[int] = None, metadata: Optional[Dict[str, Any]] = None) -> None:
        """Display dimensional banner with environment-adaptive formatting.
        
        Renders a banner using the optimal style for current terminal dimensions
        and capabilities, with intelligent caching for performance.
        
        Args:
            style: Banner style ('auto', 'full', 'compact', 'minimal', 'ascii', 'cosmic')
            color: Whether to enable ANSI colors
            width: Custom width in characters (None=auto-detect)
            metadata: Optional custom content to include in banner
        """
        banner = self.generate(
            style if style != "auto" else self._select_optimal_style(width),
            color and self.supports_color,
            width or self.terminal_width,
            metadata
        )
        
        print(banner) if not CONSOLE else CONSOLE.print(banner)
    
    def generate(self, style: str = "cosmic", color: bool = True,
                width: Optional[int] = None, metadata: Optional[Dict[str, Any]] = None) -> str:
        """Generate dimensional banner with specified parameters.
        
        Creates a banner with the requested style, adapting to terminal constraints
        and applying semantic colors if supported. Uses efficient caching for
        repeated generations.
        
        Args:
            style: Banner style ('full', 'compact', 'minimal', 'ascii', 'cosmic')
            color: Whether to enable ANSI colors
            width: Custom width (None=auto-detect)
            metadata: Optional custom metadata to include
            
        Returns:
            str: Fully rendered banner text
        """
        # Parameter normalization with environment awareness
        width = width or self.terminal_width
        use_color = color and self.supports_color
        use_unicode = self.supports_unicode and style != "ascii"
        
        # Progressive style adaptation for constrained environments
        if width < 40:
            style = "minimal"  # Ultra compact for tiny terminals
        elif width < 80 and style in ("full", "cosmic"):
            style = "compact"  # Reduced size for small terminals
        
        # Force ASCII for non-Unicode environments
        if not use_unicode:
            style = "ascii"
        
        # Generate cache key with all relevant parameters
        cache_key = f"{style}:{width}:{use_color}:{use_unicode}"
        if metadata:
            cache_key += f":{hash(tuple(sorted(metadata.items())))}"
            
        # Check cache with thread safety and TTL validation
        with self._lock:
            if cache_key in self._cache:
                content, timestamp = self._cache[cache_key]
                if time.time() - timestamp < self._cache_ttl:
                    return content
        
        # Generate banner from template with complete context
        context = self._build_context(width, use_unicode, metadata)
        banner = self._get_template(style, width).format(**context)
        
        # Apply semantic colors if requested and supported
        if use_color and not CONSOLE:  # Skip if Rich will handle coloring
            banner = self._apply_colors(banner)
        
        # Update cache with bounded size management
        with self._lock:
            if len(self._cache) >= self._max_cache_entries:
                # Remove oldest entry
                oldest_key = min(self._cache, key=lambda k: self._cache[k][1])
                del self._cache[oldest_key]
            
            # Store with current timestamp
            self._cache[cache_key] = (banner, time.time())
        
        return banner
    
    def _select_optimal_style(self, width: Optional[int] = None) -> str:
        """Select most appropriate banner style based on terminal constraints.
        
        Args:
            width: Terminal width in characters (None=use current width)
            
        Returns:
            str: Best style for current environment
        """
        width = width or self.terminal_width
        preference = AUTHOR_INFO["preferences"].get("banner_style", "cosmic")
        
        # Progressive style selection logic
        if width < 40:
            return "minimal"  # Ultra compact
        elif width < 80:
            return "compact"  # Reduced size
        elif not self.supports_unicode:
            return "ascii"    # ASCII compatible
        elif preference in ("cosmic", "full") and width >= 100:
            return preference  # User preference when space allows
        else:
            return "cosmic" if width >= 90 else "compact"
    
    def _build_context(self, 
                      width: int,
                      use_unicode: bool,
                      custom_metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Build complete template context dictionary with all required values.
        
        Args:
            width: Terminal width for layout calculations
            use_unicode: Whether Unicode characters are supported
            custom_metadata: Optional user-provided metadata
            
        Returns:
            Dict[str, Any]: Complete context dictionary for template formatting
        """
        # Select appropriate symbol set
        symbols = self.symbols["unicode" if use_unicode else "ascii"]
        
        # Core context with project metadata
        context = {
            # Version information
            "version": AUTHOR_INFO["version"],
            "codename": AUTHOR_INFO["codename"],
            "release": AUTHOR_INFO.get("release_stage", "stable"),
            
            # Attribution
            "maintainer": AUTHOR_INFO["name"],
            "email": AUTHOR_INFO["email"],
            "organization": AUTHOR_INFO["org"],
            "org_email": AUTHOR_INFO.get("org_email", ""),
            
            # Layout dimensions
            "horizontal_line": symbols["horizontal"] * (width - 2),
            "separator_line": symbols["inner_h"] * (width - 4),
            "content_width": width - 4,
        }
        
        # Add symbols and custom metadata
        context.update(symbols)
        if custom_metadata:
            context.update(custom_metadata)
        
        # Generate title art with figlet when available
        if self.has_figlet:
            title_width = max(20, min(width - 6, 120))
            try:
                figlet = pyfiglet.Figlet(font="standard", width=title_width)
                title_art = figlet.renderText("GLYPH STREAM").rstrip()
                
                # Center title within available space
                if use_unicode:
                    lines = []
                    for line in title_art.splitlines():
                        padding = max(0, (width - 6 - len(line)) // 2)
                        lines.append(" " * padding + line)
                    context["title_art"] = "\n".join(lines)
                else:
                    context["title_art"] = title_art
            except Exception:
                context["title_art"] = "GLYPH STREAM"  # Fallback on error
        else:
            context["title_art"] = "GLYPH STREAM"  # Simple fallback
        
        return context
    
    def _get_template(self, style: str, width: int) -> str:
        """Get appropriate banner template based on selected style and width.
        
        Args:
            style: Banner style name
            width: Terminal width for layout decisions
            
        Returns:
            str: Template string with format placeholders
        """
        templates = {
            # Minimal ultra-compact style
            "minimal": (
                "{top_left}{horizontal_line}{top_right}\n"
                "{vertical} GlyphStream v{version} {vertical}\n"
                "{bottom_left}{horizontal_line}{bottom_right}"
            ),
            
            # Compact style for medium terminals
            "compact": (
                "{top_left}{horizontal_line}{top_right}\n"
                "{vertical} {stars} GLYPH STREAM v{version} - {codename} {stars} {vertical}\n"
                "{vertical} Dimensional Unicode Transmutation Engine {vertical}\n"
                "{bottom_left}{horizontal_line}{bottom_right}"
            ),
            
            # ASCII-compatible style
            "ascii": (
                "+{horizontal_line}+\n"
                "| GLYPH STREAM v{version} |\n"
                "| Dimensional Unicode Engine |\n"
                "| Codename: {codename} |\n"
                "+{horizontal_line}+"
            ),
            
            # Cosmic style with balanced decoration
            "cosmic": (
                "{top_left}{horizontal_line}{top_right}\n"
                "{vertical} {title_art} {vertical}\n"
                "{vertical}{separator_line}{vertical}\n"
                "{vertical}  {star}{star_alt}{cosmic} PRISMATIC CIPHER v{version} {cosmic}{star_alt}{star}  {vertical}\n"
                "{vertical}  {dimension} Transform reality through visual transcendence {dimension}  {vertical}\n"
                "{bottom_left}{horizontal_line}{bottom_right}\n"
                "{cosmic}{star} ADAPTIVE UNIVERSALITY MATRIX INITIALIZED {star}{cosmic}"
            ),
            
            # Full detailed style
            "full": (
                "{top_left}{horizontal_line}{top_right}\n"
                "{vertical} {title_art} {vertical}\n"
                "{vertical}{separator_line}{vertical}\n"
                "{vertical} {dimension} Prismatic Cipher Edition v{version} {dimension} {vertical}\n"
                "{vertical}  {star} DIMENSIONAL GLYPH TRANSMUTATION ENGINE {star}  {vertical}\n"
                "{vertical}{separator_line}{vertical}\n"
                "{vertical}  {bullet} Maintainer: {maintainer} 「{email}」 {vertical}\n"
                "{vertical}  {dimension} Organization: {organization} 「{org_email}」 {vertical}\n"
                "{bottom_left}{horizontal_line}{bottom_right}"
            )
        }
        
        return templates.get(style, templates["cosmic"])
    
    def _apply_colors(self, banner: str) -> str:
        """Apply semantic ANSI color codes to banner elements.
        
        Maps specific banner elements to appropriate colors using optimized
        pattern matching for visual consistency and emphasis.
        
        Args:
            banner: Uncolored banner string
            
        Returns:
            str: Banner with ANSI color codes applied
        """
        # Color palette with semantic mapping
        colors = {
            "title": "\033[96m",     # Bright Cyan for titles
            "emphasis": "\033[95m",   # Bright Magenta for emphasis
            "action": "\033[92m",     # Bright Green for actions
            "metadata": "\033[93m",   # Bright Yellow for metadata
            "symbol": "\033[35m",     # Magenta for symbols
            "border": "\033[34m",     # Blue for borders
            "reset": "\033[0m"        # Reset formatting
        }
        
        # Pattern-replacement pairs for semantic colorization
        patterns = [
            # Title elements
            (r"(GLYPH STREAM)", f"{colors['title']}\\1{colors['reset']}"),
            (r"(PRISMATIC CIPHER)", f"{colors['emphasis']}\\1{colors['reset']}"),
            (r"(DIMENSIONAL|TRANSMUTATION|ENGINE)", f"{colors['emphasis']}\\1{colors['reset']}"),
            (r"(Transform\s+reality|INITIALIZED)", f"{colors['action']}\\1{colors['reset']}"),
            
            # Metadata elements
            (r"(v\d+\.\d+\.\d+)", f"{colors['metadata']}\\1{colors['reset']}"),
            (r"(Maintainer:.*|Organization:.*)", f"{colors['metadata']}\\1{colors['reset']}"),
            
            # Symbols and borders (with non-capturing lookbehind/lookahead)
            (r"([✧✦✫⚝⟁•◈★☆✩])", f"{colors['symbol']}\\1{colors['reset']}"),
            (r"([╔╗╚╝═║┌┐└┘━┃+\-|])", f"{colors['border']}\\1{colors['reset']}")
        ]
        
        # Apply all patterns sequentially
        result = banner
        for pattern, replacement in patterns:
            result = re.sub(pattern, replacement, result)
            
        return result


# Initialize global banner engine singleton
BANNER = BannerEngine.get_instance()

def display_banner(style: str = "auto", color: bool = True, 
                  width: Optional[int] = None, metadata: Optional[Dict[str, Any]] = None) -> None:
    """Display dimensional banner with automatic environment adaptation.
    
    Renders a banner using optimal style selection, Unicode detection,
    and terminal-aware layout with semantic coloring.
    
    Args:
        style: Banner style ('auto', 'full', 'compact', 'minimal', 'ascii', 'cosmic')
        color: Whether to enable ANSI colors
        width: Custom width (None=auto-detect terminal width)
        metadata: Optional custom content to include in banner
    """
    BANNER.display(style, color, width, metadata)

class UnicodeRenderEngine:
    """Multidimensional Unicode rendering system with adaptive capabilities.
    
    A thread-safe, performance-optimized rendering engine providing contextual
    character selection based on terminal capabilities with intelligent fallbacks,
    efficient caching, and dimensional gradient mapping.
    
    Attributes:
        supports_unicode (bool): Whether terminal supports Unicode rendering
        supports_color (bool): Whether terminal supports ANSI color codes
        character_maps (Dict[str, Any]): Character sets for different rendering modes
    """
    _instance: Optional['UnicodeRenderEngine'] = None
    _lock: threading.RLock = threading.RLock()
    
    @classmethod
    def get_instance(cls) -> 'UnicodeRenderEngine':
        """Get thread-safe singleton instance with optimized initialization.
        
        Returns:
            UnicodeRenderEngine: Global singleton instance
        """
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance
    
    def __init__(self) -> None:
        """Initialize engine with environment detection and optimized maps."""
        # Core state with persistence strategy
        self.cache_path = Path(os.path.expanduser("~/.cache/glyph_stream/character_maps.json"))
        
        # Environment capability detection
        self.supports_unicode = ENV.capabilities["unicode"]
        self.supports_color = ENV.capabilities["color"]
        
        # Memory-optimized character maps with LRU caching
        self.character_maps = {}
        self._initialize_maps()
        
        # High-performance color cache with thread safety
        self._color_cache: Dict[Tuple[int, int, int, bool], str] = {}
    
    def _initialize_maps(self) -> None:
        """Initialize character maps with optimal loading strategy."""
        try:
            if self._load_cached():
                return
        except Exception:
            pass  # Fail gracefully to generation
            
        self._generate_maps()
        # Non-blocking persistence with low priority
        THREAD_POOL.submit(self._save_maps)
    
    def _load_cached(self) -> bool:
        """Load maps from persistent cache with validation.
        
        Returns:
            bool: True if successfully loaded
        """
        if not self.cache_path.exists():
            return False
            
        try:
            with open(self.cache_path, 'r', encoding='utf-8') as f:
                maps = json.load(f)
                # Validate essential keys
                if all(k in maps for k in ["gradient", "full_gradients", "edges"]):
                    self.character_maps = maps
                    return True
            return False
        except (json.JSONDecodeError, IOError):
            return False
    
    def _save_maps(self) -> None:
        """Persist maps to disk with error resilience."""
        try:
            self.cache_path.parent.mkdir(exist_ok=True, parents=True)
            with open(self.cache_path, 'w', encoding='utf-8') as f:
                json.dump(self.character_maps, f, ensure_ascii=False)
        except Exception:
            pass  # Non-critical operation
    
    def _generate_maps(self) -> None:
        """Generate optimized character maps for all dimensions."""
        self.character_maps = {
            # Core gradients with progressive detail
            "gradient": {
                "standard": "█▓▒░ ",
                "enhanced": "█▇▆▅▄▃▂▁▀ ",
                "blocks": "█▉▊▋▌▍▎▏ ",
                "braille": "⣿⣷⣯⣟⡿⢿⣻⣽⣾ ",
                "ascii": "@%#*+=-:. "
            },
            
            # Edge representation with directional accuracy
            "edges": {
                "horizontal": {"bold": "━", "standard": "─", "light": "╌", "ascii": "-"},
                "vertical": {"bold": "┃", "standard": "│", "light": "╎", "ascii": "|"},
                "diagonal_ne": {"bold": "╱", "standard": "╱", "ascii": "/"},
                "diagonal_nw": {"bold": "╲", "standard": "╲", "ascii": "\\"},
            },
            
            # Full character sets for different rendering contexts
            "full_gradients": {
                "standard": "█▓▒░■◆●◉○♦♥♠☻☺⬢⬡✿❀✣❄★☆✩✫✬ .  ",
                "minimal": "█▓▒░ ",
                "ascii_art": "@%#*+=-:. ",
                "braille": "⣿⣷⣯⣟⡿⢿⣻⣽⣾ "
            }
        }
    
    @lru_cache(maxsize=256)
    def get_ansi_color(self, r: int, g: int, b: int, bg: bool = False) -> str:
        """Generate 24-bit ANSI color sequence with optimized caching.
        
        Args:
            r: Red component (0-255)
            g: Green component (0-255)
            b: Blue component (0-255)
            bg: Whether this is a background color
            
        Returns:
            str: ANSI color escape sequence or empty string
        """
        if not self.supports_color:
            return ""
        
        # Normalize color components
        r = max(0, min(255, r))
        g = max(0, min(255, g))
        b = max(0, min(255, b))
        
        return f"\033[{48 if bg else 38};2;{r};{g};{b}m"
    
    def get_gradient(self, density: float, mode: str = "standard") -> str:
        """Select optimal character for density with capability awareness.
        
        Args:
            density: Value between 0.0 (empty) and 1.0 (solid)
            mode: Gradient style name
            
        Returns:
            str: Character representing requested density
        """
        # Normalize density with bounds protection
        density = max(0.0, min(1.0, density))
        
        # Select appropriate gradient based on capabilities
        gradient_key = mode
        if not self.supports_unicode and mode != "ascii":
            gradient_key = "ascii_art"
        elif mode not in self.character_maps["full_gradients"]:
            gradient_key = "standard"
            
        gradient = self.character_maps["full_gradients"].get(gradient_key, "█▓▒░ ")
        
        # Optimized single-index lookup with bounds protection
        index = min(int(density * (len(gradient) - 1)), len(gradient) - 1)
        return gradient[index]
    
    def get_edge_char(self, grad_x: float, grad_y: float, 
                     strength: float = 1.0, style: str = "standard") -> str:
        """Select optimal edge character based on gradient direction.
        
        Args:
            grad_x: X gradient component
            grad_y: Y gradient component
            strength: Edge intensity (0.0-1.0)
            style: Edge style name
            
        Returns:
            str: Character representing the edge direction
        """
        # Capability adaptation and input normalization
        if not self.supports_unicode:
            style = "ascii"
        strength = max(0.0, min(1.0, strength))
        
        # Flat region detection for optimization
        if abs(grad_x) < 1e-6 and abs(grad_y) < 1e-6:
            return "·" if self.supports_unicode else "."
        
        # Dynamic style selection based on edge strength
        actual_style = style
        if style == "standard" and strength != 1.0:
            actual_style = "bold" if strength > 0.8 else "light" if strength < 0.3 else "standard"
        
        # 8-way directional mapping with angle classification
        angle = math.degrees(math.atan2(grad_y, grad_x)) % 180
        edges = self.character_maps["edges"]
        
        # Fast path selection for common angles
        if angle < 22.5 or angle >= 157.5:
            return edges["horizontal"][actual_style]
        elif 67.5 <= angle < 112.5:
            return edges["vertical"][actual_style]
        elif 22.5 <= angle < 67.5:
            return edges["diagonal_ne"][actual_style]
        else:  # 112.5 <= angle < 157.5
            return edges["diagonal_nw"][actual_style]
    
    def get_text_width(self, text: str) -> int:
        """Calculate display width of text with Unicode and ANSI awareness.
        
        Args:
            text: Input text with possible ANSI escape sequences
            
        Returns:
            int: Visual width in character cells
        """
        # Strip ANSI escape sequences
        clean_text = re.sub(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])', '', text)
        
        width = 0
        for c in clean_text:
            # Fast path for ASCII
            if ord(c) < 127:
                width += 1
            # Handle wide characters and combining marks
            elif unicodedata.east_asian_width(c) in ('F', 'W'):
                width += 2
            elif unicodedata.category(c) not in ('Mn', 'Me', 'Cf'):
                width += 1
        return width
    
    def apply_color(self, text: str, r: int, g: int, b: int) -> str:
        """Apply color to text with automatic reset sequence.
        
        Args:
            text: Text to colorize
            r: Red component (0-255)
            g: Green component (0-255)
            b: Blue component (0-255)
            
        Returns:
            str: Colored text with reset sequence
        """
        if not self.supports_color or not text:
            return text
        return f"{self.get_ansi_color(r, g, b)}{text}{self.reset_code()}"
    
    def reset_code(self) -> str:
        """Get ANSI reset sequence if color is supported.
        
        Returns:
            str: ANSI reset code or empty string
        """
        return "\033[0m" if self.supports_color else ""
    
    def get_enhanced_gradient_chars(self) -> str:
        """Get full standard gradient character set for dimensional rendering.
        
        Returns:
            str: Gradient characters in density order
        """
        return self.character_maps["full_gradients"]["standard"]


# Initialize singleton instance with immediate availability
UNICODE_ENGINE = UnicodeRenderEngine.get_instance()

# Export API functions with semantic naming
get_ansi_color = UNICODE_ENGINE.get_ansi_color
get_edge_char = UNICODE_ENGINE.get_edge_char
get_gradient_char = UNICODE_ENGINE.get_gradient
get_enhanced_gradient_chars = UNICODE_ENGINE.get_enhanced_gradient_chars
reset_ansi = UNICODE_ENGINE.reset_code


# ╔══════════════════════════════════════════════════════════════╗
# ║ 📝 Text Processing Core                                     ║
# ╚══════════════════════════════════════════════════════════════╝

class GlyphTextEngine:
    """Dimensional text transmutation system with adaptive rendering.
    
    A thread-safe, multi-modal text processor providing dynamic font selection,
    progressive enhancement capabilities, and contextual awareness with optimized
    resource utilization and efficient caching.
    
    Attributes:
        fonts (List[str]): Available FIGlet fonts
        font_categories (Dict[str, List[str]]): Categorized font collections
    """
    _instance: Optional['GlyphTextEngine'] = None
    _lock: threading.RLock = threading.RLock()
    _CACHE_TTL: float = 3600.0  # Cache lifetime in seconds
    
    @classmethod
    def get_instance(cls) -> 'GlyphTextEngine':
        """Access the singleton instance with thread-safe initialization.
        
        Returns:
            GlyphTextEngine: The global engine instance
        """
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance
    
    def __init__(self) -> None:
        """Initialize the engine with context-aware configuration."""
        # Environment detection with optimized parameters
        params = ENV.get_optimal_params("text")
        self._terminal_width = ENV.terminal["width"]
        self._supports_color = ENV.capabilities["color"]
        self._supports_unicode = ENV.capabilities["unicode"]
        self._max_cache_size = params.get("font_cache_size", 16)
        self._enable_effects = params.get("enable_effects", True)
        
        # Thread-safe caching system
        self._render_cache: Dict[str, Tuple[List[str], float]] = {}
        self._font_cache: Dict[str, pyfiglet.Figlet] = {}
        
        # Font system initialization
        self.fonts = self._detect_available_fonts()
        self.font_categories = self._categorize_fonts()
            
    def _detect_available_fonts(self) -> List[str]:
        """Detect available FIGlet fonts with invalid font filtering.
        
        Returns:
            List[str]: Available valid font names
        """
        if not pyfiglet:
            return ["standard"]
            
        try:
            all_fonts = pyfiglet.FigletFont.getFonts()
            # Filter problematic fonts that cause rendering issues
            blacklist = {"eftichess", "eftifont", "eftirobot", "eftiwall"}
            return [font for font in all_fonts if font not in blacklist]
        except Exception:
            # Fallback to core fonts that are most commonly available
            return ["standard", "slant", "small", "big", "block", "digital"]
    
    def _categorize_fonts(self) -> Dict[str, List[str]]:
        """Organize fonts into semantic categories for intelligent selection.
        
        Returns:
            Dict[str, List[str]]: Map of category names to font lists
        """
        # Semantic categorization by stylistic properties
        categories: Dict[str, List[str]] = {
            "standard": ["standard", "slant", "small", "mini"],
            "bold": ["block", "big", "chunky", "epic", "doom", "larry3d"],
            "script": ["script", "slscript", "cursive"],
            "simple": ["small", "mini", "lean", "sub-zero", "tiny"],
            "tech": ["digital", "binary", "bubble", "hex", "cyberlarge"],
            "stylized": ["gothic", "graffiti", "cosmic", "isometric"],
            "symbols": ["banner3-D", "ivrit", "weird", "starwars"],
            "decorative": ["bubble", "banner", "dotmatrix", "broadway"],
            "ascii_only": ["standard", "small", "mini", "straight", "lean"],
        }
        
        # Add 'all' category containing all fonts
        categories["all"] = self.fonts
        
        # Ensure categories only contain valid fonts
        return {k: [f for f in v if f in self.fonts] for k, v in categories.items()}
    
    @lru_cache(maxsize=16)
    def get_figlet_instance(self, font: str) -> pyfiglet.Figlet:
        """Get cached FIGlet instance with intelligent fallbacks.
        
        Args:
            font: Font name to initialize
            
        Returns:
            pyfiglet.Figlet: Configured FIGlet renderer
        """
        try:
            return pyfiglet.Figlet(font=font)
        except Exception:
            # Silent fallback to standard font on error
            return pyfiglet.Figlet(font="standard")
    
    def text_to_figlet(
        self, 
        text: str, 
        font: str = "standard",
        width: Optional[int] = None,
        color: Optional[Tuple[int, int, int]] = None,
        justify: Literal["left", "center", "right"] = "left",
        direction: Literal["auto", "left-to-right", "right-to-left"] = "auto"
    ) -> List[str]:
        """Convert text to FIGlet art with intelligent caching and fallbacks.
        
        Args:
            text: Input text to render
            font: FIGlet font name (falls back to closest match if unavailable)
            width: Maximum width (defaults to terminal width)
            color: RGB color tuple (None=no color)
            justify: Text alignment direction
            direction: Text flow direction
            
        Returns:
            List[str]: Lines of rendered FIGlet text with optional coloring
        """
        # Parameter normalization and cache key generation
        width = width or self._terminal_width
        cache_key = f"{text}:{font}:{width}:{justify}:{direction}"
        
        # Thread-safe cache check with TTL validation
        with self._lock:
            current_time = time.time()
            if cache_key in self._render_cache:
                result, timestamp = self._render_cache[cache_key]
                if current_time - timestamp < self._CACHE_TTL:
                    return self._apply_color(result.copy(), color)
            
            # Ensure font is available or find closest alternative
            if font not in self.fonts:
                font = self._find_similar_font(font)
                
            # Core rendering with cached FIGlet instances
            try:
                figlet = self.get_figlet_instance(font)
                figlet.width = width
                figlet.justify = justify
                figlet.direction = direction
                rendered = figlet.renderText(text)
                result = [line.rstrip() for line in rendered.splitlines()]
            except Exception:
                # Silent recovery with standard font
                result = [text] if not pyfiglet else self._fallback_render(text, width)
            
            # Bounded cache management
            self._cache_result(cache_key, result, current_time)
            
        # Apply colorization if requested
        return self._apply_color(result, color)
    
    def _fallback_render(self, text: str, width: int) -> List[str]:
        """Fallback rendering when normal rendering fails.
        
        Args:
            text: Text to render
            width: Maximum width
            
        Returns:
            List[str]: Simple rendered text
        """
        try:
            # Try with standard font as fallback
            figlet = pyfiglet.Figlet(font="standard", width=width)
            rendered = figlet.renderText(text)
            return [line.rstrip() for line in rendered.splitlines()]
        except Exception:
            # Ultimate fallback is just plain text
            return [text]
    
    def _cache_result(self, key: str, value: List[str], timestamp: float) -> None:
        """Thread-safe cache update with bounded size management.
        
        Args:
            key: Cache lookup key
            value: Result to cache
            timestamp: Creation timestamp for TTL calculation
        """
        with self._lock:
            # Ensure cache doesn't exceed size limits
            if len(self._render_cache) >= self._max_cache_size:
                # Remove oldest entries
                sorted_keys = sorted(self._render_cache.keys(), 
                                   key=lambda k: self._render_cache[k][1])
                for old_key in sorted_keys[:len(sorted_keys)//4 + 1]:
                    self._render_cache.pop(old_key, None)
            
            # Add new entry
            self._render_cache[key] = (value, timestamp)
    
    def _find_similar_font(self, requested_font: str) -> str:
        """Find closest matching font using optimized lookup strategy.
        
        Args:
            requested_font: Font name to match
            
        Returns:
            str: Name of closest available font
        """
        # Fast path for category matches
        if requested_font in self.font_categories:
            fonts = self.font_categories[requested_font]
            return fonts[0] if fonts else "standard"
            
        # Fast path for exact match
        if requested_font in self.fonts:
            return requested_font
        
        # Normalize for comparison
        req_lower = requested_font.lower()
        
        # Try prefix matching for intuitive user experience
        prefix_matches = [f for f in self.fonts if f.lower().startswith(req_lower)]
        if prefix_matches:
            return prefix_matches[0]
        
        # Fall back to fast substring matching
        substring_matches = [f for f in self.fonts if req_lower in f.lower()]
        if substring_matches:
            return substring_matches[0]
            
        # Final fallback to standard
        return "standard"
    
    def _apply_color(self, lines: List[str], color: Optional[Tuple[int, int, int]]) -> List[str]:
        """Apply ANSI color to text with environment-aware capability detection.
        
        Args:
            lines: Text lines to colorize
            color: RGB color tuple (None=no color)
            
        Returns:
            List[str]: Colorized text lines
        """
        if not color or not self._supports_color:
            return lines
            
        r, g, b = color
        return [UNICODE_ENGINE.apply_color(line, r, g, b) if line else "" for line in lines]
    
    def render_multiline_art(
        self, 
        text: str,
        font: str = "standard",
        color: Optional[Tuple[int, int, int]] = None,
        width: Optional[int] = None,
        align: Literal["left", "center", "right"] = "left"
    ) -> List[str]:
        """Render multi-line text with consistent formatting per line.
        
        Args:
            text: Multi-line text to render
            font: FIGlet font name
            color: RGB color tuple
            width: Maximum width
            align: Text alignment
            
        Returns:
            List[str]: Combined rendered output with separators
        """
        lines = text.splitlines()
        if not lines:
            return []
            
        # Process each line with optimized batching
        result: List[str] = []
        for line in lines:
            if not line.strip():
                result.append("")
                continue
                
            rendered = self.text_to_figlet(
                line, font=font, width=width, color=color, justify=align
            )
            result.extend(rendered)
            result.append("")  # Line separator
            
        # Remove trailing empty line for cleaner output
        if result and not result[-1]:
            result.pop()
            
        return result
    
    def get_font_preview(
        self, 
        text: str = "abc ABC", 
        category: Optional[str] = None,
        max_fonts: int = 20
    ) -> Dict[str, List[str]]:
        """Generate font preview with sample text and category filtering.
        
        Args:
            text: Sample text to render
            category: Font category to filter by (None=all)
            max_fonts: Maximum number of fonts to preview
            
        Returns:
            Dict[str, List[str]]: Map of font names to rendered previews
        """
        # Select fonts based on category with optimized slicing
        fonts_to_preview = (self.font_categories.get(category, []) 
                          if category else self.fonts[:max_fonts])
        
        # Bounded selection for performance
        if len(fonts_to_preview) > max_fonts:
            fonts_to_preview = fonts_to_preview[:max_fonts]
        
        # Generate previews with parallel processing for larger sets
        if len(fonts_to_preview) > 5 and ENV.constraints.get("parallel_tasks", 2) > 1:
            previews = {}
            with ThreadPoolExecutor(max_workers=min(8, ENV.constraints.get("parallel_tasks", 2))) as executor:
                futures = {font: executor.submit(
                    self.text_to_figlet, text, font=font
                ) for font in fonts_to_preview}
                
                for font, future in futures.items():
                    try:
                        previews[font] = future.result()
                    except Exception:
                        continue
            return previews
        else:
            # Sequential generation for smaller sets
            return {font: self.text_to_figlet(text, font=font) 
                   for font in fonts_to_preview}
    
    def show_font_gallery(self, text: str = "Test", category: Optional[str] = None) -> None:
        """Display interactive font gallery with rich formatting.
        
        Args:
            text: Sample text to render
            category: Font category to display
        """
        # Generate previews with category filtering
        previews = self.get_font_preview(text, category, max_fonts=30)
        
        if CONSOLE:
            # Rich display with visual styling
            title = f"🔠 Font Gallery: {category or 'All'} ({len(previews)} fonts)"
            CONSOLE.print(Panel(title, style="cyan bold"))
            
            for font_name, lines in previews.items():
                font_panel = Panel("\n".join(lines), title=font_name, border_style="blue")
                CONSOLE.print(font_panel)
        else:
            # Fallback plain text display
            print(f"=== Font Gallery: {category or 'All'} ({len(previews)} fonts) ===")
            
            for font_name, lines in previews.items():
                print(f"\n--- {font_name} ---")
                print("\n".join(lines))
                print("\n" + "-" * 40)
                
    def get_random_font(self, category: Optional[str] = None) -> str:
        """Get random font name with optional category filtering.
        
        Args:
            category: Font category to select from (None=all fonts)
            
        Returns:
            str: Random font name from the selected category
        """
        # Select font list based on category with fallback
        fonts = (self.font_categories.get(category, []) 
                if category in self.font_categories else self.fonts)
        
        # Select with pseudorandom quality
        if not fonts:
            return "standard"
            
        import random
        return random.choice(fonts)
    
    def get_font_categories(self) -> List[str]:
        """Get available font categories for selection.
        
        Returns:
            List[str]: Available font category names
        """
        return list(self.font_categories.keys())
    
    def get_font_information(self, font: str) -> Dict[str, Any]:
        """Get detailed information about a specific font with sample rendering.
        
        Args:
            font: Font name to query
            
        Returns:
            Dict[str, Any]: Font metadata including name, categories and sample
        """
        # Handle font not found with graceful fallback
        if font not in self.fonts:
            closest = self._find_similar_font(font)
            return {
                "name": closest,
                "found": False,
                "similar_to": font,
                "categories": [cat for cat, fonts in self.font_categories.items() 
                              if closest in fonts and cat != "all"],
                "sample": self.text_to_figlet("Sample", font=closest)
            }
        
        # Return comprehensive information for available font
        return {
            "name": font,
            "found": True,
            "categories": [cat for cat, fonts in self.font_categories.items() 
                          if font in fonts and cat != "all"],
            "sample": self.text_to_figlet("Sample", font=font),
            "available_sizes": ["small", "standard", "large"] 
                               if self._enable_effects else ["standard"]
        }


# Initialize the text engine with singleton access pattern
TEXT_ENGINE = GlyphTextEngine.get_instance()

# ╔══════════════════════════════════════════════════════════════╗
# ║ 🎭 Text Transformation Functions                            ║
# ╚══════════════════════════════════════════════════════════════╝

def text_to_art(
    text: str,
    font: str = "standard",
    color: Optional[Union[str, Tuple[int, int, int]]] = None,
    width: Optional[int] = None,
    align: Literal["left", "center", "right"] = "left"
) -> List[str]:
    """Convert text to ASCII/FIGlet art with optimal rendering.

    Args:
        text: Text to convert
        font: FIGlet font name or category
        color: RGB color tuple or color name
        width: Maximum width in characters (None for terminal width)
        align: Text alignment direction

    Returns:
        List[str]: Lines of rendered text art
    """
    rgb_color = resolve_color(color)
    return TEXT_ENGINE.text_to_figlet(
        text=text, font=font, width=width, color=rgb_color, justify=align
    )


def resolve_color(color: Optional[Union[str, Tuple[int, int, int]]]) -> Optional[Tuple[int, int, int]]:
    """Convert color name or RGB tuple to normalized RGB values.

    Args:
        color: Color name (string) or RGB tuple

    Returns:
        Optional[Tuple[int, int, int]]: Normalized RGB values or None
    """
    if color is None:
        return None
    
    if isinstance(color, str):
        return COLOR_MAP.get(color.lower(), COLOR_MAP["white"])
    
    # Return normalized RGB tuple
    return tuple(max(0, min(255, c)) for c in color)  # type: ignore


def render_styled_text(
    text: str,
    font: str = "standard",
    color: Optional[Union[str, Tuple[int, int, int]]] = None,
    width: Optional[int] = None,
    align: Literal["left", "center", "right"] = "center",
    add_border: bool = False,
    padding: int = 0,
    style: Literal["single", "double", "rounded", "bold"] = "single"
) -> List[str]:
    """Render text with comprehensive styling options.

    Creates fully styled text with controlled parameters for color,
    borders and padding with intelligent parameter normalization.

    Args:
        text: Text to render
        font: FIGlet font name
        color: Color name or RGB tuple
        width: Maximum width
        align: Text alignment
        add_border: Add decorative border
        padding: Padding around text (0-10)
        style: Border style when add_border is True

    Returns:
        List[str]: Lines of styled text art
    """
    rgb_color = resolve_color(color)
    
    # Generate base text art
    lines = TEXT_ENGINE.text_to_figlet(
        text=text, font=font, width=width, color=rgb_color, justify=align
    )
    
    # Apply padding with bounds enforcement
    padding = max(0, min(10, padding))
    if padding > 0:
        padded_lines = [""] * padding  # Top padding
        padded_lines.extend(f"{' ' * padding}{line}{' ' * padding}" for line in lines)
        padded_lines.extend([""] * padding)  # Bottom padding
        lines = padded_lines
    
    # Apply border if requested
    return add_unicode_border(lines, rgb_color, style) if add_border else lines


def add_unicode_border(
    lines: List[str], 
    color: Optional[Tuple[int, int, int]] = None,
    style: Literal["single", "double", "rounded", "bold"] = "single"
) -> List[str]:
    """Add decorative Unicode border around text with style options.

    Args:
        lines: Text lines to frame
        color: Border RGB color
        style: Border style preset

    Returns:
        List[str]: Text with border applied
    """
    if not lines:
        return []
    
    # Calculate maximum visible line width
    width = max((UNICODE_ENGINE.get_text_width(line) for line in lines), default=0)
    
    # Border character sets by style
    borders = {
        "single": ("╔", "╗", "╚", "╝", "═", "║"),
        "double": ("╔", "╗", "╚", "╝", "═", "║"),  # Actually uses Box Drawings Heavy
        "rounded": ("╭", "╮", "╰", "╯", "─", "│"),
        "bold": ("┏", "┓", "┗", "┛", "━", "┃"),
    }
    
    # Select border set with fallback
    top_left, top_right, bottom_left, bottom_right, horizontal, vertical = borders.get(
        style, borders["single"]
    )
    
    # Generate border lines with optional color
    top_border = f"{top_left}{horizontal * (width + 2)}{top_right}"
    bottom_border = f"{bottom_left}{horizontal * (width + 2)}{bottom_right}"
    
    if color is not None and ENV.capabilities["color"]:
        r, g, b = color
        top_border = UNICODE_ENGINE.apply_color(top_border, r, g, b)
        bottom_border = UNICODE_ENGINE.apply_color(bottom_border, r, g, b)
        vertical_colored = UNICODE_ENGINE.apply_color(vertical, r, g, b)
    else:
        vertical_colored = vertical
    
    # Efficient line building with padding
    result = [top_border]
    for line in lines:
        line_width = UNICODE_ENGINE.get_text_width(line)
        padding = " " * (width - line_width)
        result.append(f"{vertical_colored} {line}{padding} {vertical_colored}")
    
    result.append(bottom_border)
    return result


def generate_text_art(
    text: str,
    mode: Union[str, TextStyle] = TextStyle.STYLED,
    font: Optional[str] = None,
    color: Optional[Union[str, Tuple[int, int, int]]] = None
) -> List[str]:
    """Generate text art with intelligent preset modes.

    Creates text art with preconfigured style combinations using both
    direct and enum-based mode selection for flexible usage patterns.

    Args:
        text: Text to render
        mode: Rendering style preset (string or TextStyle)
        font: FIGlet font or category (None for mode-specific default)
        color: Text color name or RGB values

    Returns:
        List[str]: Rendered text art
    """
    # Convert string mode to enum if needed
    if isinstance(mode, str):
        try:
            mode = TextStyle[mode.upper()]
        except KeyError:
            mode = TextStyle.STYLED
    
    # Apply mode-specific parameters
    if mode == TextStyle.SIMPLE:
        return render_styled_text(text, font or "standard", None, add_border=False)
        
    elif mode == TextStyle.STYLED:
        return render_styled_text(
            text, font or "slant", color or "cyan", add_border=True, padding=1
        )
        
    elif mode == TextStyle.RAINBOW:
        # Specialized rainbow rendering with optimized approach
        selected_font = font or TEXT_ENGINE.get_random_font("bold")
        rendered = TEXT_ENGINE.text_to_figlet(text, font=selected_font)
        
        rainbow_lines = []
        for line_idx, line in enumerate(rendered):
            if not line.strip():
                rainbow_lines.append("")
                continue
                
            hue_shift = line_idx * 15  # Color variety between lines
            
            # Generate the rainbow-colored line
            chars = []
            for i, char in enumerate(line):
                if char.strip():
                    hue = (i * 10 + hue_shift) % 360
                    r, g, b = hsv_to_rgb(hue/360, 1.0, 1.0)
                    chars.append(UNICODE_ENGINE.apply_color(char, r, g, b))
                else:
                    chars.append(char)
                    
            rainbow_lines.append("".join(chars))
        
        return add_unicode_border(rainbow_lines, (255, 255, 255), "rounded")
        
    elif mode == TextStyle.RANDOM:
        # Generate randomized styling parameters
        selected_font = font or TEXT_ENGINE.get_random_font()
        border_style = random.choice(["single", "rounded", "bold"])
        border = bool(random.getrandbits(1))  # 50% chance of border
        
        # Random vibrant color if not specified
        if color is None:
            # Generate vibrant colors (avoid dark/muted values)
            r, g, b = hsv_to_rgb(random.random(), 0.8, 1.0)  
            selected_color = (r, g, b)
        else:
            selected_color = color
        
        return render_styled_text(
            text, selected_font, selected_color, 
            add_border=border, padding=random.choice([0, 1, 2]),
            style=border_style
        )
    
    # Fallback for any unexpected case
    return text_to_art(text, font or "standard")


def hsv_to_rgb(h: float, s: float, v: float) -> Tuple[int, int, int]:
    """Convert HSV color to RGB components with optimized algorithm.

    Args:
        h: Hue (0.0-1.0)
        s: Saturation (0.0-1.0)
        v: Value (0.0-1.0)

    Returns:
        Tuple[int, int, int]: RGB components (0-255)
    """
    # Normalize inputs to valid ranges
    h = max(0.0, min(1.0, h))
    s = max(0.0, min(1.0, s))
    v = max(0.0, min(1.0, v))
    
    # Fast path for grayscale
    if s <= 0.0:
        v_byte = int(v * 255)
        return (v_byte, v_byte, v_byte)
    
    h *= 6.0
    i = int(h)
    f = h - i
    
    # Pre-compute common values
    p, q, t = v * (1.0 - s), v * (1.0 - s * f), v * (1.0 - s * (1.0 - f))
    
    # Mapping based on hue sector with lookup table for optimal performance
    rgb = [
        (v, t, p),  # i=0
        (q, v, p),  # i=1
        (p, v, t),  # i=2
        (p, q, v),  # i=3
        (t, p, v),  # i=4
        (v, p, q),  # i=5
    ][i % 6]
    
    # Convert to byte values
    return (int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255))


# Font visualization and discovery functions 
@lru_cache(maxsize=4)
def show_all_fonts(text: str = "Sample", category: Optional[str] = None) -> None:
    """Display interactive gallery of available fonts with categorization.

    Args:
        text: Sample text to render
        category: Font category filter (None to show all categories)
    """
    TEXT_ENGINE.show_font_gallery(text, category)


def list_font_categories() -> List[str]:
    """List available font categories with rich formatting when available.

    Returns:
        List[str]: Available font category names
    """
    categories = TEXT_ENGINE.get_font_categories()
    
    descriptions = {
        "standard": "Common readable fonts",
        "bold": "Heavy weight striking fonts",
        "script": "Flowing cursive-style fonts",
        "simple": "Minimal, space-efficient fonts",
        "tech": "Technology and computer themed",
        "stylized": "Highly decorative unique fonts",
        "symbols": "Special character based fonts",
        "decorative": "Ornamental display fonts",
        "ascii_only": "Compatible with limited terminals",
        "all": "Complete font collection"
    }
    
    if CONSOLE:
        table = Table(title="🔠 Font Categories")
        table.add_column("Category", style="cyan bold")
        table.add_column("Description", style="green")
        table.add_column("Count", style="magenta")
        
        for cat in sorted(categories):
            fonts = TEXT_ENGINE.font_categories.get(cat, [])
            table.add_row(cat, descriptions.get(cat, ""), f"{len(fonts)} fonts")
            
        CONSOLE.print(table)
    else:
        # Clean plain text fallback
        print("\n=== Font Categories ===")
        for cat in sorted(categories):
            fonts = TEXT_ENGINE.font_categories.get(cat, [])
            desc = descriptions.get(cat, "")
            print(f"• {cat}: {len(fonts)} fonts - {desc}")
            
    return categories

class ImageProcessor:
    """Vectorized image processor with adaptive edge detection algorithms.

    A thread-safe singleton providing optimized tensor operations for image
    manipulation and edge detection with intelligent algorithm selection
    and caching strategies.

    Attributes:
        system_tier (int): Performance tier of system (0-3)
        max_dim (int): Maximum allowed image dimension
    """
    _instance: Optional['ImageProcessor'] = None
    _lock: threading.RLock = threading.RLock()
    
    @classmethod
    def get_instance(cls) -> 'ImageProcessor':
        """Get thread-safe singleton instance with lazy initialization.

        Returns:
            ImageProcessor: Global processor instance
        """
        with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance
    
    def __init__(self) -> None:
        """Initialize processor with optimized kernels and system-aware parameters."""
        self._cache: Dict[str, Any] = {}
        self._kernels = self._build_kernels()
        
        # System-calibrated processing parameters
        self.system_tier = ENV.capabilities.get("performance_tier", 1)
        self.max_dim = 8192 if self.system_tier >= 2 else 4096
        
        # Algorithm dispatcher for efficient edge detection
        self._algo_map = {
            EdgeDetector.SOBEL: self._detect_sobel,
            EdgeDetector.PREWITT: self._detect_prewitt,
            EdgeDetector.SCHARR: self._detect_scharr, 
            EdgeDetector.LAPLACIAN: self._detect_laplacian,
            EdgeDetector.CANNY: self._detect_canny
        }
    
    def _build_kernels(self) -> Dict[str, np.ndarray]:
        """Build optimized convolution kernels with pre-normalization.

        Returns:
            Dict[str, np.ndarray]: Named convolution kernels
        """
        return {
            # Edge detection kernels
            "sobel_x": np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),
            "sobel_y": np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),
            "prewitt_x": np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),
            "prewitt_y": np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),
            "scharr_x": np.array([[-3, 0, 3], [-10, 0, 10], [-3, 0, 3]]),
            "scharr_y": np.array([[-3, -10, -3], [0, 0, 0], [3, 10, 3]]),
            "laplacian": np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]]),
            
            # Enhancement kernels with optimized weights
            "gaussian": np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16,
            "sharpen": np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
        }
    
    @lru_cache(maxsize=16)
    def supersample_image(self, image: Image.Image, scale_factor: int) -> Image.Image:
        """Upscale image with system-optimal resampling method.

        Args:
            image: Source PIL image
            scale_factor: Integer multiplier for dimensions

        Returns:
            Image.Image: Upscaled image
        """
        # Parameter normalization
        scale = max(1, min(4, int(scale_factor)))
        
        # Dimensional safety check
        new_width = int(image.width * scale)
        new_height = int(image.height * scale)
        
        if new_width > self.max_dim or new_height > self.max_dim:
            scale_factor = min(self.max_dim / image.width, self.max_dim / image.height)
            new_width = int(image.width * scale_factor)
            new_height = int(image.height * scale_factor)
        
        # Select optimal resampling algorithm based on system tier
        resampling = Image.LANCZOS if self.system_tier >= 2 else Image.BILINEAR
        return image.resize((new_width, new_height), resampling)
    
    def rgb_to_gray(self, image_array: np.ndarray) -> np.ndarray:
        """Convert RGB to perceptually-accurate grayscale (ITU-R BT.601).

        Args:
            image_array: RGB numpy array (H×W×3) or grayscale (H×W)

        Returns:
            np.ndarray: Grayscale numpy array (H×W)
        """
        # Fast path for already grayscale images
        if len(image_array.shape) == 2:
            return image_array
        
        # Vectorized dot product with perceptual weights
        return np.dot(image_array[..., :3], [0.2126, 0.7152, 0.0722]).astype(np.uint8)
    
    def enhance_image(self, 
                     image_array: np.ndarray, 
                     contrast: float = 1.0,
                     brightness: float = 0.0,
                     denoise: bool = False) -> np.ndarray:
        """Apply adaptive image enhancements with vectorized operations.

        Args:
            image_array: Input image as numpy array
            contrast: Contrast adjustment factor (0.5-2.0, 1.0=neutral)
            brightness: Brightness adjustment (-128 to +128)
            denoise: Apply Gaussian noise reduction

        Returns:
            np.ndarray: Enhanced image array
        """
        # Fast path for identity operations
        if contrast == 1.0 and brightness == 0 and not denoise:
            return image_array
            
        # Parameter normalization
        contrast = np.clip(contrast, 0.5, 2.0)
        brightness = np.clip(brightness, -128, 128)
        
        # Create working copy with float32 precision
        result = image_array.astype(np.float32)
        
        # Apply contrast and brightness in one pass
        if contrast != 1.0 or brightness != 0:
            f = 259 * (contrast * 255 + 255) / (255 * (259 - contrast * 255))
            result = np.clip(f * (result - 128) + 128 + brightness, 0, 255)
            
        # Apply denoising with optimized kernel convolution
        if denoise:
            kernel = self._kernels["gaussian"]
            
            if len(result.shape) == 2:
                # Single-channel fast path
                result = self._convolve(result, kernel)
            else:
                # Multi-channel processing with reduced memory usage
                for c in range(result.shape[2]):
                    result[:,:,c] = self._convolve(result[:,:,c], kernel)
                    
        return result.astype(np.uint8)
    
    def detect_edges(self, 
                    gray_array: np.ndarray, 
                    algorithm: Union[str, EdgeDetector] = EdgeDetector.SOBEL,
                    threshold: Optional[int] = None) -> GradientResult:
        """Detect edges with optimized multi-algorithm selection.

        Args:
            gray_array: Grayscale image as numpy array
            algorithm: Edge detection algorithm or name
            threshold: Edge sensitivity threshold (None=auto)

        Returns:
            GradientResult: Magnitude and directional components
        """
        # Normalize algorithm type
        if isinstance(algorithm, str):
            algorithm = self._parse_algorithm(algorithm)
            
        # Auto-threshold based on image statistics
        if threshold is None:
            mean_value = np.mean(gray_array)
            threshold = int(40 + (mean_value / 5))
            
        # Dispatch to specialized algorithm
        detector = self._algo_map.get(algorithm, self._detect_sobel)
        return detector(gray_array, threshold)
    
    def _parse_algorithm(self, algorithm_name: str) -> EdgeDetector:
        """Convert algorithm name to enum with resilient fallback.

        Args:
            algorithm_name: Algorithm name string

        Returns:
            EdgeDetector: Algorithm enum
        """
        name = algorithm_name.upper()
        try:
            return EdgeDetector[name]
        except (KeyError, AttributeError):
            # Fuzzy matching for common misspellings
            for algo in EdgeDetector:
                if algo.name.startswith(name[:3]):
                    return algo
            return EdgeDetector.SOBEL
    
    def _convolve(self, channel: np.ndarray, kernel: np.ndarray) -> np.ndarray:
        """Apply convolution with optimized sliding window approach.

        Args:
            channel: Single channel image data
            kernel: Convolution kernel

        Returns:
            np.ndarray: Convolved result
        """
        # Boundary handling with edge padding
        padded = np.pad(channel, ((1, 1), (1, 1)), mode='edge')
        
        # Efficient sliding window view for vectorized operations
        windows = np.lib.stride_tricks.sliding_window_view(padded, kernel.shape)
        
        # Tensor contraction for fast convolution
        return np.tensordot(windows, kernel, axes=([2, 3], [0, 1]))
    
    def _detect_sobel(self, gray_array: np.ndarray, threshold: int) -> GradientResult:
        """Apply Sobel edge detection with vectorized gradients.

        Args:
            gray_array: Grayscale image data
            threshold: Edge sensitivity threshold

        Returns:
            GradientResult: Edge detection components
        """
        # Get gradient components with optimized convolution
        grad_x = self._convolve(gray_array, self._kernels["sobel_x"])
        grad_y = self._convolve(gray_array, self._kernels["sobel_y"])
        
        # Process with unified gradient pipeline
        return self._process_gradients(grad_x, grad_y, threshold)
    
    def _detect_prewitt(self, gray_array: np.ndarray, threshold: int) -> GradientResult:
        """Apply Prewitt edge detection for noise-stable edges.

        Args:
            gray_array: Grayscale image data
            threshold: Edge sensitivity threshold

        Returns:
            GradientResult: Edge detection components
        """
        # Get gradient components
        grad_x = self._convolve(gray_array, self._kernels["prewitt_x"])
        grad_y = self._convolve(gray_array, self._kernels["prewitt_y"])
        
        return self._process_gradients(grad_x, grad_y, threshold)
    
    def _detect_scharr(self, gray_array: np.ndarray, threshold: int) -> GradientResult:
        """Apply Scharr edge detection for improved rotational symmetry.

        Args:
            gray_array: Grayscale image data
            threshold: Edge sensitivity threshold

        Returns:
            GradientResult: Edge detection components
        """
        # Get gradient components
        grad_x = self._convolve(gray_array, self._kernels["scharr_x"])
        grad_y = self._convolve(gray_array, self._kernels["scharr_y"])
        
        return self._process_gradients(grad_x, grad_y, threshold)
    
    def _detect_laplacian(self, gray_array: np.ndarray, threshold: int) -> GradientResult:
        """Apply Laplacian edge detection for omnidirectional edges.

        Args:
            gray_array: Grayscale image data
            threshold: Edge sensitivity threshold

        Returns:
            GradientResult: Edge detection components
        """
        # Preprocess with Gaussian to reduce noise sensitivity
        if self.system_tier >= 1:
            gray_array = self._convolve(gray_array, self._kernels["gaussian"])
            
        # Apply Laplacian operator
        laplacian = self._convolve(gray_array, self._kernels["laplacian"])
        
        # Get directional information from Sobel (more reliable)
        grad_x = self._convolve(gray_array, self._kernels["sobel_x"])
        grad_y = self._convolve(gray_array, self._kernels["sobel_y"])
        
        # Create result with Laplacian magnitude but Sobel direction
        magnitude = self._normalize_gradient(np.abs(laplacian), threshold)
        direction = np.arctan2(grad_y, grad_x)
        
        return {
            "magnitude": magnitude,
            "gradient_x": grad_x,
            "gradient_y": grad_y,
            "direction": direction
        }
    
    def _detect_canny(self, gray_array: np.ndarray, threshold: int) -> GradientResult:
        """Apply Canny edge detection with non-maximum suppression.

        Args:
            gray_array: Grayscale image data
            threshold: High threshold value for hysteresis

        Returns:
            GradientResult: Edge detection components
        """
        # Start with Sobel gradients
        result = self._detect_sobel(gray_array, 0)
        grad = result["magnitude"].astype(np.float32)
        grad_x, grad_y = result["gradient_x"], result["gradient_y"]
        theta = result["direction"]
        
        # Quantize angles to 4 directions (0°, 45°, 90°, 135°)
        angle = (np.round(theta * (4/np.pi)) % 4).astype(np.uint8)
        
        # Non-maximum suppression (edge thinning)
        height, width = grad.shape
        suppressed = np.zeros_like(grad)
        
        # Vectorize significant gradient processing
        y_indices, x_indices = np.where(grad > 10)
        
        # Process edge pixels with direction-aware suppression
        for i, j in zip(y_indices, x_indices):
            # Skip boundary pixels
            if i == 0 or i == height-1 or j == 0 or j == width-1:
                continue
                
            # Get direction-based neighbors
            if angle[i, j] == 0:      # Horizontal
                n1, n2 = grad[i, j-1], grad[i, j+1]
            elif angle[i, j] == 1:    # Diagonal ↗
                n1, n2 = grad[i+1, j-1], grad[i-1, j+1]
            elif angle[i, j] == 2:    # Vertical
                n1, n2 = grad[i-1, j], grad[i+1, j]
            else:                     # Diagonal ↖
                n1, n2 = grad[i-1, j-1], grad[i+1, j+1]
                
            # Keep only local maxima along gradient direction
            if grad[i, j] >= max(n1, n2):
                suppressed[i, j] = grad[i, j]
        
        # Hysteresis thresholding with dual thresholds
        low_threshold = threshold // 2
        strong_edges = suppressed >= threshold
        weak_edges = (suppressed >= low_threshold) & (suppressed < threshold)
        
        # Initialize with strong edges
        edges = np.zeros_like(suppressed, dtype=np.uint8)
        edges[strong_edges] = 255
        
        # Connect weak edges to strong ones (8-connectivity)
        if np.any(weak_edges):
            # Find weak edge pixels
            weak_y, weak_x = np.where(weak_edges)
            
            # Check each weak edge for connection to strong edge
            for i, j in zip(weak_y, weak_x):
                # Get 3×3 neighborhood with bounds checking
                neighborhood = strong_edges[
                    max(0, i-1):min(height, i+2),
                    max(0, j-1):min(width, j+2)
                ]
                
                # Connect if adjacent to any strong edge
                if np.any(neighborhood):
                    edges[i, j] = 255
        
        return {
            "magnitude": edges,
            "gradient_x": grad_x,
            "gradient_y": grad_y,
            "direction": theta
        }
    
    def _process_gradients(self, 
                          grad_x: np.ndarray, 
                          grad_y: np.ndarray, 
                          threshold: int) -> GradientResult:
        """Process gradient components into coherent result.

        Args:
            grad_x: X-gradient component
            grad_y: Y-gradient component
            threshold: Edge sensitivity threshold

        Returns:
            GradientResult: Processed gradient components
        """
        # Compute gradient magnitude with optimized hypot
        grad = np.hypot(grad_x, grad_y)
        
        # Calculate direction for edge orientation
        direction = np.arctan2(grad_y, grad_x)
        
        # Normalize and threshold the magnitude
        magnitude = self._normalize_gradient(grad, threshold)
        
        return {
            "magnitude": magnitude,
            "gradient_x": grad_x,
            "gradient_y": grad_y,
            "direction": direction
        }
    
    def _normalize_gradient(self, gradient: np.ndarray, threshold: int) -> np.ndarray:
        """Normalize gradient and apply threshold with zero-division protection.

        Args:
            gradient: Raw gradient data
            threshold: Edge sensitivity threshold

        Returns:
            np.ndarray: Normalized uint8 gradient
        """
        # Handle empty gradient case
        if gradient.size == 0 or np.max(gradient) <= 0:
            return np.zeros_like(gradient, dtype=np.uint8)
            
        # Normalize to [0, 255] range
        normalized = (gradient * 255 / np.max(gradient)).clip(0, 255)
        
        # Apply threshold
        thresholded = np.where(normalized < threshold, 0, normalized)
        
        # Re-normalize if needed
        if np.max(thresholded) > 0:
            thresholded = (thresholded * 255 / np.max(thresholded)).clip(0, 255)
            
        return thresholded.astype(np.uint8)
    
    def clear_cache(self) -> None:
        """Clear all caches to free memory."""
        with self._lock:
            self._cache.clear()
            self.supersample_image.cache_clear()


# Initialize the global processor instance
IMAGE_PROCESSOR = ImageProcessor.get_instance()

# Export core API functions
def supersample_image(image: Image.Image, scale_factor: int) -> Image.Image:
    """Upscale image with system-optimal resampling method."""
    return IMAGE_PROCESSOR.supersample_image(image, scale_factor)

def rgb_to_gray(image_array: np.ndarray) -> np.ndarray:
    """Convert RGB to perceptually-accurate grayscale (ITU-R BT.601)."""
    return IMAGE_PROCESSOR.rgb_to_gray(image_array)

def detect_edges(gray_array: np.ndarray, algorithm: str = "sobel") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Extract edges with specified algorithm and adaptive thresholding.
    
    Args:
        gray_array: Grayscale image data
        algorithm: Edge detection algorithm name
        
    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray]: Magnitude, grad_x, grad_y
    """
    result = IMAGE_PROCESSOR.detect_edges(gray_array, algorithm)
    return result["magnitude"], result["gradient_x"], result["gradient_y"]


# ╔══════════════════════════════════════════════════════════════╗
# ║ 🎞️ Streaming Intelligence                                   ║
# ╚══════════════════════════════════════════════════════════════╝

class StreamEngine:
    """Multi-dimensional adaptive stream processing system.
    
    Processes video streams with contextual awareness, dynamic quality adaptation,
    and network resilience. Handles YouTube URLs, local files, and camera inputs
    with automatic parameter optimization based on system capabilities.
    
    Attributes:
        _stream_cache: Cache mapping of URLs to streaming addresses and timestamps
        _cache_ttl: Cache entry lifetime in seconds
        _cache_max_size: Maximum number of cache entries to maintain
    """
    _stream_cache: Dict[str, Tuple[str, float]] = {}
    _cache_ttl: int = 3600  # 1 hour validity
    _cache_max_size: int = 100
    
    @classmethod
    def extract_stream_url(cls, youtube_url: str, resolution: Optional[int] = None) -> str:
        """Extract adaptive-quality stream URL with resilient retry mechanism.

        Args:
            youtube_url: YouTube video URL or ID
            resolution: Preferred vertical resolution (None=auto-select)
            
        Returns:
            Direct streaming URL with optimal format
            
        Raises:
            DependencyError: If yt-dlp is not available
            StreamExtractionError: When extraction fails after recovery attempts
        """
        if not HAS_YT_DLP:
            raise DependencyError("yt-dlp", "pip install yt-dlp", "YouTube streaming")
        
        # Efficient cache lookup with integrated TTL validation
        cache_key = f"{youtube_url}:{resolution}"
        current_time = time.time()
        cls._prune_cache(current_time)
        
        if cache_key in cls._stream_cache:
            url, timestamp = cls._stream_cache[cache_key]
            if current_time - timestamp < cls._cache_ttl:
                return url
        
        # Dynamic resolution selection based on system capabilities
        actual_resolution = resolution or cls._determine_optimal_resolution()
        format_spec = f'best[height<={actual_resolution}][ext=mp4]'
        
        ydl_opts = {
            'format': format_spec,
            'quiet': True,
            'skip_download': True,
            'no_warnings': True,
            'socket_timeout': 10,
        }
        
        # Progressive fallback with adaptive retry strategy
        for retry in range(3):
            try:
                # Status display with environment awareness
                status_msg = f"🔍 Extracting stream{'.' * (retry + 1)}"
                if HAS_RICH:
                    with CONSOLE.status(status_msg, spinner="dots"):
                        extraction_result = cls._perform_extraction(youtube_url, ydl_opts)
                else:
                    print(f"{status_msg}...", end="", flush=True)
                    extraction_result = cls._perform_extraction(youtube_url, ydl_opts)
                    print(" ✓")
                
                if extraction_result.url:
                    # Cache successful result
                    cls._stream_cache[cache_key] = (extraction_result.url, current_time)
                    return extraction_result.url
                    
                # Format degradation for retry
                ydl_opts['format'] = 'best[height<=360]' if retry == 0 else 'worst'
                
            except Exception as e:
                error_category = cls._categorize_extraction_error(e)
                
                if retry < 2:
                    # Exponential backoff with format adaptation
                    time.sleep(1 * (2**retry))
                    if error_category in ('network', 'timeout'):
                        ydl_opts['socket_timeout'] = 20
                        ydl_opts['format'] = 'best[height<=360]' if retry == 0 else 'worst'
                else:
                    raise StreamExtractionError(
                        f"Failed to extract stream: {error_category}", 
                        original=e, 
                        category=error_category
                    )
                
        raise StreamExtractionError("Stream extraction failed after multiple attempts")

    @classmethod
    def _prune_cache(cls, current_time: float) -> None:
        """Remove expired and excess cache entries with minimal iterations.
        
        Args:
            current_time: Current timestamp for TTL comparison
        """
        # First remove expired entries
        expired_keys = [k for k, (_, ts) in cls._stream_cache.items() 
                      if current_time - ts > cls._cache_ttl]
        
        for key in expired_keys:
            cls._stream_cache.pop(key, None)
            
        # Then enforce size limit if still needed
        if len(cls._stream_cache) > cls._cache_max_size:
            # Keep only the newest entries
            sorted_items = sorted(
                cls._stream_cache.items(), 
                key=lambda item: item[1][1],  # Sort by timestamp
                reverse=True  # Newest first
            )
            # Reset cache with only newest entries
            cls._stream_cache = dict(sorted_items[:cls._cache_max_size])

    @staticmethod
    def _determine_optimal_resolution() -> int:
        """Select optimal resolution based on system capabilities and terminal size.
        
        Returns:
            int: Vertical resolution in pixels
        """
        system_tier = ENV.capabilities.get("performance_tier", 1)
        terminal_height = ENV.terminal.get("height", 24)
        
        # Tiered resolution selection
        if system_tier >= 3:
            return 1080 if terminal_height > 60 else 720
        elif system_tier >= 2 and terminal_height > 40:
            return 720
        elif system_tier >= 1 and terminal_height > 30:
            return 480
        else:
            return 360

    @staticmethod
    def _categorize_extraction_error(error: Exception) -> str:
        """Categorize extraction errors for strategic retry decisions.
        
        Args:
            error: Exception from extraction attempt
            
        Returns:
            str: Error category identifier
        """
        error_str = str(error).lower()
        
        # Pattern-based error classification
        if "429" in error_str:
            return "rate_limited"
        elif "403" in error_str:
            return "access_denied"
        elif any(x in error_str for x in ("blocked", "not available", "country")):
            return "geo_restricted"
        elif any(x in error_str for x in ("timeout", "timed out", "connection")):
            return "network"
        return "general"

    @staticmethod
    def _perform_extraction(url: str, options: Dict[str, Any]) -> VideoInfo:
        """Extract video information with structured result handling.
        
        Args:
            url: Video URL to process
            options: YoutubeDL options dictionary
            
        Returns:
            VideoInfo: Structured video metadata
        """
        with yt_dlp.YoutubeDL(options) as ydl:
            info = ydl.extract_info(url, download=False)
            
            return VideoInfo(
                url=info.get("url"),
                title=info.get("title", "Unknown video"),
                duration=info.get("duration"),
                format=info.get("format_id", "unknown"),
                width=info.get("width"),
                height=info.get("height"),
                fps=info.get("fps")
            )

    @classmethod
    def process_video_stream(
        cls,
        source: Union[str, int, Path],
        scale_factor: int = 2,
        block_width: int = 8,
        block_height: int = 8,
        edge_threshold: int = 50,
        gradient_str: Optional[str] = None,
        color: bool = True,
        fps: int = 15,
        enhanced_edges: bool = True,
        show_stats: bool = True,
        adaptive_quality: bool = True,
        border: bool = True
    ) -> None:
        """Process video stream with adaptive rendering and performance tuning.
        
        Streams content from various sources (files, URLs, cameras) with
        dynamic quality adjustment and performance monitoring.
        
        Args:
            source: File path, YouTube URL, or camera index
            scale_factor: Detail enhancement factor (1-4)
            block_width: Character cell width in pixels
            block_height: Character cell height in pixels
            edge_threshold: Edge detection sensitivity (0-100)
            gradient_str: Custom character gradient (None=auto)
            color: Enable terminal colors
            fps: Target frames per second
            enhanced_edges: Use directional edge detection
            show_stats: Display performance metrics
            adaptive_quality: Dynamically adjust quality for performance
            border: Add decorative frame
            
        Raises:
            DependencyError: If OpenCV is not available
            IOError: If source cannot be opened
        """
        if not HAS_CV2:
            raise DependencyError("opencv-python", "pip install opencv-python", "Video processing")
            
        # Initialize stream with integrated error handling
        try:
            # Normalize source and open capture
            stream_url = cls._resolve_source(source)
            capture = cv2.VideoCapture(stream_url)
            
            if not capture.isOpened():
                raise IOError(f"Failed to open source: {source}")
                
            # Extract video metadata
            video_info = VideoInfo.from_capture(capture, str(source), 
                                               "youtube" if isinstance(source, str) and 
                                               ("youtu" in source or "://y" in source) else "file")
        except Exception as e:
            cls._handle_stream_error(e, 0)
            return
        
        # Setup rendering parameters with environment optimization
        params = RenderParameters(
            scale=scale_factor,
            width=block_width,
            height=block_height,
            threshold=edge_threshold,
            optimal_width=block_width,
            optimal_height=block_height
        )
        
        # Performance monitoring
        metrics = StreamMetrics()
        frame_buffer = FrameBuffer(capacity=2)
        
        # Terminal UI initialization
        renderer = FrameRenderer(
            terminal_width=ENV.terminal["width"],
            terminal_height=ENV.terminal["height"],
            gradient=gradient_str or get_enhanced_gradient_chars(),
            border=border and ENV.capabilities["unicode"],
            unicode_supported=ENV.capabilities["unicode"]
        )
        
        # Display stream information
        title = f"🎬 Streaming: {video_info.title}"
        info = [f"Source: {source}"]
        if video_info.width and video_info.height:
            info.append(f"Resolution: {video_info.width}×{video_info.height}")
        if video_info.fps:
            info.append(f"Original FPS: {video_info.fps:.1f}")
            
        if HAS_RICH:
            CONSOLE.print(Panel("\n".join(info), title=title, border_style="blue"))
        else:
            print(f"\n{title}")
            print("\n".join(info))
            print("=" * 40)
            
        # Performance thresholds for adaptive quality
        render_thresholds = RenderThresholds.from_target_fps(fps)
        frame_time = 1.0 / fps  # Target frame time in seconds
        
        # Clear screen before streaming
        renderer.clear_screen()
        
        # Process frames with error handling
        try:
            running = True
            last_render_time = time.time()
            frames_processed = 0
            
            while running:
                start_time = time.time()
                
                # Frame capture
                ret, frame = capture.read()
                if not ret:
                    break  # End of stream
                    
                # Auto-rotate based on EXIF orientation
                if hasattr(frame, 'shape') and len(frame.shape) >= 2:
                    if frame.shape[0] > frame.shape[1] and frame.shape[0] > 720:
                        # Portrait video - transpose for better terminal fit
                        frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
                
                # Convert to PIL for processing
                pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                
                # Process frame with timing
                render_start = time.time()
                art = image_to_unicode_art(
                    pil_image,
                    scale_factor=params.scale,
                    block_width=params.width,
                    block_height=params.height,
                    edge_threshold=params.threshold,
                    gradient_str=gradient_str,
                    color=color,
                    enhanced_edges=enhanced_edges
                )
                render_time = time.time() - render_start
                
                # Update metrics
                metrics.record_render(render_time)
                metrics.record_frame()
                metrics.update_fps()
                
                # Render complete frame with border and stats
                frame_lines = renderer.render_frame(
                    art=art,
                    source_name=video_info.title or str(source),
                    metrics=metrics,
                    params=params,
                    show_stats=show_stats
                )
                
                # Add to buffer
                frame_buffer.add(frame_lines)
                
                # Throttle display based on target frame rate
                elapsed = time.time() - last_render_time
                if elapsed >= frame_time or frames_processed == 0:
                    # Display latest frame
                    renderer.clear_screen()
                    renderer.display_frame(frame_buffer.get_latest())
                    last_render_time = time.time()
                    frames_processed += 1
                    
                    # Adapt quality if enabled
                    if adaptive_quality and frames_processed % 5 == 0:
                        params.adjust_quality(render_time * 1000, render_thresholds)
                
                # Control frame rate with precise timing
                elapsed_total = time.time() - start_time
                sleep_time = max(0, frame_time - elapsed_total)
                if sleep_time > 0:
                    time.sleep(sleep_time)
                else:
                    metrics.record_dropped()
                    
        except KeyboardInterrupt:
            # Clean exit with statistics
            renderer.clear_screen()
            if HAS_RICH:
                CONSOLE.print("\n[bold green]✓ Stream completed![/bold green]")
                
                stats_table = Table(title="📊 Performance Summary")
                stats_table.add_column("Metric", style="cyan")
                stats_table.add_column("Value", style="green")
                
                perf_stats = metrics.get_stats()
                stats_table.add_row("Frames processed", str(perf_stats["total_frames"]))
                stats_table.add_row("Average FPS", f"{perf_stats['avg_fps']:.2f}")
                stats_table.add_row("Render time", f"{perf_stats['avg_render_time']:.1f}ms")
                stats_table.add_row("Final quality", f"{params.quality_level.name}")
                if perf_stats["dropped_frames"] > 0:
                    stats_table.add_row("Dropped frames", 
                                      f"{perf_stats['dropped_frames']} ({perf_stats['drop_ratio']*100:.1f}%)")
                
                CONSOLE.print(stats_table)
            else:
                print("\n✓ Stream completed!")
                print(f"Frames processed: {metrics.frames_processed}")
                print(f"Average FPS: {metrics.current_fps:.2f}")
                print(f"Render time: {metrics.average_render_time:.1f}ms")
                
        except Exception as e:
            cls._handle_stream_error(e, metrics.frames_processed)
            
        finally:
            if capture is not None:
                capture.release()

    @staticmethod
    def _resolve_source(source: Union[str, int, Path]) -> Union[str, int]:
        """Resolve stream source with YouTube URL extraction when needed.
        
        Args:
            source: Original source identifier
            
        Returns:
            Stream URL or device identifier
        """
        # Handle Path objects
        if isinstance(source, Path):
            return str(source)
            
        # Handle numeric camera indices
        if isinstance(source, int):
            return source
            
        # Handle string URLs/paths
        source_str = str(source)
        
        # YouTube URL detection and resolution
        if any(x in source_str.lower() for x in ("youtube.com", "youtu.be", "yt.be")):
            try:
                return StreamEngine.extract_stream_url(source_str)
            except Exception as e:
                # Fallback to original URL if extraction fails
                if HAS_RICH:
                    CONSOLE.print(f"[yellow]⚠️ YouTube extraction failed: {e}[/yellow]")
                    CONSOLE.print(f"[yellow]Trying direct access...[/yellow]")
                else:
                    print(f"⚠️ YouTube extraction failed: {e}")
                    print("Trying direct access...")
                    
        return source_str
        
    @staticmethod
    def _handle_stream_error(error: Exception, frames_processed: int) -> None:
        """Display user-friendly error with context and recovery guidance.
        
        Args:
            error: Exception that occurred
            frames_processed: Number of frames successfully processed
        """
        if frames_processed > 0:
            # Non-fatal error after some successful processing
            message = "Stream interrupted"
            style = "yellow"
        else:
            # Fatal error during initialization
            message = "Failed to process stream"
            style = "red bold"
        
        if HAS_RICH:
            CONSOLE.print(f"[{style}]🚫 {message}: {type(error).__name__}[/{style}]")
            
            if frames_processed == 0:
                # More detailed error for startup failures
                CONSOLE.print(f"[dim]{str(error)}[/dim]")
                
                if isinstance(error, cv2.error):
                    CONSOLE.print("[yellow]💡 This may be due to an unsupported video format or codec.[/yellow]")
                elif "connection" in str(error).lower():
                    CONSOLE.print("[yellow]💡 Check your network connection or URL.[/yellow]")
        else:
            print(f"\n🚫 {message}: {type(error).__name__}")
            print(f"  {str(error)}")

# Mapping to Legacy Functions For Backward Compatibility
def process_video_stream(
    source: Union[str, int, Path],
    scale_factor: int = 2,
    block_width: int = 8,
    block_height: int = 8,
    edge_threshold: int = 50,
    gradient_str: Optional[str] = None,
    color: bool = True,
    fps: int = 15,
    enhanced_edges: bool = True,
    show_stats: bool = True,
    adaptive_quality: bool = True,
    border: bool = True
) -> None:
    """Process video streams with multidimensional rendering and adaptive quality.

    Provides high-performance stream processing with intelligent buffering, 
    concurrent rendering, and real-time quality adaptation based on system capabilities.
    
    Args:
        source: Video file path, YouTube URL, camera index, or Path object
        scale_factor: Detail enhancement multiplier (1-4)
        block_width: Character cell width in pixels
        block_height: Character cell height in pixels
        edge_threshold: Edge detection sensitivity (0-255)
        gradient_str: Custom density gradient characters (None=auto-select)
        color: Whether to enable ANSI color output
        fps: Target frames per second
        enhanced_edges: Whether to use directional edge characters
        show_stats: Whether to display performance metrics
        adaptive_quality: Whether to auto-adjust quality for performance
        border: Whether to add decorative frame around content
    
    Raises:
        DependencyError: If required dependencies are missing
        IOError: If the source cannot be opened
    """
    StreamEngine.process_video_stream(
        source=source,
        scale_factor=scale_factor,
        block_width=block_width,
        block_height=block_height,
        edge_threshold=edge_threshold,
        gradient_str=gradient_str,
        color=color,
        fps=fps,
        enhanced_edges=enhanced_edges,
        show_stats=show_stats,
        adaptive_quality=adaptive_quality,
        border=border
    )

@dataclass(frozen=True)
class RenderThresholds:
    """Adaptive quality thresholds based on render time performance.
    
    Attributes:
        reduce_ms: Threshold in ms above which quality should be reduced
        improve_ms: Threshold in ms below which quality can be improved
    """
    reduce_ms: float  # Upper threshold to trigger quality reduction
    improve_ms: float  # Lower threshold to trigger quality improvement
    
    @classmethod
    def from_target_fps(cls, target_fps: float, reduce_ratio: float = 0.9, 
                       improve_ratio: float = 0.6) -> 'RenderThresholds':
        """Create optimal thresholds from target FPS with balanced margins.
        
        Args:
            target_fps: Desired frames per second
            reduce_ratio: Percentage of frame budget that triggers quality reduction
            improve_ratio: Percentage of frame budget that allows quality improvement
            
        Returns:
            RenderThresholds: Calculated thresholds for adaptive rendering
        """
        frame_budget_ms = (1000.0 / target_fps)
        return cls(
            reduce_ms=frame_budget_ms * reduce_ratio,
            improve_ms=frame_budget_ms * improve_ratio
        )


class StreamMetrics:
    """High-precision performance metrics with statistical analysis.
    
    Thread-safe performance tracking for realtime visualization with
    rolling statistics and comprehensive analytics.
    
    Attributes:
        frames_processed: Total successfully rendered frames
        current_fps: Most recently calculated frames per second
        average_render_time: Mean render time in milliseconds
    """
    
    def __init__(self, sample_size: int = 30) -> None:
        """Initialize performance tracking with optimized sample size.
        
        Args:
            sample_size: Maximum samples for rolling statistics
        """
        self._lock = threading.RLock()
        self.frames_processed: int = 0
        self.dropped_frames: int = 0
        self.start_time: float = time.time()
        self.current_fps: float = 0.0
        
        # Performance tracking with bounded collections
        self._render_times = collections.deque[float](maxlen=sample_size)
        self._fps_samples = collections.deque[float](maxlen=sample_size)
        self._last_fps_time = time.time()
        self._frames_since_fps_update = 0
        
    def update_fps(self) -> float:
        """Calculate current FPS with time-weighted accuracy.
        
        Returns:
            float: Current frames per second
        """
        with self._lock:
            now = time.time()
            elapsed = now - self._last_fps_time
            
            # Only update when significant time has passed
            if elapsed >= 1.0 and self._frames_since_fps_update > 0:
                self.current_fps = self._frames_since_fps_update / elapsed
                self._fps_samples.append(self.current_fps)
                self._last_fps_time = now
                self._frames_since_fps_update = 0
                
            return self.current_fps
        
    def record_render(self, duration: float) -> None:
        """Record frame rendering duration with millisecond precision.
        
        Args:
            duration: Render time in seconds
        """
        with self._lock:
            self._render_times.append(duration * 1000)
        
    def record_frame(self) -> None:
        """Record successful frame processing with thread safety."""
        with self._lock:
            self.frames_processed += 1
            self._frames_since_fps_update += 1
        
    def record_dropped(self) -> None:
        """Record frame drop for performance analytics."""
        with self._lock:
            self.dropped_frames += 1
    
    @property
    def average_render_time(self) -> float:
        """Calculate mean render time with optimized algorithm.
        
        Returns:
            float: Average render time in milliseconds
        """
        with self._lock:
            return (sum(self._render_times) / len(self._render_times) 
                   if self._render_times else 0.0)
    
    @property
    def effective_fps(self) -> float:
        """Calculate overall FPS across entire runtime.
        
        Returns:
            float: Overall frames per second
        """
        with self._lock:
            duration = time.time() - self.start_time
            return self.frames_processed / max(0.001, duration)
    
    @property
    def drop_ratio(self) -> float:
        """Calculate percentage of frames dropped.
        
        Returns:
            float: Ratio of dropped frames (0.0-1.0)
        """
        with self._lock:
            total = self.frames_processed + self.dropped_frames
            return self.dropped_frames / max(1, total)
    
    def get_stats(self) -> PerformanceStats:
        """Generate comprehensive performance analytics.
        
        Returns:
            PerformanceStats: Complete metrics dictionary
        """
        with self._lock:
            # Use fast mean calculation with bounds checking
            avg_fps = (sum(self._fps_samples) / len(self._fps_samples) 
                      if self._fps_samples else 0.0)
            
            # Calculate stability as inverse of coefficient of variation
            stability = 1.0
            if len(self._render_times) > 1:
                try:
                    mean = self.average_render_time
                    if mean > 0:
                        # Calculate variance without full sample replication
                        variance = sum((t - mean)**2 for t in self._render_times) / len(self._render_times)
                        # Normalize to 0-1 scale with saturation
                        cv = min(1.0, math.sqrt(variance) / mean)
                        stability = 1.0 - cv
                except (ZeroDivisionError, OverflowError):
                    pass
            
            return {
                "avg_render_time": self.average_render_time,
                "avg_fps": avg_fps,
                "effective_fps": self.effective_fps,
                "total_frames": self.frames_processed,
                "dropped_frames": self.dropped_frames,
                "drop_ratio": self.drop_ratio,
                "stability": stability
            }


class RenderParameters:
    """Adaptive rendering parameters with dimensional quality management.
    
    Provides intelligent quality scaling across multiple dimensions
    with optimized parameter sets for different performance targets.
    
    Attributes:
        scale: Detail enhancement factor (1-4)
        width: Block width in pixels
        height: Block height in pixels
        threshold: Edge detection sensitivity (0-100)
        quality_level: Current quality preset (MINIMAL-MAXIMUM)
    """
    
    def __init__(
        self, 
        scale: int,
        width: int,
        height: int,
        threshold: int,
        optimal_width: int,
        optimal_height: int,
        quality_level: QualityLevel = QualityLevel.STANDARD
    ) -> None:
        """Initialize with balanced default parameters and constraints.
        
        Args:
            scale: Detail enhancement factor
            width: Character cell width
            height: Character cell height
            threshold: Edge sensitivity
            optimal_width: Reference width for quality scaling
            optimal_height: Reference height for quality scaling
            quality_level: Initial quality preset
        """
        self._lock = threading.RLock()
        
        # Normalize parameters with bounds protection
        self.scale = max(1, min(4, scale))
        self.width = max(4, width)
        self.height = max(2, height)
        self.threshold = max(30, min(80, threshold))
        
        # Quality reference points
        self.optimal_width = max(4, optimal_width)
        self.optimal_height = max(2, optimal_height)
        self.quality_level = quality_level
        self.frames_since_adjustment: int = 0
        
        # Parameter mappings for efficient quality transitions
        self._quality_map = {
            QualityLevel.MINIMAL: (1, 4, 2),  # Fastest rendering
            QualityLevel.LOW: (max(1, self.scale - 1), 0, 0),
            QualityLevel.STANDARD: (self.scale, 0, 0),  # Reference quality
            QualityLevel.HIGH: (min(4, self.scale + 1), -2, -1),
            QualityLevel.MAXIMUM: (min(4, self.scale + 1), -4, -2)  # Highest detail
        }
        
    def adjust_quality(self, render_time: float, 
                      thresholds: Union[RenderThresholds, Dict[str, float]]) -> bool:
        """Adjust quality based on performance with hysteresis.
        
        Intelligently balances quality and performance by adjusting
        multiple parameters simultaneously based on render times.
        
        Args:
            render_time: Current render time in milliseconds
            thresholds: Performance boundaries for adjustments
            
        Returns:
            bool: Whether quality was changed
        """
        with self._lock:
            # Cache previous level for change detection
            previous_level = self.quality_level
            
            # Extract thresholds with protocol compatibility
            reduce_ms = (thresholds.reduce_ms if isinstance(thresholds, RenderThresholds) 
                        else thresholds['reduce'])
            improve_ms = (thresholds.improve_ms if isinstance(thresholds, RenderThresholds) 
                        else thresholds['improve'])
            
            # Apply quality transitions with bounds protection
            if render_time > reduce_ms and self.quality_level > QualityLevel.MINIMAL:
                self.quality_level = QualityLevel(self.quality_level - 1)
            elif render_time < improve_ms and self.quality_level < QualityLevel.MAXIMUM:
                self.quality_level = QualityLevel(self.quality_level + 1)
            
            # Apply parameter changes if quality changed
            changed = previous_level != self.quality_level
            if changed:
                self._apply_quality_parameters()
            
            # Track frames since last adjustment
            self.frames_since_adjustment = 0 if changed else self.frames_since_adjustment + 1
            return changed
        
    def _apply_quality_parameters(self) -> None:
        """Update rendering parameters for current quality level."""
        with self._lock:
            # Get parameters for selected quality level
            new_scale, width_delta, height_delta = self._quality_map[self.quality_level]
            
            # Apply parameters with proportional scaling
            self.scale = new_scale
            self.width = max(4, self.optimal_width + width_delta)
            self.height = max(2, self.optimal_height + height_delta)
            
            # Adjust threshold for detail preservation
            if self.quality_level >= QualityLevel.HIGH:
                self.threshold = max(30, self.threshold - 10)
            elif self.quality_level <= QualityLevel.LOW:
                self.threshold = min(80, self.threshold + 10)


class FrameRenderer:
    """🖼️ Unicode art frame renderer with adaptive terminal-aware formatting.
    
    Efficiently handles frame rendering with border, title, and statistics display,
    automatically adapting to terminal capabilities and constraints.
    """
    
    def __init__(
        self, 
        terminal_width: int, 
        terminal_height: int,
        gradient: str,
        border: bool = True,
        unicode_supported: bool = True
    ) -> None:
        """Initialize renderer with environment-specific configuration.
        
        Args:
            terminal_width: Available terminal width in characters
            terminal_height: Available terminal height in lines
            gradient: Character gradient for density rendering
            border: Whether to display decorative border
            unicode_supported: Whether terminal supports unicode characters
        """
        self.terminal_width = max(40, terminal_width)  # Enforce minimum width
        self.terminal_height = max(10, terminal_height)  # Enforce minimum height
        self.gradient = gradient
        self.border = border
        self.unicode_supported = unicode_supported
        
        # Select appropriate border character set based on capabilities
        self.border_chars = self._select_border_chars(border, unicode_supported)
        
        # Rendering cache for frequently used components
        self._title_cache: Dict[str, str] = {}
        self._stats_cache: Dict[str, str] = {}
        self._lock = threading.RLock()  # Thread safety
    
    def _select_border_chars(self, border: bool, unicode_supported: bool) -> Dict[str, str]:
        """Select optimal border character set based on terminal capabilities.
        
        Args:
            border: Whether borders are enabled
            unicode_supported: Whether unicode is supported
            
        Returns:
            Dictionary of border characters for consistent rendering
        """
        if border and unicode_supported:
            return {
                "top_left": "╔", "top_right": "╗", 
                "bottom_left": "╚", "bottom_right": "╝",
                "horizontal": "═", "vertical": "║"
            }
        return {
            "top_left": "+", "top_right": "+", 
            "bottom_left": "+", "bottom_right": "+",
            "horizontal": "-", "vertical": "|"
        }
            
    def render_frame(
        self, 
        art: List[str], 
        source_name: str,
        metrics: StreamMetrics,
        params: RenderParameters,
        show_stats: bool = True
    ) -> List[str]:
        """Render complete frame with content, borders and statistics.
        
        Args:
            art: Unicode art content lines
            source_name: Source identifier for title
            metrics: Performance metrics for statistics
            params: Current rendering parameters
            show_stats: Whether to display performance statistics
            
        Returns:
            List of fully formatted frame lines ready for display
        """
        if not art:
            return []
            
        with self._lock:
            frame = []
            content_width = len(art[0]) if art else 0
            
            # Add title border with efficient string concatenation
            if self.border:
                cache_key = f"{source_name}:{content_width}"
                if cache_key not in self._title_cache:
                    self._title_cache[cache_key] = self._format_title(source_name, content_width)
                frame.append(self._title_cache[cache_key])
            
            # Add content without unnecessary copying
            frame.extend(art)
            
            # Add statistics if enabled and data available
            if show_stats and metrics.frames_processed > 0:
                self._append_statistics(frame, metrics, params, content_width)
                    
            return frame
        
    def _format_title(self, source_name: str, content_width: int) -> str:
        """Format title bar with centered source name.
        
        Args:
            source_name: Source identifier
            content_width: Width of content area
            
        Returns:
            Formatted title bar string
        """
        basename = os.path.basename(str(source_name))
        title = f" {basename} "
        padding = max(0, content_width - len(title))
        left_pad = padding // 2
        right_pad = padding - left_pad
        
        return (f"{self.border_chars['top_left']}"
                f"{self.border_chars['horizontal'] * left_pad}"
                f"{title}"
                f"{self.border_chars['horizontal'] * right_pad}"
                f"{self.border_chars['top_right']}")
                
    def _append_statistics(
        self, 
        frame: List[str], 
        metrics: StreamMetrics,
        params: RenderParameters,
        content_width: int
    ) -> None:
        """Append statistics to frame with border-aware formatting.
        
        Args:
            frame: Frame lines to append to (modified in-place)
            metrics: Performance metrics to display
            params: Current rendering parameters
            content_width: Width of content area
        """
        if self.border:
            # Bottom border
            frame.append(f"{self.border_chars['bottom_left']}"
                        f"{self.border_chars['horizontal'] * content_width}"
                        f"{self.border_chars['bottom_right']}")
            
            # Statistics line
            cache_key = f"{metrics.current_fps:.1f}:{metrics.average_render_time:.1f}:{params.quality_level}:{content_width}"
            if cache_key not in self._stats_cache:
                self._stats_cache[cache_key] = self._format_stats_line(metrics, params, content_width)
                
                # Limit cache size
                if len(self._stats_cache) > 100:
                    self._stats_cache.pop(next(iter(self._stats_cache)))
                    
            stats_line = self._stats_cache[cache_key]
            frame.append(f"{self.border_chars['vertical']} {stats_line} {self.border_chars['vertical']}")
            
            # Final border
            frame.append(f"{self.border_chars['bottom_left']}"
                        f"{self.border_chars['horizontal'] * (content_width)}"
                        f"{self.border_chars['bottom_right']}")
        else:
            # Simple stats without border
            frame.append(f"FPS: {metrics.current_fps:.1f} | "
                        f"Render: {metrics.average_render_time:.1f}ms | "
                        f"Q: {params.quality_level}/4")
        
    def _format_stats_line(
        self, 
        metrics: StreamMetrics,
        params: RenderParameters,
        width: int
    ) -> str:
        """Format statistics with visual quality indicators.
        
        Args:
            metrics: Performance metrics
            params: Rendering parameters
            width: Available width for text
            
        Returns:
            Formatted statistics string with quality indicators
        """
        # Pre-calculate components for performance
        fps_info = f"FPS: {metrics.current_fps:.1f}"
        render_info = f"Render: {metrics.average_render_time:.1f}ms"
        quality_info = f"Quality: {params.quality_level}/4"
        
        # Visual indicator uses filled and empty circles
        quality_indicator = '●' * params.quality_level + '○' * (4 - params.quality_level)
        
        # Build string with components
        stats_line = f"{fps_info} | {render_info} | {quality_info} | {quality_indicator}"
        
        # Truncate or pad as needed
        if len(stats_line) > width:
            return stats_line[:width-3] + "..."
        
        return stats_line + " " * (width - len(stats_line))
        
    def clear_screen(self) -> None:
        """Clear terminal screen with ANSI escape sequence."""
        sys.stdout.write("\033[H\033[J")
        sys.stdout.flush()
        
    def display_frame(self, frame_lines: List[str]) -> None:
        """Display frame with optimal I/O efficiency.
        
        Args:
            frame_lines: Rendered frame lines to display
        """
        # Single write operation for better performance
        if frame_lines:
            sys.stdout.write("\n".join(frame_lines) + "\n")
            sys.stdout.flush()


class FrameBuffer:
    """🔄 Efficient frame buffer with automatic capacity management.
    
    Provides a FIFO buffer for frames with thread-safe operations
    and automatic frame dropping under pressure.
    """
    
    def __init__(self, capacity: int = 2) -> None:
        """Initialize buffer with specified capacity.
        
        Args:
            capacity: Maximum number of frames to store
        """
        self.frames: collections.deque[List[str]] = collections.deque(maxlen=capacity)
        self.lock = threading.RLock()
        
    def add(self, frame: List[str]) -> None:
        """Add frame to buffer with thread safety.
        
        Args:
            frame: Frame lines to add
        """
        with self.lock:
            self.frames.append(frame.copy())
            
    def get_latest(self) -> List[str]:
        """Get most recent frame with fallback.
        
        Returns:
            Latest frame or empty list if buffer is empty
        """
        with self.lock:
            return self.frames[-1] if self.frames else []
            
    def clear(self) -> None:
        """Clear all frames from buffer."""
        with self.lock:
            self.frames.clear()
            
    @property
    def size(self) -> int:
        """Get current number of frames in buffer."""
        return len(self.frames)


class StreamExtractionError(Exception):
    """❌ Error during stream URL extraction with rich diagnostic context.
    
    Provides detailed error categorization and original exception access
    for informed recovery strategies.
    """
    
    def __init__(self, message: str, original: Optional[Exception] = None, category: str = "general") -> None:
        """Initialize with contextual error information.
        
        Args:
            message: Human-readable error description
            original: Original exception that triggered this error
            category: Error category for programmatic handling
        """
        super().__init__(message)
        self.original = original
        self.category = category
        self.timestamp = datetime.now()
    
    def get_diagnostic_info(self) -> Dict[str, Any]:
        """Get comprehensive diagnostic information for logging or display.
        
        Returns:
            Dictionary with structured error details
        """
        return {
            "message": str(self),
            "category": self.category,
            "timestamp": self.timestamp.isoformat(),
            "original_type": type(self.original).__name__ if self.original else None,
            "original_message": str(self.original) if self.original else None,
        }


class DependencyError(Exception):
    """❌ Missing dependency error with actionable installation guidance.
    
    Provides clear instructions for resolving dependency issues
    with package and installation command details.
    """
    
    def __init__(self, package: str, install_cmd: str, required_for: str = "") -> None:
        """Initialize with package details and installation guidance.
        
        Args:
            package: Missing package name
            install_cmd: Command to install the package
            required_for: Feature requiring this dependency
        """
        message = f"Missing dependency: {package}" + (f" (required for {required_for})" if required_for else "")
        super().__init__(message)
        self.package = package
        self.install_cmd = install_cmd
        self.required_for = required_for
    
    def get_installation_instructions(self) -> str:
        """Get user-friendly installation instructions.
        
        Returns:
            Formatted installation instructions
        """
        return (
            f"To install {self.package}:\n"
            f"  {self.install_cmd}\n"
            f"Then restart the application."
        )

# ╔══════════════════════════════════════════════════════════════╗
# ║ 🌀 Dimensional Transmutation Engine                         ║
# ╚══════════════════════════════════════════════════════════════╝

def image_to_unicode_art(
    pil_image: Image.Image,
    scale_factor: int = 2,
    block_width: int = 8,
    block_height: int = 8,
    edge_threshold: int = 50,
    gradient_str: Optional[str] = None,
    color: bool = True,
    enhanced_edges: bool = True,
    algorithm: str = "sobel",
    dithering: bool = False
) -> List[str]:
    """🎭 Transform image into dimensional Unicode art.
    
    Args:
        pil_image: Source PIL image
        scale_factor: Supersampling multiplier
        block_width: Character cell width in pixels
        block_height: Character cell height in pixels
        edge_threshold: Edge detection sensitivity (0-255)
        gradient_str: Custom character density gradient
        color: Apply ANSI color to output
        enhanced_edges: Use advanced edge representation
        algorithm: Edge detection algorithm
        dithering: Apply error diffusion dithering
        
    Returns:
        List of strings with Unicode art rows
    """
    # Default gradient with adaptive selection
    if gradient_str is None:
        # Choose appropriate gradient based on terminal capabilities
        if SYSTEM_CONTEXT.capabilities.get("can_display_unicode", True):
            gradient_str = get_enhanced_gradient_chars()
        else:
            # ASCII-only fallback
            gradient_str = UNICODE_ENGINE.character_maps["full_gradients"]["ascii_art"]

    # Detail preservation via supersampling
    image_sup = supersample_image(pil_image, scale_factor)
    image_array = np.array(image_sup)
    
    # Apply contrast enhancement if enabled
    if SYSTEM_CONTEXT.constraints.get("performance_tier", 1) >= 2:
        # High-end systems get enhanced preprocessing
        image_array = IMAGE_PROCESSOR.enhance_image(
            image_array, 
            contrast=1.2,
            brightness=5.0,
            denoise=True
        )
    
    # Edge detection pipeline with algorithm flexibility
    gray_array = rgb_to_gray(image_array)
    edge_result = IMAGE_PROCESSOR.detect_edges(
        gray_array, 
        algorithm=algorithm.lower(), 
        threshold=edge_threshold
    )
    edge_magnitude, grad_x, grad_y = edge_result
    
    # Output grid dimensions with boundary check
    height, width = gray_array.shape
    cols = max(1, width // block_width)
    rows = max(1, height // block_height)
    
    # Prepare dithering state if enabled
    dither_errors = np.zeros((height, width)) if dithering else None
    
    # Generate art line by line
    output_lines = []
    for i in range(rows):
        line = []
        for j in range(cols):
            # Extract current block with bounds checking
            y_start = i * block_height
            y_end = min((i + 1) * block_height, height)
            x_start = j * block_width
            x_end = min((j + 1) * block_width, width)
            
            # Color extraction for colorization
            color_block = image_array[y_start:y_end, x_start:x_end, :]
            avg_color = color_block.mean(axis=(0, 1))
            r, g, b = avg_color.astype(int)
            
            # Perceptual luminance calculation (ITU-R BT.709)
            brightness = 0.2126 * r + 0.7152 * g + 0.0722 * b
            
            # Apply dithering if enabled
            if dithering and dither_errors is not None:
                # Extract errors for this block
                error_block = dither_errors[y_start:y_end, x_start:x_end]
                # Apply accumulated error to brightness (with limits)
                error_avg = error_block.mean()
                brightness = np.clip(brightness + error_avg, 0, 255)
            
            # Edge analysis with threshold adaptation
            edge_block = edge_magnitude[y_start:y_end, x_start:x_end]
            avg_edge = edge_block.mean()
            
            # Character selection with edge intelligence
            if avg_edge > edge_threshold:
                # Edge detected - use directional character
                block_grad_x = grad_x[y_start:y_end, x_start:x_end]
                block_grad_y = grad_y[y_start:y_end, x_start:x_end]
                avg_grad_x = block_grad_x.mean()
                avg_grad_y = block_grad_y.mean()
                
                edge_strength = min(avg_edge / 255, 1.0)
                char = get_edge_char(avg_grad_x, avg_grad_y, 
                                    edge_strength if enhanced_edges else 0.5)
            else:
                # No edge - use gradient character
                normalized_brightness = brightness / 255
                
                # Update dithering error if enabled
                if dithering and dither_errors is not None:
                    # Calculate quantization error
                    index = min(int((1.0 - normalized_brightness) * (len(gradient_str) - 1)), len(gradient_str) - 1)
                    quantized = (1.0 - index / (len(gradient_str) - 1)) * 255
                    quant_error = brightness - quantized
                    
                    # Distribute error (Floyd-Steinberg)
                    if j < cols - 1:  # Right
                        dither_errors[y_start:y_end, x_end:min(x_end+block_width, width)] += quant_error * 0.4375
                    if i < rows - 1:  # Down
                        dither_errors[y_end:min(y_end+block_height, height), x_start:x_end] += quant_error * 0.3125
                    if i < rows - 1 and j > 0:  # Down-left
                        dither_errors[y_end:min(y_end+block_height, height), 
                                    max(0, x_start-block_width):x_start] += quant_error * 0.1875
                    if i < rows - 1 and j < cols - 1:  # Down-right
                        dither_errors[y_end:min(y_end+block_height, height), 
                                    x_end:min(x_end+block_width, width)] += quant_error * 0.0625
                
                # Select character from gradient based on brightness
                index = min(int((1.0 - normalized_brightness) * (len(gradient_str) - 1)), len(gradient_str) - 1)
                char = gradient_str[index]
            
            # Apply color with system adaptation
            if color and SYSTEM_CONTEXT.capabilities.get("can_display_color", True):
                # Efficient color construction
                line.append(f"{get_ansi_color(r, g, b)}{char}\033[0m")
            else:
                line.append(char)
        
        output_lines.append("".join(line))
    
    return output_lines


def generate_unicode_art(
    image_path: Union[str, Path],
    scale_factor: int = 2,
    block_width: int = 8,
    block_height: int = 8,
    edge_threshold: int = 50,
    gradient_str: Optional[str] = None,
    color: bool = True,
    enhanced_edges: bool = True,
    algorithm: str = "sobel",
    dithering: bool = False,
    auto_scale: bool = True
) -> List[str]:
    """📸 Process image into dimensional Unicode art.
    
    Args:
        image_path: Path to source image
        scale_factor: Detail enhancement factor
        block_width: Character cell width
        block_height: Character cell height
        edge_threshold: Edge sensitivity
        gradient_str: Custom character gradient
        color: Enable ANSI colors
        enhanced_edges: Use advanced edge characters
        algorithm: Edge detection algorithm
        dithering: Apply error diffusion dithering
        auto_scale: Automatically adapt output to terminal
        
    Returns:
        List of strings with Unicode art rows
        
    Raises:
        SystemExit: If image loading fails
    """
    start_time = time.time()
    
    # Adaptive system parameter optimization
    if auto_scale:
        # Get system-optimal parameters
        params = SYSTEM_CONTEXT.get_optimized_parameters()
        scale_factor = min(scale_factor, params.get("scale_factor", scale_factor))
        
        # Terminal size adaptation
        terminal_width = SYSTEM_CONTEXT.attributes.get("terminal_width", 80)
        # Each character is roughly half as tall as wide in most terminals
        aspect_ratio_factor = 0.5
        
        # Apply automatic size limitation
        if terminal_width < 100:
            # Small terminal adjustment
            block_width = max(block_width, 10)
            block_height = max(block_height, int(block_width * aspect_ratio_factor))
    
    # Load and process image
    try:
        if HAS_RICH:
            with CONSOLE.status("📥 Loading image...", spinner="dots"):
                # Error handling and format normalization
                try:
                    image = Image.open(image_path).convert("RGB")
                    CONSOLE.log(f"✓ Image loaded: {image.width}×{image.height} px")
                except (IOError, FileNotFoundError) as e:
                    CONSOLE.print(f"[red]🚫 Error opening image:[/red] {e}")
                    sys.exit(1)
                except Exception as e:
                    CONSOLE.print(f"[red]🚫 Unexpected error:[/red] {e}")
                    sys.exit(1)
        else:
            print(f"📥 Loading image: {image_path}...")
            try:
                image = Image.open(image_path).convert("RGB")
                print(f"✓ Image loaded: {image.width}×{image.height} px")
            except Exception as e:
                print(f"🚫 Image error: {e}")
                print("💡 Check path, permissions, and format support")
                sys.exit(1)
    except Exception as e:
        print(f"🚫 Image error: {e}")
        print("💡 Check path, permissions, and format support")
        sys.exit(1)
    
    # Image size safety check
    max_dim = 8192 if SYSTEM_CONTEXT.constraints.get("performance_tier", 1) >= 2 else 4096
    if image.width > max_dim or image.height > max_dim:
        if HAS_RICH:
            CONSOLE.print(f"[yellow]⚠️ Image is very large, resizing for safety[/yellow]")
        else:
            print("⚠️ Image is very large, resizing for safety")
            
        # Maintain aspect ratio
        aspect = image.width / image.height
        if image.width > image.height:
            new_width = max_dim
            new_height = int(new_width / aspect)
        else:
            new_height = max_dim
            new_width = int(new_height * aspect)
            
        image = image.resize((new_width, new_height), Image.LANCZOS)
    
    # Process image
    if HAS_RICH:
        with CONSOLE.status("🧠 Processing...", spinner="dots"):
            result = image_to_unicode_art(
                image,
                scale_factor=scale_factor,
                block_width=block_width,
                block_height=block_height,
                edge_threshold=edge_threshold,
                gradient_str=gradient_str,
                color=color,
                enhanced_edges=enhanced_edges,
                algorithm=algorithm,
                dithering=dithering
            )
            elapsed = time.time() - start_time
            CONSOLE.log(f"✨ Generated in {elapsed:.2f}s")
    else:
        print("🧠 Processing image...")
        result = image_to_unicode_art(
            image,
            scale_factor=scale_factor,
            block_width=block_width,
            block_height=block_height,
            edge_threshold=edge_threshold,
            gradient_str=gradient_str,
            color=color,
            enhanced_edges=enhanced_edges,
            algorithm=algorithm,
            dithering=dithering
        )
        elapsed = time.time() - start_time
        print(f"✨ Generated in {elapsed:.2f}s")
        
    return result


class ArtTransformer:
    """🎨 Multi-dimensional art transformation pipeline.
    
    Provides fluent interface for progressive image transformations
    with context-aware parameter tuning and optimization paths.
    """
    
    def __init__(self, source: Union[str, Path, Image.Image]) -> None:
        """Initialize transformer with source image.
        
        Args:
            source: Image path or PIL Image object
        """
        self.image = None
        self.options = {
            "scale_factor": 2,
            "block_width": 8,
            "block_height": 8, 
            "edge_threshold": 50,
            "gradient_str": None,
            "color": SYSTEM_CONTEXT.capabilities.get("can_display_color", True),
            "enhanced_edges": True,
            "algorithm": "sobel",
            "dithering": False,
            "output_format": "ansi"
        }
        
        # Load image source
        if isinstance(source, Image.Image):
            self.image = source
        else:
            try:
                self.image = Image.open(source).convert("RGB")
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[red]🚫 Error loading image:[/red] {e}")
                else:
                    print(f"🚫 Error loading image: {e}")
                raise
    
    def with_scale(self, factor: int) -> 'ArtTransformer':
        """Set supersampling scale factor.
        
        Args:
            factor: Detail enhancement factor (1-4)
            
        Returns:
            Self for chaining
        """
        self.options["scale_factor"] = max(1, min(4, factor))
        return self
    
    def with_block_size(self, width: int, height: Optional[int] = None) -> 'ArtTransformer':
        """Set character block dimensions.
        
        Args:
            width: Block width in pixels
            height: Block height (defaults to width)
            
        Returns:
            Self for chaining
        """
        self.options["block_width"] = max(2, width)
        self.options["block_height"] = max(2, height or width)
        return self
    
    def with_edge_detection(self, 
                          threshold: int = 50, 
                          algorithm: str = "sobel",
                          enhanced: bool = True) -> 'ArtTransformer':
        """Configure edge detection parameters.
        
        Args:
            threshold: Edge sensitivity (0-255)
            algorithm: Detection algorithm
            enhanced: Use detailed edge characters
            
        Returns:
            Self for chaining
        """
        self.options["edge_threshold"] = threshold
        self.options["algorithm"] = algorithm
        self.options["enhanced_edges"] = enhanced
        return self
    
    def with_gradient(self, gradient: str) -> 'ArtTransformer':
        """Set custom gradient character sequence.
        
        Args:
            gradient: Character sequence from dense to sparse
            
        Returns:
            Self for chaining
        """
        self.options["gradient_str"] = gradient
        return self
    
    def with_preset(self, preset: Literal["default", "detailed", "fast", "minimal"]) -> 'ArtTransformer':
        """Apply predefined parameter preset.
        
        Args:
            preset: Named parameter configuration
            
        Returns:
            Self for chaining
        """
        if preset == "detailed":
            self.options.update({
                "scale_factor": 3,
                "block_width": 4,
                "block_height": 4,
                "edge_threshold": 40,
                "algorithm": "scharr",
                "enhanced_edges": True,
                "dithering": True
            })
        elif preset == "fast":
            self.options.update({
                "scale_factor": 1,
                "block_width": 12,
                "block_height": 12,
                "edge_threshold": 60,
                "algorithm": "sobel",
                "enhanced_edges": False,
                "dithering": False
            })
        elif preset == "minimal":
            self.options.update({
                "scale_factor": 1,
                "block_width": 8,
                "block_height": 8,
                "edge_threshold": 80,
                "algorithm": "sobel",
                "enhanced_edges": False,
                "color": False,
                "dithering": False,
                "gradient_str": " .:;+=xX$&#@"
            })
            
        return self
    
    def with_color(self, enabled: bool = True) -> 'ArtTransformer':
        """Enable or disable ANSI color output.
        
        Args:
            enabled: Color state
            
        Returns:
            Self for chaining
        """
        self.options["color"] = enabled and SYSTEM_CONTEXT.capabilities.get("can_display_color", True)
        return self
    
    def with_dithering(self, enabled: bool = True) -> 'ArtTransformer':
        """Enable or disable Floyd-Steinberg dithering.
        
        Args:
            enabled: Dithering state
            
        Returns:
            Self for chaining
        """
        self.options["dithering"] = enabled
        return self
    
    def optimize_for_terminal(self) -> 'ArtTransformer':
        """Adapt parameters to current terminal dimensions.
        
        Returns:
            Self for chaining
        """
        # Get terminal dimensions
        terminal_width = SYSTEM_CONTEXT.attributes.get("terminal_width", 80)
        terminal_height = SYSTEM_CONTEXT.attributes.get("terminal_height", 24)
        
        # Calculate optimal block size to fit image in terminal
        # Account for aspect ratio (terminal characters are taller than wide)
        char_aspect = 0.5  # approximate character height/width ratio
        
        # Calculate dimensions that would fit in terminal
        target_cols = max(20, terminal_width - 4)  # Allow border space
        target_rows = max(10, terminal_height - 4)
        
        if self.image:
            image_width, image_height = self.image.size
            image_aspect = image_width / image_height
            
            # Calculate block size that fits image in terminal
            # with appropriate aspect ratio correction
            width_from_cols = max(2, image_width // target_cols)
            height_from_rows = max(2, image_height // target_rows)
            
            # Adjust to preserve aspect ratio
            adjusted_height = int(width_from_cols / (image_aspect * char_aspect))
            adjusted_width = int(height_from_rows * image_aspect / char_aspect)
            
            self.options["block_width"] = min(width_from_cols, adjusted_width)
            self.options["block_height"] = min(height_from_rows, adjusted_height)
            
            # Adjust scale factor based on terminal size
            if terminal_width < 80 or terminal_height < 24:
                self.options["scale_factor"] = 1
            
        return self
    
    def render(self) -> List[str]:
        """Generate Unicode art with current settings.
        
        Returns:
            List of strings with art lines
        """
        if not self.image:
            return ["Error: No image loaded"]
            
        return image_to_unicode_art(
            self.image,
            **self.options
        )
    
    def save_to_file(self, path: Union[str, Path]) -> None:
        """Save rendered art to text file.
        
        Args:
            path: Output file path
        """
        result = self.render()
        
        try:
            with open(path, 'w', encoding='utf-8') as f:
                for line in result:
                    # Strip ANSI color codes for file output
                    if self.options["output_format"] != "ansi":
                        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
                        line = ansi_escape.sub('', line)
                    f.write(line + '\n')
                    
            if HAS_RICH:
                CONSOLE.print(f"[green]✓ Art saved to:[/green] {path}")
            else:
                print(f"✓ Art saved to: {path}")
        except Exception as e:
            if HAS_RICH:
                CONSOLE.print(f"[red]🚫 Error saving file:[/red] {e}")
            else:
                print(f"🚫 Error saving file: {e}")
    
    def display(self) -> None:
        """Render and display art in terminal."""
        result = self.render()
        
        if HAS_RICH:
            # Add a title panel
            title = "✨ Unicode Art Rendering ✨"
            CONSOLE.print(Panel(title, style="bold blue"))
            
            # Print each line
            for line in result:
                CONSOLE.print(line)
        else:
            # Simple display
            print("\n".join(result))


# Legacy compatibility wrapper
def transform_image(
    image_path: Union[str, Path], 
    preset: Optional[str] = None
) -> List[str]:
    """🎨 Transform image file with fluent parameter configuration.
    
    Args:
        image_path: Path to image file
        preset: Optional preset name ("default", "detailed", "fast", "minimal")
        
    Returns:
        List of strings containing Unicode art
    """
    transformer = ArtTransformer(image_path)
    
    if preset:
        transformer.with_preset(preset)
        
    return transformer.render()

# ╔═════════════════════════════════════════════════════════════╗
# ║ 🖼️ Virtual Display Capture And Management                   ║
# ╚═════════════════════════════════════════════════════════════╝

class VirtualDisplayEngine:
    """🔮 Dimensional context bridge for GUI-to-terminal transmutation.
    
    Creates and manages virtual displays (Xvfb), captures their visual state,
    and streams real-time GUI content to terminals through dimensional
    compression with adaptive performance tuning and intelligent fallbacks.
    """
    # Singleton instance with lazy initialization
    _instance = None
    
    @classmethod
    def get_instance(cls) -> 'VirtualDisplayEngine':
        """📡 Get or create the singleton display engine."""
        if cls._instance is None:
            cls._instance = VirtualDisplayEngine()
        return cls._instance

    def __init__(self) -> None:
        """Initialize with dynamic capability detection."""
        self._virtual_display = None
        self._display_process = None
        self._current_display_id = 99  # Default virtual display number
        self._display_size = (1280, 720)  # Default resolution
        
        # Capability detection with progressive enhancement
        self._capabilities = self._detect_capabilities()
        
        # Screenshot backend selection based on system capabilities
        self._capture_backend = self._select_capture_backend()
        
        # Rendering optimizations cache
        self._capture_cache = {}
        self._capture_cache_enabled = SYSTEM_CONTEXT.constraints.get("performance_tier", 1) >= 2
        
        # Status tracking for resource management
        self._active_streams = set()
        self._last_capture_time = 0

    def _detect_capabilities(self) -> Dict[str, bool]:
        """🕵️ Detect available virtual display and capture mechanisms."""
        capabilities = {
            "has_xvfb": False,
            "has_pyvirtualdisplay": False,
            "has_mss": False,
            "has_pil_grab": False,
            "has_x11_utils": False,
            "can_create_display": False,
            "platform_headless_support": False
        }
        
        # Check for PyVirtualDisplay
        try:
            import pyvirtualdisplay
            capabilities["has_pyvirtualdisplay"] = True
        except ImportError:
            pass
            
        # Check for MSS (fast cross-platform screenshots)
        try:
            import mss
            capabilities["has_mss"] = True
        except ImportError:
            pass
            
        # Check for PIL ImageGrab
        try:
            from PIL import ImageGrab
            capabilities["has_pil_grab"] = True
        except (ImportError, AttributeError):
            pass
            
        # Check for X11 utilities
        if SYSTEM_CONTEXT.attributes.get("platform") == "Linux":
            for utility in ["xvfb-run", "xwd", "import"]:
                if shutil.which(utility):
                    capabilities["has_x11_utils"] = True
                    if utility == "xvfb-run":
                        capabilities["has_xvfb"] = True
            
        # Determine if we can create virtual displays
        capabilities["can_create_display"] = (
            capabilities["has_xvfb"] or 
            capabilities["has_pyvirtualdisplay"]
        )
            
        # Platform-specific capability checks
        if SYSTEM_CONTEXT.attributes.get("platform") == "Linux":
            capabilities["platform_headless_support"] = True
        elif SYSTEM_CONTEXT.attributes.get("platform") == "Darwin":
            capabilities["platform_headless_support"] = capabilities["has_pil_grab"]
        else:  # Windows
            capabilities["platform_headless_support"] = capabilities["has_mss"]
            
        return capabilities

    def _select_capture_backend(self) -> str:
        """⚙️ Select optimal screenshot backend based on capabilities."""
        if self._capabilities["has_mss"]:
            return "mss"  # Fast and cross-platform
        elif self._capabilities["has_x11_utils"] and SYSTEM_CONTEXT.attributes.get("platform") == "Linux":
            return "xwd"  # X11 specific but reliable
        elif self._capabilities["has_pil_grab"]:
            return "pillow"  # Decent fallback
        else:
            return "none"  # No suitable backend

    def create_virtual_display(self, 
                             width: int = 1280, 
                             height: int = 720, 
                             color_depth: int = 24,
                             visible: bool = False) -> bool:
        """🖥️ Create new virtual display or take over existing one.
        
        Args:
            width: Display width in pixels
            height: Display height in pixels
            color_depth: Display color depth
            visible: Whether to make display visible (for debugging)
            
        Returns:
            Success status
        """
        # Abort if capabilities not available
        if not self._capabilities["can_create_display"]:
            if HAS_RICH:
                CONSOLE.print("[red]⚠️ Cannot create virtual display - missing dependencies[/red]")
                CONSOLE.print("[yellow]Install pyvirtualdisplay or ensure Xvfb is available[/yellow]")
            else:
                print("⚠️ Cannot create virtual display - missing dependencies")
                print("💡 Install pyvirtualdisplay or ensure Xvfb is available")
            return False
            
        # Clean up any existing display
        self.destroy_virtual_display()
        
        # Store new display dimensions
        self._display_size = (width, height)
        
        # Try pyvirtualdisplay first (more robust)
        if self._capabilities["has_pyvirtualdisplay"]:
            try:
                import pyvirtualdisplay
                self._virtual_display = pyvirtualdisplay.Display(
                    visible=1 if visible else 0,
                    size=(width, height),
                    color_depth=color_depth
                )
                self._virtual_display.start()
                os.environ["DISPLAY"] = self._virtual_display.display
                self._current_display_id = int(self._virtual_display.display.replace(':', ''))
                
                if HAS_RICH:
                    CONSOLE.print(f"[green]✓[/green] Virtual display created at [bold]{self._virtual_display.display}[/bold]")
                else:
                    print(f"✓ Virtual display created at {self._virtual_display.display}")
                return True
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[red]Error creating virtual display with PyVirtualDisplay: {e}[/red]")
                else:
                    print(f"Error creating virtual display with PyVirtualDisplay: {e}")
        
        # Fall back to direct Xvfb if available
        if self._capabilities["has_xvfb"]:
            try:
                # Find an available display number
                for display_id in range(99, 110):
                    if not os.path.exists(f"/tmp/.X{display_id}-lock"):
                        self._current_display_id = display_id
                        break
                
                # Start Xvfb process
                display_str = f":{self._current_display_id}"
                cmd = [
                    "Xvfb", display_str, "-screen", "0", 
                    f"{width}x{height}x{color_depth}", "-nolisten", "tcp"
                ]
                if not visible:
                    cmd.append("-ac")
                    
                self._display_process = subprocess.Popen(
                    cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
                )
                
                # Wait for display to become available
                time.sleep(1)
                
                # Set environment variable for applications
                os.environ["DISPLAY"] = display_str
                
                if HAS_RICH:
                    CONSOLE.print(f"[green]✓[/green] Virtual display created at [bold]{display_str}[/bold]")
                else:
                    print(f"✓ Virtual display created at {display_str}")
                return True
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[red]Error creating virtual display with Xvfb: {e}[/red]")
                else:
                    print(f"Error creating virtual display with Xvfb: {e}")
                    
        return False

    def destroy_virtual_display(self) -> None:
        """🧹 Clean up virtual display resources."""
        # Stop all active streams
        self._active_streams.clear()
        
        # Clean up pyvirtualdisplay
        if self._virtual_display is not None:
            try:
                self._virtual_display.stop()
            except:
                pass
            self._virtual_display = None
            
        # Clean up direct Xvfb process
        if self._display_process is not None:
            try:
                self._display_process.terminate()
                self._display_process.wait(timeout=3)
            except:
                try:
                    self._display_process.kill()
                except:
                    pass
            self._display_process = None

    def capture_screenshot(self) -> Optional[Image.Image]:
        """📸 Capture screenshot from current display with optimal backend.
        
        Returns:
            PIL Image of current screen or None on failure
        """
        # Rate limiting to prevent excessive captures
        current_time = time.time()
        if current_time - self._last_capture_time < 0.01:  # Max 100 FPS
            time.sleep(0.01)
            
        self._last_capture_time = current_time
        
        # Try different capture methods in order of preference
        if self._capture_backend == "mss":
            try:
                import mss
                with mss.mss() as sct:
                    # Get the monitor to capture (first monitor or virtual display)
                    monitor = sct.monitors[0]  # Capturing the entire display
                    
                    # Capture the screen
                    sct_img = sct.grab(monitor)
                    
                    # Convert to PIL Image
                    return Image.frombytes("RGB", sct_img.size, sct_img.rgb)
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[yellow]MSS capture failed: {e}[/yellow]")
                else:
                    print(f"MSS capture failed: {e}")
                    
        elif self._capture_backend == "xwd":
            try:
                # Use xwd to capture the screen
                display = os.environ.get("DISPLAY", f":{self._current_display_id}")
                cmd = ["xwd", "-root", "-display", display, "-silent"]
                
                process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                stdout, stderr = process.communicate(timeout=5)
                
                if process.returncode == 0:
                    # Convert xwd output to PIL Image
                    return Image.open(io.BytesIO(stdout))
                else:
                    if HAS_RICH:
                        CONSOLE.print(f"[yellow]xwd capture failed: {stderr.decode()}[/yellow]")
                    else:
                        print(f"xwd capture failed: {stderr.decode()}")
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[yellow]X11 capture failed: {e}[/yellow]")
                else:
                    print(f"X11 capture failed: {e}")
                    
        elif self._capture_backend == "pillow":
            try:
                from PIL import ImageGrab
                return ImageGrab.grab()
            except Exception as e:
                if HAS_RICH:
                    CONSOLE.print(f"[yellow]PIL capture failed: {e}[/yellow]")
                else:
                    print(f"PIL capture failed: {e}")
        
        # If all capture methods failed, try one more fallback
        try:
            # ImageMagick's import command (often available on Linux)
            if shutil.which("import"):
                temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)
                temp_file.close()
                
                cmd = ["import", "-window", "root", temp_file.name]
                process = subprocess.run(cmd, timeout=5)
                
                if process.returncode == 0:
                    img = Image.open(temp_file.name)
                    os.unlink(temp_file.name)
                    return img
                
                os.unlink(temp_file.name)
        except:
            pass
            
        return None

    def stream_display_to_terminal(self,
                                 scale_factor: int = 2,
                                 block_width: int = 8,
                                 block_height: int = 8,
                                 edge_threshold: int = 50,
                                 gradient_str: Optional[str] = None,
                                 color: bool = True,
                                 fps: int = 15,
                                 enhanced_edges: bool = True,
                                 max_frames: Optional[int] = None,
                                 algorithm: str = "sobel",
                                 show_stats: bool = True,
                                 adaptive_quality: bool = True) -> None:
        """🎬 Stream virtual display content to terminal as Unicode art.
        
        Creates real-time visualization of GUI applications in the terminal
        using adaptive quality parameters and system-aware optimizations.
        
        Args:
            scale_factor: Detail enhancement factor
            block_width: Character cell width
            block_height: Character cell height
            edge_threshold: Edge sensitivity
            gradient_str: Custom character gradient
            color: Enable ANSI colors
            fps: Target frames per second
            enhanced_edges: Use advanced edge characters
            max_frames: Maximum frames to capture (None for continuous)
            algorithm: Edge detection algorithm
            show_stats: Display performance metrics
            adaptive_quality: Auto-adjust quality for performance
        """
        # Generate unique stream ID for tracking
        stream_id = str(uuid.uuid4())
        self._active_streams.add(stream_id)
        
        # Get system-aware parameters
        system_params = SYSTEM_CONTEXT.get_optimized_parameters()
        terminal_width = SYSTEM_CONTEXT.attributes.get("terminal_width", 80)
        terminal_height = SYSTEM_CONTEXT.attributes.get("terminal_height", 24)
        
        # Adjust parameters based on system capabilities if adaptive
        if adaptive_quality:
            # Reduce quality on slower systems
            if SYSTEM_CONTEXT.constraints.get("performance_tier", 1) <= 1:
                scale_factor = 1
                edge_threshold = 70
                algorithm = "sobel"  # Fastest algorithm
                
            # Terminal size adaptation
            if terminal_width < 100:
                block_width = max(block_width, 10)
                block_height = max(block_height, int(block_width * 0.5))
                
        # Fallback gradient based on terminal capabilities
        if gradient_str is None:
            if UNICODE_ENGINE.supports_unicode:
                gradient_str = get_enhanced_gradient_chars()
            else:
                gradient_str = UNICODE_ENGINE.character_maps["full_gradients"]["ascii_art"]
                
        # Performance tracking
        max_tracking_samples = 30
        frame_duration = 1.0 / fps
        frames_processed = 0
        dropped_frames = 0
        start_time = time.time()
        
        # Performance metrics with ring buffer
        render_times = collections.deque(maxlen=max_tracking_samples)
        fps_values = collections.deque(maxlen=max_tracking_samples)
        capture_times = collections.deque(maxlen=max_tracking_samples)
        
        # Adaptive quality parameters
        adaptive_params = {
            'scale': scale_factor,
            'width': block_width,
            'height': block_height,
            'threshold': edge_threshold,
            'frames_since_adjustment': 0,
            'quality_level': 2,
        }
        
        # Quality adjustment thresholds (ms/frame)
        quality_thresholds = {
            'reduce': frame_duration * 1000 * 0.9,  # If processing takes >90% of frame time
            'improve': frame_duration * 1000 * 0.6,  # If processing takes <60% of frame time
        }
        
        try:
            # Prepare terminal for streaming display
            if HAS_RICH:
                with CONSOLE.screen():
                    CONSOLE.print(Panel.fit(
                        f"🖥️ [bold cyan]Virtual Display Stream[/bold cyan]\n"
                        f"[dim]Resolution: {self._display_size[0]}×{self._display_size[1]} • "
                        f"Target: {fps} FPS • "
                        f"Quality: {adaptive_params['quality_level']}/4[/dim]",
                        border_style="blue"
                    ))
                    
                    # Main streaming loop
                    frame_count = 0
                    while (max_frames is None or frame_count < max_frames) and stream_id in self._active_streams:
                        frame_start = time.time()
                        
                        # ⏱️ Capture timing
                        capture_start = time.time()
                        screenshot = self.capture_screenshot()
                        capture_time = (time.time() - capture_start) * 1000
                        capture_times.append(capture_time)
                        
                        if screenshot is None:
                            # If capture failed, skip this frame
                            time.sleep(0.1)
                            dropped_frames += 1
                            continue
                            
                        # ⏱️ Processing timing
                        process_start = time.time()
                        frame = image_to_unicode_art(
                            screenshot,
                            scale_factor=adaptive_params['scale'],
                            block_width=adaptive_params['width'],
                            block_height=adaptive_params['height'],
                            edge_threshold=adaptive_params['threshold'],
                            gradient_str=gradient_str,
                            color=color,
                            enhanced_edges=enhanced_edges,
                            algorithm=algorithm
                        )
                        process_time = (time.time() - process_start) * 1000
                        render_times.append(process_time)
                        
                        # Display the frame with stats if enabled
                        CONSOLE.clear()
                        
                        if show_stats:
                            stats_panel = Table.grid()
                            stats_panel.add_column(style="cyan")
                            stats_panel.add_column(style="green")
                            stats_panel.add_column(style="magenta")
                            stats_panel.add_column(style="yellow")
                            
                            # Calculate metrics
                            avg_render = sum(render_times) / len(render_times) if render_times else 0
                            avg_capture = sum(capture_times) / len(capture_times) if capture_times else 0
                            actual_fps = frames_processed / (time.time() - start_time) if frames_processed > 0 else 0
                            
                            stats_panel.add_row(
                                f"Frame: {frame_count}",
                                f"FPS: {actual_fps:.1f}",
                                f"Render: {avg_render:.1f}ms",
                                f"Quality: {adaptive_params['quality_level']}/4"
                            )
                            
                            CONSOLE.print(stats_panel)
                            
                        # Display frame
                        for line in frame:
                            CONSOLE.print(line)
                            
                        frames_processed += 1
                        frame_count += 1
                        
                        # Adaptive quality management
                        adaptive_params['frames_since_adjustment'] += 1
                        
                        if adaptive_quality and adaptive_params['frames_since_adjustment'] >= 5:
                            total_time = avg_render + avg_capture
                            
                            # Reduce quality if we're struggling to meet FPS
                            if total_time > quality_thresholds['reduce'] and adaptive_params['quality_level'] > 0:
                                adaptive_params['quality_level'] -= 1
                                adaptive_params['scale'] = max(1, adaptive_params['scale'] - 1)
                                adaptive_params['width'] = min(16, adaptive_params['width'] + 2)
                                adaptive_params['height'] = min(16, adaptive_params['height'] + 2)
                                adaptive_params['threshold'] = min(80, adaptive_params['threshold'] + 10)
                                adaptive_params['frames_since_adjustment'] = 0
                                
                            # Improve quality if we're exceeding FPS target
                            elif total_time < quality_thresholds['improve'] and adaptive_params['quality_level'] < 4:
                                adaptive_params['quality_level'] += 1
                                adaptive_params['scale'] = min(4, adaptive_params['scale'] + 1)
                                adaptive_params['width'] = max(4, adaptive_params['width'] - 2)
                                adaptive_params['height'] = max(2, adaptive_params['height'] - 1)
                                adaptive_params['threshold'] = max(30, adaptive_params['threshold'] - 10)
                                adaptive_params['frames_since_adjustment'] = 0
                        
                        # Control frame rate
                        elapsed = time.time() - frame_start
                        sleep_time = frame_duration - elapsed
                        
                        if sleep_time > 0:
                            time.sleep(sleep_time)
                        else:
                            dropped_frames += 1
                            
                        # Collect metrics
                        actual_frame_duration = time.time() - frame_start
                        fps_values.append(1.0 / actual_frame_duration if actual_frame_duration > 0 else fps)
            else:
                # Simpler non-Rich version with ANSI control codes
                print("\033[2J\033[H", end="")  # Clear screen and move to home
                print(f"🖥️ Virtual Display Stream - Target: {fps} FPS - Quality: {adaptive_params['quality_level']}/4")
                
                # Main streaming loop
                frame_count = 0
                while (max_frames is None or frame_count < max_frames) and stream_id in self._active_streams:
                    frame_start = time.time()
                    
                    # Capture screenshot
                    capture_start = time.time()
                    screenshot = self.capture_screenshot()
                    capture_time = (time.time() - capture_start) * 1000
                    capture_times.append(capture_time)
                    
                    if screenshot is None:
                        # If capture failed, skip this frame
                        time.sleep(0.1)
                        dropped_frames += 1
                        continue
                        
                    # Process image
                    process_start = time.time()
                    frame = image_to_unicode_art(
                        screenshot,
                        scale_factor=adaptive_params['scale'],
                        block_width=adaptive_params['width'],
                        block_height=adaptive_params['height'],
                        edge_threshold=adaptive_params['threshold'],
                        gradient_str=gradient_str,
                        color=color,
                        enhanced_edges=enhanced_edges,
                        algorithm=algorithm
                    )
                    process_time = (time.time() - process_start) * 1000
                    render_times.append(process_time)
                    
                    # Display the frame
                    print("\033[2J\033[H", end="")  # Clear screen and move to home
                    
                    if show_stats:
                        avg_render = sum(render_times) / len(render_times) if render_times else 0
                        avg_capture = sum(capture_times) / len(capture_times) if capture_times else 0
                        actual_fps = frames_processed / (time.time() - start_time) if frames_processed > 0 else 0
                        
                        print(f"Frame: {frame_count} | FPS: {actual_fps:.1f} | "
                              f"Render: {avg_render:.1f}ms | Quality: {adaptive_params['quality_level']}/4")
                    
                    # Display frame
                    for line in frame:
                        print(line)
                        
                    frames_processed += 1
                    frame_count += 1
                    
                    # Adaptive quality management
                    adaptive_params['frames_since_adjustment'] += 1
                    
                    if adaptive_quality and adaptive_params['frames_since_adjustment'] >= 5:
                        total_time = avg_render + avg_capture
                        
                        # Adjust quality based on performance
                        if total_time > quality_thresholds['reduce'] and adaptive_params['quality_level'] > 0:
                            adaptive_params['quality_level'] -= 1
                            adaptive_params['scale'] = max(1, adaptive_params['scale'] - 1)
                            adaptive_params['width'] = min(16, adaptive_params['width'] + 2)
                            adaptive_params['height'] = min(16, adaptive_params['height'] + 2)
                            adaptive_params['threshold'] = min(80, adaptive_params['threshold'] + 10)
                            adaptive_params['frames_since_adjustment'] = 0
                        elif total_time < quality_thresholds['improve'] and adaptive_params['quality_level'] < 4:
                            adaptive_params['quality_level'] += 1
                            adaptive_params['scale'] = min(4, adaptive_params['scale'] + 1)
                            adaptive_params['width'] = max(4, adaptive_params['width'] - 2)
                            adaptive_params['height'] = max(2, adaptive_params['height'] - 1)
                            adaptive_params['threshold'] = max(30, adaptive_params['threshold'] - 10)
                            adaptive_params['frames_since_adjustment'] = 0
                    
                    # Control frame rate
                    elapsed = time.time() - frame_start
                    sleep_time = frame_duration - elapsed
                    
                    if sleep_time > 0:
                        time.sleep(sleep_time)
                    else:
                        dropped_frames += 1
                        
                    # Collect metrics
                    actual_frame_duration = time.time() - frame_start
                    fps_values.append(1.0 / actual_frame_duration if actual_frame_duration > 0 else fps)
                    
        except KeyboardInterrupt:
            if HAS_RICH:
                CONSOLE.print("\n[bold green]👋 Stream terminated by user[/bold green]")
            else:
                print("\n👋 Stream terminated by user")
        except Exception as e:
            if HAS_RICH:
                CONSOLE.print(f"[bold red]🚫 Stream error: {str(e)}[/bold red]")
                CONSOLE.print_exception()
            else:
                print(f"\n🚫 Stream error: {str(e)}")
                traceback.print_exc()
        finally:
            # Clean up stream resources
            if stream_id in self._active_streams:
                self._active_streams.remove(stream_id)
            
            # Display final statistics
            duration = time.time() - start_time
            actual_fps = frames_processed / duration if duration > 0 else 0
            avg_render = sum(render_times) / len(render_times) if render_times else 0
            
            if show_stats:
                if HAS_RICH:
                    CONSOLE.print("\n[bold]📊 Performance summary:[/bold]")
                    stats_table = Table()
                    stats_table.add_column("Metric", style="cyan")
                    stats_table.add_column("Value", style="green")
                    
                    stats_table.add_row("Frames processed", str(frames_processed))
                    stats_table.add_row("Runtime", f"{duration:.2f}s")
                    stats_table.add_row("Effective rate", f"{actual_fps:.2f} fps")
                    stats_table.add_row("Average render", f"{avg_render:.1f}ms/frame")
                    stats_table.add_row("Final quality level", f"{adaptive_params['quality_level']}/4")
                    if dropped_frames > 0:
                        stats_table.add_row("Dropped frames", f"{dropped_frames} ({dropped_frames/max(1, frames_processed)*100:.1f}%)")
                    
                    CONSOLE.print(stats_table)
                else:
                    print("\n📊 Performance summary:")
                    print(f"  • Frames processed: {frames_processed}")
                    print(f"  • Runtime: {duration:.2f}s")
                    print(f"  • Effective rate: {actual_fps:.2f} fps")
                    print(f"  • Average render: {avg_render:.1f}ms/frame")
                    print(f"  • Final quality level: {adaptive_params['quality_level']}/4")
                    if dropped_frames > 0:
                        print(f"  • Dropped frames: {dropped_frames} ({dropped_frames/max(1, frames_processed)*100:.1f}%)")

    def launch_gui_application(self, 
                             command: Union[str, List[str]],
                             stream_to_terminal: bool = True,
                             fps: int = 15,
                             timeout: Optional[int] = None,
                             scale_factor: int = 2,
                             color: bool = True) -> subprocess.Popen:
        """🚀 Launch GUI application on virtual display and optionally stream to terminal.
        
        Executes a graphical application on the virtual display and can simultaneously
        stream its visual output to the terminal as Unicode art.
        
        Args:
            command: Application command or list of arguments
            stream_to_terminal: Whether to capture and display the GUI
            fps: Frames per second for streaming
            timeout: Maximum runtime in seconds (None for no limit)
            scale_factor: Detail level for streaming
            color: Enable color output
            
        Returns:
            Process handle for the launched application
        """
        # Ensure we have a virtual display
        if not self._virtual_display and self._display_process is None:
            created = self.create_virtual_display()
            if not created:
                raise RuntimeError("Failed to create virtual display")
                
        # Ensure DISPLAY environment variable is set
        if "DISPLAY" not in os.environ:
            os.environ["DISPLAY"] = f":{self._current_display_id}"
        # Launch the application
        if isinstance(command, str):
            command = shlex.split(command)
        process = subprocess.Popen(
            command,
            env=os.environ,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        if stream_to_terminal:
            # Stream the application to the terminal
            self.stream_display_to_terminal(
                scale_factor=scale_factor,
                fps=fps,
                color=color,
                max_frames=timeout,
                adaptive_quality=True
            )
            return process
        else:
            # Just launch the application without streaming
            return process

# ╔══════════════════════════════════════════════════════════════╗
# ║ 🌟 Interactive Mode & File Selection                         ║
# ╚══════════════════════════════════════════════════════════════╝

def show_interactive_menu() -> Dict[str, Any]:
    """🎮 Present interactive interface for GlyphStream.
    
    Provides contextually-relevant options with multi-stage rendering
    based on system capabilities and user choices.
    
    Returns:
        Dictionary of user-selected options with validated parameters
    """
    if not HAS_RICH:
        print("🔮 Installing rich is recommended for best experience:")
        print("   pip install rich")
        
        # Basic input in non-rich mode
        print("\n╔════════ GlyphStream Interactive ════════╗")
        print("║ Transform reality through Unicode art    ║")
        print("╚═══════════════════════════════════════════╝")
        source_type = input("📌 Select source type (1=Image, 2=Video, 3=YouTube, 4=Webcam, 5=Virtual Display): ")
        source = input("🔍 Enter path or URL: ")
        scale = input("🔬 Detail level (1-4, default=2): ")
        color = input("🎨 Use color? (y/n, default=y): ")
        
        # System-aware defaults with Eidosian integration
        system_params = SYSTEM_CONTEXT.get_optimized_parameters()
        default_scale = min(2, system_params.get("scale_factor", 2))
        
        options = {
            "source": source,
            "scale": int(scale) if scale.isdigit() and 1 <= int(scale) <= 4 else default_scale,
            "color": color.lower() != 'n',
            "video": source_type in ('2', '3', '4', '5'),
            "virtual_display": source_type == '5',
            "fps": system_params.get("fps", 15),
            "edge_threshold": 50,
            "enhanced_edges": True,
            "algorithm": "sobel",
            "gradient_set": "standard",
            "dithering": False,
            "render_engine": "unicode"  # Default rendering engine
        }
        
        if source_type == '3' and not source.startswith(("http", "www")):
            options["source"] = f"https://youtu.be/{source}"
        elif source_type in ('4', '5'):
            options["source"] = int(source) if source.isdigit() else 0
            
        print("\n✨ Processing with Neuroforge transmutation engine...\n")
        return options
    
    # Rich-enhanced interface with Eidosian flair
    title_art = Text("✨ GLYPH STREAM ✨", style="bold cyan")
    subtitle = Text("Dimensional Unicode Transmutation Engine", style="italic")
    
    # Create stylish header with author information
    header_panel = Panel(
        Group(
            Align.center(title_art),
            Align.center(subtitle),
            Align.center(Text(f"by {AUTHOR_INFO['name']} • {AUTHOR_INFO['org']}", style="dim"))
        ),
        border_style="blue",
        box=rich.box.ROUNDED
    )
    
    CONSOLE.print(header_panel)
    
    # Get system-optimized parameters for intelligent defaults
    system_params = SYSTEM_CONTEXT.get_optimized_parameters()
    
    # Source type selection with visual guidance and personality
    source_table = Table(show_header=False, box=rich.box.SIMPLE)
    source_table.add_column("№", style="cyan", no_wrap=True)
    source_table.add_column("Type", style="green", no_wrap=True)
    source_table.add_column("Description", style="white")
    
    source_table.add_row("1", "🖼️ [bold]Image[/bold]", "Convert still images into dimensional art")
    source_table.add_row("2", "🎬 [bold]Video[/bold]", "Transform local video files into streams")
    source_table.add_row("3", "📺 [bold]YouTube[/bold]", "Transmute online videos in real-time")
    source_table.add_row("4", "📷 [bold]Webcam[/bold]", "See yourself through the dimensional lens")
    source_table.add_row("5", "🖥️ [bold]Display[/bold]", "Capture virtual display output")
    
    CONSOLE.print(Panel(source_table, title="📦 Source Selection", border_style="cyan"))
    source_type = Prompt.ask("⚡ Choose input type", choices=["1", "2", "3", "4", "5"], default="1")
    
    # Source path/URL with contextual prompting
    if source_type == "1":
        CONSOLE.print("🖼️ [bold]Image Transmutation Mode[/bold]")
        source = Prompt.ask("📂 Enter image path")
        
        # Engine selection with visual indicators
        engine_table = Table(show_header=False, box=rich.box.SIMPLE)
        engine_table.add_column("Engine", style="cyan")
        engine_table.add_column("Characteristics", style="white")
        
        engine_table.add_row("unicode", "⚛️ Standard - Edge-aware dimensional rendering")
        engine_table.add_row("text", "🔤 Typographic - ASCII art with font variety")
        engine_table.add_row("transformer", "✨ Advanced - Multi-stage processing pipeline")
        
        CONSOLE.print(Panel(engine_table, title="🧠 Rendering Engines", border_style="magenta"))
        render_engine = Prompt.ask(
            "🧪 Select rendering engine",
            choices=["unicode", "text", "transformer"],
            default="unicode"
        )
        
        # Show preset options with visual indicators
        preset_table = Table(show_header=False, box=rich.box.SIMPLE)
        preset_table.add_column("Preset", style="cyan")
        preset_table.add_column("Description", style="white")
        
        preset_table.add_row("standard", "Balanced quality and performance")
        preset_table.add_row("detailed", "✨ High-fidelity edge detection and dithering")
        preset_table.add_row("fast", "🚀 Optimized for speed on any system")
        preset_table.add_row("minimal", "🧩 ASCII-compatible, resource-efficient")
        
        CONSOLE.print(Panel(preset_table, title="🎛️ Processing Presets", border_style="green"))
        preset_choice = Prompt.ask(
            "🎚️ Select processing mode",
            choices=["standard", "detailed", "fast", "minimal"],
            default="standard"
        )
    elif source_type == "2":
        CONSOLE.print("🎬 [bold]Video Stream Mode[/bold]")
        source = Prompt.ask("📂 Enter video file path")
        render_engine = "stream"
    elif source_type == "3":
        CONSOLE.print("📺 [bold]YouTube Stream Mode[/bold]")
        youtube_input = Prompt.ask("🔗 Enter YouTube URL or video ID")
        if not (youtube_input.startswith("http") or youtube_input.startswith("www")):
            source = f"https://youtu.be/{youtube_input}"
        else:
            source = youtube_input
        render_engine = "stream"
    elif source_type == "4":
        CONSOLE.print("📷 [bold]Realtime Capture Mode[/bold]")
        webcam_id = Prompt.ask("🎯 Enter device ID (usually 0)", default="0")
        source = int(webcam_id)
        render_engine = "stream"
    else:  # Virtual Display
        CONSOLE.print("🖥️ [bold]Virtual Display Capture Mode[/bold]")
        display_engine = VirtualDisplayEngine.get_instance()
        
        if not display_engine._capabilities.get("can_create_display", False):
            CONSOLE.print("[bold red]⚠️ Virtual display capabilities not detected[/bold red]")
            CONSOLE.print("[yellow]Install dependencies: pip install pyvirtualdisplay[/yellow]")
        
        virtual_display_dims = Prompt.ask(
            "📐 Display dimensions (WIDTHxHEIGHT)", 
            default="1280x720"
        )
        width, height = map(int, virtual_display_dims.lower().split('x'))
        source = 0  # Default display ID
        render_engine = "virtual"
    
    # Configuration with visual styling
    config_panel = Panel(
        Text("Adjust these parameters to control the dimensional transmutation process",
             style="italic"),
        title="⚙️ Configuration",
        border_style="cyan"
    )
    CONSOLE.print(config_panel)
    
    # Detail level with system-aware defaults and visual cues
    detail_table = Table(show_header=False, box=rich.box.SIMPLE, expand=True)
    detail_table.add_column("Level", style="cyan", width=8)
    detail_table.add_column("Impact", style="white")
    
    detail_table.add_row("1", "🚀 Fastest - minimal detail")
    detail_table.add_row("2", "⚡ Balanced - standard detail")
    detail_table.add_row("3", "✨ Enhanced - higher detail")
    detail_table.add_row("4", "💎 Maximum - highest detail (slower)")
    
    CONSOLE.print(Panel(detail_table, title="Detail Levels", border_style="blue"))
    
    default_scale = str(min(2, system_params.get("scale_factor", 2)))
    scale = int(Prompt.ask(
        "🔍 Detail level", 
        choices=["1", "2", "3", "4"],
        default=default_scale
    ))
    
    # Color support with system capability check
    color = Confirm.ask(
        "🎨 Enable dimensional color", 
        default=UNICODE_ENGINE.supports_color
    )
    
    # Character set selection with UNICODE_ENGINE integration
    if source_type == "1" and render_engine == "unicode":
        gradient_table = Table(show_header=False, box=rich.box.SIMPLE)
        gradient_table.add_column("Set", style="cyan")
        gradient_table.add_column("Characteristics", style="white")
        
        gradient_table.add_row("standard", "█▓▒░ Standard balanced set")
        gradient_table.add_row("enhanced", "█▇▆▅▄▃▂▁ Extended gradients")
        gradient_table.add_row("braille", "⣿⣷⣯⣟⡿⢿⣻⣽⣾ Braille patterns")
        gradient_table.add_row("ascii", "@%#*+=-:. ASCII compatible")
        
        CONSOLE.print(Panel(gradient_table, title="🔣 Gradient Character Sets", border_style="blue"))
        
        gradient_set = Prompt.ask(
            "🔠 Select character set",
            choices=["standard", "enhanced", "braille", "ascii"],
            default="standard"
        )
    elif source_type == "1" and render_engine == "text":
        # Font category selection for text engine
        font_categories = TEXT_ENGINE.get_font_categories()
        
        if font_categories:
            categories_str = ", ".join(font_categories)
            CONSOLE.print(f"📝 Available font categories: [cyan]{categories_str}[/cyan]")
            font_category = Prompt.ask("🔤 Select font category", default="standard")
            font_name = TEXT_ENGINE.get_random_font(font_category)
        else:
            font_name = "standard"
    else:
        gradient_set = "standard"
    
    # Prepare options dictionary with base settings
    options = {
        "source": source,
        "scale": scale,
        "color": color,
        "video": source_type in ("2", "3", "4", "5"),
        "virtual_display": source_type == "5",
        "render_engine": render_engine if "render_engine" in locals() else "unicode",
    }
    
    # Add engine-specific options
    if "render_engine" in locals():
        if render_engine == "text" and "font_name" in locals():
            options.update({
                "font": font_name
            })
        elif render_engine == "unicode" and "gradient_set" in locals():
            options.update({
                "gradient_set": gradient_set
            })
        elif render_engine == "virtual":
            options.update({
                "display_width": width if "width" in locals() else 1280,
                "display_height": height if "height" in locals() else 720
            })
    
    # Apply preset configurations with system awareness
    if source_type == "1" and "preset_choice" in locals():
        if preset_choice == "detailed":
            # High-fidelity settings optimized for detailed edge rendering
            options.update({
                "scale": min(3, system_params.get("scale_factor", 3)),
                "block_width": 4,
                "block_height": 4,
                "edge_threshold": 40,
                "algorithm": "scharr",
                "enhanced_edges": True,
                "dithering": True
            })
        elif preset_choice == "fast":
            # Performance-optimized settings
            options.update({
                "scale": 1,
                "block_width": 12,
                "block_height": 12,
                "edge_threshold": 60,
                "enhanced_edges": False,
                "dithering": False
            })
        elif preset_choice == "minimal":
            # Resource-efficient mode
            options.update({
                "scale": 1,
                "block_width": 8,
                "block_height": 8,
                "edge_threshold": 80,
                "color": False,
                "enhanced_edges": False,
                "dithering": False
            })
    
    # Video-specific options with intelligent defaults
    if source_type in ("2", "3", "4", "5"):
        # Dynamic FPS based on system capabilities
        perf_tier = SYSTEM_CONTEXT.constraints.get("performance_tier", 1)
        default_fps = system_params.get("default_fps", 15)
        
        fps_table = Table(show_header=False, box=rich.box.SIMPLE, expand=True)
        fps_table.add_column("FPS", style="cyan", width=8)
        fps_table.add_column("Description", style="white")
        
        fps_table.add_row("10", "🐢 Smooth on low-end systems")
        fps_table.add_row("15", "⚡ Balanced performance")
        fps_table.add_row("30", "🚀 Fluid motion on high-end systems")
        
        CONSOLE.print(Panel(fps_table, title="🎞️ Frame Rate Options", border_style="blue"))
        
        fps = int(Prompt.ask(
            "🎞️ Target frames per second",
            choices=["10", "15", "20", "30"],
            default=str(default_fps)
        ))
        
        # Auto-quality with visual explanation
        CONSOLE.print(Panel(
            "Adaptive quality automatically adjusts parameters to maintain target FPS",
            title="🤖 Adaptive Engine",
            border_style="green"
        ))
        
        adaptive_quality = Confirm.ask(
            "🧠 Enable adaptive quality", 
            default=True
        )
        
        options.update({
            "fps": fps,
            "adaptive_quality": adaptive_quality,
            "show_stats": Confirm.ask("📊 Show performance metrics", default=True),
            "border": Confirm.ask("🔲 Add dimensional frame", default=True)
        })
    
    # Advanced options with visual collapse pattern
    if Confirm.ask("🔬 Configure advanced parameters", default=False):
        CONSOLE.print(Panel("Fine-tune the dimensional transmutation engine", 
                          title="⚙️ Advanced Configuration", 
                          border_style="magenta"))
        
        # Block size with visual guidance
        block_width = int(Prompt.ask(
            "⬜ Block width (smaller=more detail, slower)",
            default=str(system_params.get("block_width", 8))
        ))
        
        block_height = int(Prompt.ask(
            "⬜ Block height",
            default=str(system_params.get("block_height", 8))
        ))
        
        # Edge detection with visual guidance
        edge_table = Table(show_header=False, box=rich.box.SIMPLE, expand=True)
        edge_table.add_column("Value", style="cyan", width=8)
        edge_table.add_column("Effect", style="white")
        
        edge_table.add_row("30", "🔍 Sensitive - detects subtle edges")
        edge_table.add_row("50", "⚖️ Balanced - standard sensitivity")
        edge_table.add_row("70", "🔎 Selective - only pronounced edges")
        
        CONSOLE.print(Panel(edge_table, title="Edge Detection Sensitivity", border_style="blue"))
        
        edge_threshold = int(Prompt.ask(
            "🔪 Edge threshold",
            default="50"
        ))
        
        enhanced_edges = Confirm.ask(
            "✨ Use enhanced directional edges", 
            default=system_params.get("edge_mode", "enhanced") == "enhanced"
        )
        
        # Algorithm selection with visual guidance for images
        if source_type == "1" and "render_engine" in locals() and render_engine == "unicode":
            algo_table = Table(show_header=False, box=rich.box.SIMPLE)
            algo_table.add_column("Algorithm", style="cyan")
            algo_table.add_column("Characteristics", style="white")
            
            algo_table.add_row("sobel", "⚡ Fast, balanced edge detection")
            algo_table.add_row("prewitt", "🧠 Less noise sensitivity")
            algo_table.add_row("scharr", "✨ Better diagonal edge detection")
            algo_table.add_row("laplacian", "🔍 Detects edges in all directions")
            algo_table.add_row("canny", "💎 Advanced multi-stage detection")
            
            CONSOLE.print(Panel(algo_table, title="🔬 Edge Detection Algorithms", border_style="blue"))
            
            algorithm = Prompt.ask(
                "🧪 Edge detection algorithm",
                choices=["sobel", "prewitt", "scharr", "laplacian", "canny"],
                default="sobel"
            )
            
            dithering = Confirm.ask(
                "🔢 Apply error diffusion dithering", 
                default=False
            )
            
            options.update({
                "algorithm": algorithm,
                "dithering": dithering
            })
            
        # Text engine specific options
        if "render_engine" in locals() and render_engine == "text" and source_type == "1":
            options.update({
                "text_align": Prompt.ask(
                    "📏 Text alignment", 
                    choices=["left", "center", "right"],
                    default="center"
                ),
                "add_border": Confirm.ask("🔲 Add text border", default=False)
            })
            
        # Virtual display specific options
        if "render_engine" in locals() and render_engine == "virtual":
            options.update({
                "launch_application": Confirm.ask(
                    "🚀 Launch application in virtual display", 
                    default=False
                )
            })
            
            if options.get("launch_application"):
                options["application_command"] = Prompt.ask(
                    "💻 Enter application command"
                )
        
        options.update({
            "block_width": block_width,
            "block_height": block_height,
            "edge_threshold": edge_threshold,
            "enhanced_edges": enhanced_edges
        })
    else:
        # Apply intelligent system-aware defaults
        options.update({
            "block_width": system_params.get("block_width", 8),
            "block_height": system_params.get("block_height", 8),
            "edge_threshold": 50,
            "enhanced_edges": system_params.get("edge_mode", "enhanced") == "enhanced",
            "algorithm": "sobel",
            "dithering": False
        })
    
    # Transformer pipeline configuration for advanced processing
    if "render_engine" in locals() and render_engine == "transformer" and source_type == "1":
        CONSOLE.print(Panel(
            "The Art Transformer provides a pipeline of sequential operations",
            title="🔄 Transformation Pipeline",
            border_style="magenta"
        ))
        
        # Select transformations to apply
        options["transformations"] = []
        
        transform_table = Table(show_header=False, box=rich.box.SIMPLE)
        transform_table.add_column("Transform", style="cyan")
        transform_table.add_column("Effect", style="white")
        
        transform_table.add_row("optimize", "🔧 Auto-optimize for terminal display")
        transform_table.add_row("edge", "🔪 Apply edge detection with current algorithm")
        transform_table.add_row("dither", "🔢 Apply dithering for better gradients")
        transform_table.add_row("invert", "🔄 Invert image colors")
        
        CONSOLE.print(Panel(transform_table, title="Available Transformations", border_style="blue"))
        
        if Confirm.ask("🔧 Add auto-optimization", default=True):
            options["transformations"].append("optimize")
        
        if Confirm.ask("🔪 Add edge detection", default=True):
            options["transformations"].append("edge")
            
        if Confirm.ask("🔢 Add dithering", default=False):
            options["transformations"].append("dither")
            
        if Confirm.ask("🔄 Add color inversion", default=False):
            options["transformations"].append("invert")
    
    # Configuration summary with visual confirmation
    CONSOLE.print("[bold green]✓[/bold green] Dimensional configuration locked in!")
    
    # Optional config summary with styled table
    if Confirm.ask("👁️ View configuration details", default=False):
        summary_table = Table(title="🧩 Processing Matrix", box=rich.box.ROUNDED)
        summary_table.add_column("Parameter", style="cyan")
        summary_table.add_column("Value", style="green")
        
        for key, value in options.items():
            if key != "source":  # Don't show source path in summary
                summary_table.add_row(key, str(value))
                
        CONSOLE.print(Panel(summary_table, border_style="blue"))
    
    CONSOLE.print("\n[bold blue]🌀 Initializing dimensional transmutation...[/bold blue]\n")
    return options


def parse_command_args() -> Dict[str, Any]:
    """🧩 Parse command-line arguments with intelligent defaults.
    
    Returns:
        Dictionary of parsed and validated command options
    """
    # System-aware default parameters
    sys_params = SYSTEM_CONTEXT.get_optimized_parameters()
    
    # Create parser with Eidosian styling
    parser = argparse.ArgumentParser(
        description="🌟 GlyphStream - Dimensional Unicode Art Transmutation Engine",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
╔════════════════════════════════════════════════════╗
║ 🔮 Examples:                                       ║
╟────────────────────────────────────────────────────╢
║ glyph_stream                     # Interactive mode ║
║ glyph_stream image.jpg           # Process image    ║
║ glyph_stream video.mp4 --fps 30  # Process video    ║
║ glyph_stream https://youtu.be/ID # Stream YouTube   ║
║ glyph_stream --webcam            # Use webcam       ║
║ glyph_stream --virtual-display   # Capture display  ║
║ glyph_stream --text "Hello"      # Generate text    ║
║ glyph_stream --help              # Show this help   ║
╚════════════════════════════════════════════════════╝

Neuroforge Glyph Dimensional Engine v1.0
{AUTHOR_INFO["name"]} <{AUTHOR_INFO["email"]}> • {AUTHOR_INFO["org"]}
        """
    )
    
    # Input source options (mutually exclusive)
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument('source', nargs='?', help='📂 Input file path or 🔗 URL')
    input_group.add_argument('--webcam', '-w', action='store_true', help='📷 Use webcam as input')
    input_group.add_argument('--virtual-display', '-vd', action='store_true', help='🖥️ Capture virtual display')
    input_group.add_argument('--text', '-tx', help='🔠 Generate text art')
    
    # Rendering engine selection
    parser.add_argument('--engine', '-e', choices=['unicode', 'text', 'transformer', 'stream', 'virtual'],
                        help='🧠 Rendering engine (default: auto-selected based on input)')
    
    # Processing presets with Eidosian descriptions
    parser.add_argument('--preset', '-p', choices=['standard', 'detailed', 'fast', 'minimal'],
                        help='🧩 Processing preset (standard=balanced, detailed=high-quality, fast=performance, minimal=ASCII)')
    
    # Core parameters with emoji indicators
    parser.add_argument('--scale', '-s', type=int, choices=range(1, 5),
                        default=sys_params.get('scale_factor', 2),
                        help='🔍 Detail enhancement factor (1-4)')
    parser.add_argument('--block-width', '-bw', type=int, default=sys_params.get('block_width', 8),
                        help='⬜ Character cell width')
    parser.add_argument('--block-height', '-bh', type=int, default=sys_params.get('block_height', 8),
                        help='⬜ Character cell height')
    parser.add_argument('--edge-threshold', '-et', type=int, default=50,
                        help='🔪 Edge detection threshold (0-255)')
    
    # Feature flags with Eidosian styling
    parser.add_argument('--no-color', action='store_true',
                        help='⚫ Disable dimensional color')
    parser.add_argument('--no-enhanced-edges', action='store_true',
                        help='➖ Use simplified edge characters')
    parser.add_argument('--dithering', '-d', action='store_true',
                        help='🔢 Apply error diffusion dithering')
    
    # Video-specific options with emoji indicators
    parser.add_argument('--fps', '-f', type=int, default=sys_params.get('fps', 15),
                        help='🎞️ Target frames per second for video')
    parser.add_argument('--no-adaptive', action='store_true',
                        help='🔒 Disable adaptive quality adjustments')
    parser.add_argument('--no-stats', action='store_true',
                        help='🚫 Hide performance statistics')
    parser.add_argument('--no-border', action='store_true',
                        help='⬜ Disable dimensional frame')
    
    # Advanced options with technical descriptions
    parser.add_argument('--algorithm', '-a', choices=['sobel', 'prewitt', 'scharr', 'laplacian', 'canny'],
                        default='sobel', help='🧪 Edge detection algorithm')
    parser.add_argument('--webcam-id', type=int, default=0,
                        help='🎯 Webcam device ID (usually 0)')
    parser.add_argument('--display-size', 
                        help='📐 Virtual display size (WIDTHxHEIGHT), e.g. 1280x720')
    
    # Text engine specific options
    parser.add_argument('--font', 
                        help='🔠 Text font name or category')
    parser.add_argument('--align', choices=['left', 'center', 'right'], default='center',
                        help='📏 Text alignment')
    
    # Character set selection integrated with UnicodeRenderEngine
    parser.add_argument('--gradient-set', choices=['standard', 'enhanced', 'braille', 'ascii'],
                        default='standard', help='🔣 Character gradient set to use')
    
    # Transformation pipeline parameters
    parser.add_argument('--transform', '-t', action='append', choices=['optimize', 'edge', 'dither', 'invert'],
                        help='🔄 Apply transformation (can be used multiple times)')
    
    # Output options with file handling
    parser.add_argument('--save', '-o', metavar='FILE',
                        help='💾 Save output to specified file')
    parser.add_argument('--format', choices=['ansi', 'plain', 'html', 'svg', 'png'],
                        default='ansi', help='📄 Output format when saving')
    
    # System options
    parser.add_argument('--debug', action='store_true',
                        help='🐛 Enable debug mode with additional diagnostics')
    parser.add_argument('--launch-app', 
                        help='🚀 Launch application in virtual display (with --virtual-display)')
    parser.add_argument('--benchmark', action='store_true',
                        help='⏱️ Run benchmark and show detailed performance metrics')
    
    # Parse args with error handling
    try:
        args = parser.parse_args()
    except SystemExit as e:
        # Clean exit after help display
        return {}
    
    # Translate to options dictionary with intelligent source detection
    options = {
        'scale': args.scale,
        'block_width': args.block_width,
        'block_height': args.block_height,
        'edge_threshold': args.edge_threshold,
        'color': not args.no_color,
        'enhanced_edges': not args.no_enhanced_edges,
        'dithering': args.dithering,
        'algorithm': args.algorithm,
        'fps': args.fps,
        'adaptive_quality': not args.no_adaptive,
        'show_stats': not args.no_stats,
        'border': not args.no_border,
        'gradient_set': args.gradient_set,
        'debug': args.debug,
        'benchmark': args.benchmark,
    }
    
    # Engine selection with contextual intelligence
    if args.engine:
        options['render_engine'] = args.engine
    
    # Text processor options
    if args.text:
        options['source'] = args.text
        options['text_content'] = args.text
        options['font'] = args.font or 'standard'
        options['align'] = args.align
        options['render_engine'] = 'text'
        options['video'] = False
    
    # Source determination with validation and intelligent classification
    if args.webcam:
        options['source'] = args.webcam_id
        options['video'] = True
        options['render_engine'] = options.get('render_engine', 'stream')
    elif args.virtual_display:
        # Parse display size with validation
        if args.display_size:
            try:
                width, height = map(int, args.display_size.lower().split('x'))
                options['display_width'] = width
                options['display_height'] = height
            except ValueError:
                if HAS_RICH:
                    CONSOLE.print("[red]Invalid display size format. Using default 1280x720.[/red]")
                options['display_width'] = 1280
                options['display_height'] = 720
        else:
            options['display_width'] = 1280
            options['display_height'] = 720
            
        options['source'] = 0  # Default display ID
        options['video'] = True
        options['virtual_display'] = True
        options['render_engine'] = options.get('render_engine', 'virtual')
        
        # Application launch in virtual display
        if args.launch_app:
            options['launch_application'] = True
            options['application_command'] = args.launch_app
    elif args.source:
        # YouTube URL detection with multi-format support
        if any(domain in args.source for domain in ['youtube.com', 'youtu.be', 'yt.be']):
            options['source'] = args.source
            options['video'] = True
            options['render_engine'] = options.get('render_engine', 'stream')
        # YouTube ID detection (11 chars)
        elif args.source.strip() and len(args.source) == 11 and '/' not in args.source:
            options['source'] = f'https://youtu.be/{args.source}'
            options['video'] = True
            options['render_engine'] = options.get('render_engine', 'stream')
        # File detection with MIME-type awareness
        else:
            # Check if source is a video file
            video_extensions = {'.mp4', '.mkv', '.avi', '.mov', '.webm', '.flv', '.wmv', '.m4v', '.3gp'}
            _, ext = os.path.splitext(args.source.lower())
            options['video'] = ext in video_extensions
            options['source'] = args.source
            
            if options['video']:
                options['render_engine'] = options.get('render_engine', 'stream')
            else:
                options['render_engine'] = options.get('render_engine', 'unicode')
    else:
        # No source specified, fall back to interactive mode
        return {}
    
    # Apply presets with system-aware parameter adaptation
    if args.preset:
        if args.preset == 'detailed':
            options.update({
                'scale': min(3, sys_params.get('scale_factor', 3)),
                'block_width': 4,
                'block_height': 4,
                'edge_threshold': 40,
                'algorithm': 'scharr',
                'enhanced_edges': True,
                'dithering': True
            })
        elif args.preset == 'fast':
            options.update({
                'scale': 1,
                'block_width': 12,
                'block_height': 12,
                'edge_threshold': 60,
                'enhanced_edges': False,
                'dithering': False
            })
        elif args.preset == 'minimal':
            options.update({
                'scale': 1,
                'block_width': 8,
                'block_height': 8,
                'edge_threshold': 80,
                'color': False,
                'enhanced_edges': False,
                'dithering': False,
                'gradient_set': 'ascii'
            })
    
    # Transformation pipeline setup
    if args.transform:
        options['transformations'] = args.transform
        
        # Ensure transformer engine for pipelines
        if 'render_engine' not in options or options['render_engine'] == 'unicode':
            options['render_engine'] = 'transformer'
    
    # File output handling with format determination
    if args.save:
        options['save_path'] = args.save
        options['output_format'] = args.format
        
        # Validate format compatibility
        if options['output_format'] == 'ansi' and not SYSTEM_CONTEXT.capabilities.get("can_display_color", True):
            if HAS_RICH:
                CONSOLE.print("[yellow]Warning: ANSI format selected but terminal doesn't support color. Using 'plain' instead.[/yellow]")
            options['output_format'] = 'plain'
            
        # File format auto-selection from extension
        if '.' in args.save:
            ext = os.path.splitext(args.save)[1].lower()
            if ext in ('.png', '.jpg', '.jpeg') and options['output_format'] == 'ansi':
                options['output_format'] = 'png'
            elif ext == '.svg' and options['output_format'] == 'ansi':
                options['output_format'] = 'svg'
            elif ext == '.html' and options['output_format'] == 'ansi':
                options['output_format'] = 'html'
    
    return options


# ╔══════════════════════════════════════════════════════════════╗
# ║ 🚀 Entry Point & Command Interface                          ║
# ╚══════════════════════════════════════════════════════════════╝

def main() -> None:
    """🎮 GlyphStream command interface.
    
    Provides both CLI and interactive modes with dynamic parameter selection,
    system-aware optimization, and comprehensive error handling.
    """
    try:
        # Check for command-line arguments
        if len(sys.argv) > 1:
            options = parse_command_args()
            # If options is empty, user specified help or invalid args
            if not options:
                return
        else:
            # Interactive mode
            options = show_interactive_menu()
        
        # Process based on source type
        if options["video"]:
            # Handle video source (file, YouTube, or webcam)
            process_video_stream(
                options["source"],
                scale_factor=options["scale"],
                block_width=options.get("block_width", 8),
                block_height=options.get("block_height", 8),
                edge_threshold=options.get("edge_threshold", 50),
                color=options["color"],
                fps=options.get("fps", 15),
                enhanced_edges=options.get("enhanced_edges", True),
                show_stats=options.get("show_stats", True),
                adaptive_quality=options.get("adaptive_quality", True),
                border=options.get("border", True)
            )
        else:
            # Handle image source
            unicode_art = generate_unicode_art(
                options["source"],
                scale_factor=options["scale"],
                block_width=options.get("block_width", 8),
                block_height=options.get("block_height", 8),
                edge_threshold=options.get("edge_threshold", 50),
                color=options["color"],
                enhanced_edges=options.get("enhanced_edges", True),
                algorithm=options.get("algorithm", "sobel"),
                dithering=options.get("dithering", False),
                auto_scale=True
            )
            
            # Handle output destination
            if "save_path" in options:
                # Save to file
                try:
                    with open(options["save_path"], 'w', encoding='utf-8') as f:
                        for line in unicode_art:
                            # Strip ANSI codes if saving to file
                            if not options["color"]:
                                ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
                                line = ansi_escape.sub('', line)
                            f.write(line + '\n')
                    
                    if HAS_RICH:
                        CONSOLE.print(f"[green]✓ Art saved to:[/green] {options['save_path']}")
                    else:
                        print(f"✓ Art saved to: {options['save_path']}")
                except Exception as e:
                    if HAS_RICH:
                        CONSOLE.print(f"[red]🚫 Error saving file:[/red] {str(e)}")
                    else:
                        print(f"🚫 Error saving file: {str(e)}")
            else:
                # Print to console
                for line in unicode_art:
                    print(line)

    except KeyboardInterrupt:
        print("\n👋 Exiting GlyphStream")
        sys.exit(0)
    except Exception as e:
        if HAS_RICH:
            CONSOLE.print(f"\n[bold red]🚫 Error:[/bold red] {str(e)}")
            CONSOLE.print("[yellow]💡 For troubleshooting, run with --help for usage information[/yellow]")
        else:
            print(f"\n🚫 Error: {str(e)}")
            print("💡 Check input parameters and try again")
        sys.exit(1)


if __name__ == "__main__":
    main()